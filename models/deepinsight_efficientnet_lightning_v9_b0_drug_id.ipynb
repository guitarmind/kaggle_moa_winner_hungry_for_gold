{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.6.0+cu101\n",
      "PyTorch Lightning Version: 1.0.4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transform non-image features to an image matrix using t-SNE non-linear dimensionality reduction.\n",
    "\n",
    "[V9]\n",
    "* Tuning resolution and image size\n",
    "\n",
    "[TODO]\n",
    "* PCGrad (Project Conflicting Gradients)\n",
    "* Separate gene expression, cell vaibility and other features\n",
    "\n",
    "EfficientNet Setup Parameters:\n",
    "https://github.com/rwightman/gen-efficientnet-pytorch/blob/master/geffnet/gen_efficientnet.py#L502\n",
    "\"\"\"\n",
    "\n",
    "kernel_mode = False\n",
    "training_mode = True\n",
    "\n",
    "import sys\n",
    "if kernel_mode:\n",
    "    sys.path.insert(0, \"../input/iterative-stratification\")\n",
    "    sys.path.insert(0, \"../input/pytorch-lightning\")\n",
    "    sys.path.insert(0, \"../input/gen-efficientnet-pytorch\")\n",
    "    sys.path.insert(0, \"../input/pytorch-optimizer\")\n",
    "    sys.path.insert(0, \"../input/pytorch-ranger\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer, LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch_optimizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional import classification\n",
    "\n",
    "import geffnet\n",
    "\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug.augmenters.size import CropToFixedSize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "rand_seed = 1120\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning Version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if kernel_mode:\n",
    "#     !mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "#     !cp ../input/gen-efficientnet-pretrained/tf_efficientnet_*.pth /root/.cache/torch/hub/checkpoints/\n",
    "#     !ls -la /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 9\n",
    "model_type = \"b0\"\n",
    "pretrained_model = f\"tf_efficientnet_{model_type}_ns\"\n",
    "experiment_name = f\"deepinsight_efficientnet_v{version}_{model_type}_drug_id\"\n",
    "\n",
    "if kernel_mode:\n",
    "    dataset_folder = \"../input/lish-moa\"\n",
    "    model_output_folder = f\"./{experiment_name}\" if training_mode \\\n",
    "        else f\"../input/deepinsight-efficientnet-v{version}-{model_type}-drug-id/{experiment_name}\"\n",
    "else:\n",
    "    dataset_folder = \"/workspace/Kaggle/MoA\"\n",
    "    model_output_folder = f\"{dataset_folder}/{experiment_name}\" if training_mode \\\n",
    "        else f\"/workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v{version}_{model_type}_drug_id/{experiment_name}\"\n",
    "\n",
    "if training_mode:\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    # Dedicated logger for experiment\n",
    "    exp_logger = TensorBoardLogger(model_output_folder,\n",
    "                                   name=f\"overall_logs\",\n",
    "                                   default_hp_metric=False)\n",
    "\n",
    "# debug_mode = True\n",
    "debug_mode = False\n",
    "\n",
    "num_workers = 2 if kernel_mode else 6\n",
    "# gpus = [0, 1]\n",
    "gpus = [0]\n",
    "# gpus = [1]\n",
    "\n",
    "epochs = 40\n",
    "patience = 16\n",
    "\n",
    "# learning_rate = 1e-3\n",
    "learning_rate = 0.000366  # Suggested Learning Rate from LR finder (V7)\n",
    "learning_rate *= len(gpus)\n",
    "weight_decay = 1e-5\n",
    "# weight_decay = 1e-6\n",
    "# weight_decay = 0\n",
    "\n",
    "# T_max = 10  # epochs\n",
    "T_max = 5  # epochs\n",
    "T_0 = 5  # epochs\n",
    "\n",
    "accumulate_grad_batches = 1\n",
    "gradient_clip_val = 10.0\n",
    "\n",
    "drop_rate = 0.3  # B3\n",
    "\n",
    "batch_size = 48\n",
    "infer_batch_size = 96 if not kernel_mode else 256\n",
    "# image_size = 300\n",
    "image_size = 200\n",
    "resolution = 100\n",
    "\n",
    "# batch_size = 48\n",
    "# infer_batch_size = 96 if not kernel_mode else 256\n",
    "# image_size = 300  # B3\n",
    "# resolution = 300\n",
    "\n",
    "# Prediction Clipping Thresholds\n",
    "prob_min = 0.001\n",
    "prob_max = 0.999\n",
    "\n",
    "# Swap Noise\n",
    "swap_prob = 0.1\n",
    "swap_portion = 0.15\n",
    "\n",
    "label_smoothing = 0.001\n",
    "\n",
    "# DeepInsight Transform\n",
    "perplexity = 5\n",
    "\n",
    "drop_connect_rate = 0.2\n",
    "fc_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_features.csv\", engine='c')\n",
    "train_labels = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_targets_scored.csv\", engine='c')\n",
    "\n",
    "train_extra_labels = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_targets_nonscored.csv\", engine='c')\n",
    "\n",
    "test_features = pd.read_csv(\n",
    "    f\"{dataset_folder}/test_features.csv\", engine='c')\n",
    "\n",
    "sample_submission = pd.read_csv(\n",
    "    f\"{dataset_folder}/sample_submission.csv\", engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by sig_id to ensure that all row orders match\n",
    "train_features = train_features.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\n",
    "train_labels = train_labels.sort_values(by=[\"sig_id\"], axis=0,\n",
    "                                        inplace=False).reset_index(drop=True)\n",
    "train_extra_labels = train_extra_labels.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\n",
    "\n",
    "sample_submission = sample_submission.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23814, 876), (23814, 207), (23814, 403))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, train_labels.shape, train_extra_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3982, 876)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Include Drug ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_drug = pd.read_csv(f\"{dataset_folder}/train_drug.csv\", engine='c')\n",
    "train_features = train_features.merge(train_drug, on='sig_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(873, 772, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_features = [\"cp_type\", \"cp_dose\"]\n",
    "numeric_features = [\n",
    "    c for c in train_features.columns\n",
    "    if c not in [\"sig_id\", \"drug_id\"] and c not in category_features\n",
    "]\n",
    "all_features = category_features + numeric_features\n",
    "gene_experssion_features = [c for c in numeric_features if c.startswith(\"g-\")]\n",
    "cell_viability_features = [c for c in numeric_features if c.startswith(\"c-\")]\n",
    "len(numeric_features), len(gene_experssion_features), len(\n",
    "    cell_viability_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206, 402)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes = [c for c in train_labels.columns if c != \"sig_id\"]\n",
    "train_extra_classes = [c for c in train_extra_labels.columns if c != \"sig_id\"]\n",
    "len(train_classes), len(train_extra_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Drop Control Type Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_features = train_features[train_features[\"cp_type\"] == \"trt_cp\"].copy()\n",
    "train_labels = train_labels.iloc[train_features.index, :].copy()\n",
    "train_extra_labels = train_extra_labels.iloc[train_features.index, :].copy()\n",
    "\n",
    "train_features = train_features.reset_index(drop=True)\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "train_extra_labels = train_extra_labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21948, 877), (21948, 207), (21948, 403))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, train_labels.shape, train_extra_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for df in [train_features, test_features]:\n",
    "    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n",
    "    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    21948\n",
       "Name: cp_type, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11196\n",
       "1    10752\n",
       "Name: cp_dose, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_dose\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5    7602\n",
       "1.0    7180\n",
       "0.0    7166\n",
       "Name: cp_time, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_time\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DeepInsight Transform (t-SNE)\n",
    "Based on https://github.com/alok-ai-lab/DeepInsight, but with some minor corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Modified from DeepInsight Transform\n",
    "# https://github.com/alok-ai-lab/DeepInsight/blob/master/pyDeepInsight/image_transformer.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib import pyplot as plt\n",
    "import inspect\n",
    "\n",
    "\n",
    "class DeepInsightTransformer:\n",
    "    \"\"\"Transform features to an image matrix using dimensionality reduction\n",
    "\n",
    "    This class takes in data normalized between 0 and 1 and converts it to a\n",
    "    CNN compatible 'image' matrix\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_extractor='tsne',\n",
    "                 perplexity=30,\n",
    "                 pixels=100,\n",
    "                 random_state=None,\n",
    "                 n_jobs=None):\n",
    "        \"\"\"Generate an ImageTransformer instance\n",
    "\n",
    "        Args:\n",
    "            feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n",
    "                class instance with method `fit_transform` that returns a\n",
    "                2-dimensional array of extracted features.\n",
    "            pixels: int (square matrix) or tuple of ints (height, width) that\n",
    "                defines the size of the image matrix.\n",
    "            random_state: int or RandomState. Determines the random number\n",
    "                generator, if present, of a string defined feature_extractor.\n",
    "            n_jobs: The number of parallel jobs to run for a string defined\n",
    "                feature_extractor.\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "        if isinstance(feature_extractor, str):\n",
    "            fe = feature_extractor.casefold()\n",
    "            if fe == 'tsne_exact'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='exact',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'tsne'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='barnes_hut',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'pca'.casefold():\n",
    "                fe = PCA(n_components=2, random_state=self.random_state)\n",
    "            elif fe == 'kpca'.casefold():\n",
    "                fe = KernelPCA(n_components=2,\n",
    "                               kernel='rbf',\n",
    "                               random_state=self.random_state,\n",
    "                               n_jobs=self.n_jobs)\n",
    "            else:\n",
    "                raise ValueError((\"Feature extraction method '{}' not accepted\"\n",
    "                                  ).format(feature_extractor))\n",
    "            self._fe = fe\n",
    "        elif hasattr(feature_extractor, 'fit_transform') and \\\n",
    "                inspect.ismethod(feature_extractor.fit_transform):\n",
    "            self._fe = feature_extractor\n",
    "        else:\n",
    "            raise TypeError('Parameter feature_extractor is not a '\n",
    "                            'string nor has method \"fit_transform\"')\n",
    "\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "\n",
    "        # The resolution of transformed image\n",
    "        self._pixels = pixels\n",
    "        self._xrot = None\n",
    "\n",
    "    def fit(self, X, y=None, plot=False):\n",
    "        \"\"\"Train the image transformer from the training set (X)\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            y: Ignored. Present for continuity with scikit-learn\n",
    "            plot: boolean of whether to produce a scatter plot showing the\n",
    "                feature reduction, hull points, and minimum bounding rectangle\n",
    "\n",
    "        Returns:\n",
    "            self: object\n",
    "        \"\"\"\n",
    "        # Transpose to get (n_features, n_samples)\n",
    "        X = X.T\n",
    "\n",
    "        # Perform dimensionality reduction\n",
    "        x_new = self._fe.fit_transform(X)\n",
    "\n",
    "        # Get the convex hull for the points\n",
    "        chvertices = ConvexHull(x_new).vertices\n",
    "        hull_points = x_new[chvertices]\n",
    "\n",
    "        # Determine the minimum bounding rectangle\n",
    "        mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n",
    "\n",
    "        # Rotate the matrix\n",
    "        # Save the rotated matrix in case user wants to change the pixel size\n",
    "        self._xrot = np.dot(mbr_rot, x_new.T).T\n",
    "\n",
    "        # Determine feature coordinates based on pixel dimension\n",
    "        self._calculate_coords()\n",
    "\n",
    "        # plot rotation diagram if requested\n",
    "        if plot is True:\n",
    "            # Create subplots\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n",
    "            ax[0, 0].scatter(x_new[:, 0],\n",
    "                             x_new[:, 1],\n",
    "                             cmap=plt.cm.get_cmap(\"jet\", 10),\n",
    "                             marker=\"x\",\n",
    "                             alpha=1.0)\n",
    "            ax[0, 0].fill(x_new[chvertices, 0],\n",
    "                          x_new[chvertices, 1],\n",
    "                          edgecolor='r',\n",
    "                          fill=False)\n",
    "            ax[0, 0].fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "            plt.show()\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def pixels(self):\n",
    "        \"\"\"The image matrix dimensions\n",
    "\n",
    "        Returns:\n",
    "            tuple: the image matrix dimensions (height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        return self._pixels\n",
    "\n",
    "    @pixels.setter\n",
    "    def pixels(self, pixels):\n",
    "        \"\"\"Set the image matrix dimension\n",
    "\n",
    "        Args:\n",
    "            pixels: int or tuple with the dimensions (height, width)\n",
    "            of the image matrix\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "        self._pixels = pixels\n",
    "        # recalculate coordinates if already fit\n",
    "        if hasattr(self, '_coords'):\n",
    "            self._calculate_coords()\n",
    "\n",
    "    def _calculate_coords(self):\n",
    "        \"\"\"Calculate the matrix coordinates of each feature based on the\n",
    "        pixel dimensions.\n",
    "        \"\"\"\n",
    "        ax0_coord = np.digitize(self._xrot[:, 0],\n",
    "                                bins=np.linspace(min(self._xrot[:, 0]),\n",
    "                                                 max(self._xrot[:, 0]),\n",
    "                                                 self._pixels[0])) - 1\n",
    "        ax1_coord = np.digitize(self._xrot[:, 1],\n",
    "                                bins=np.linspace(min(self._xrot[:, 1]),\n",
    "                                                 max(self._xrot[:, 1]),\n",
    "                                                 self._pixels[1])) - 1\n",
    "        self._coords = np.stack((ax0_coord, ax1_coord))\n",
    "\n",
    "    def transform(self, X, empty_value=0):\n",
    "        \"\"\"Transform the input matrix into image matrices\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "                where n_features matches the training set.\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        img_coords = pd.DataFrame(np.vstack(\n",
    "            (self._coords, X.clip(0, 1))).T).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).mean()\n",
    "\n",
    "        img_matrices = []\n",
    "        blank_mat = np.zeros(self._pixels)\n",
    "        if empty_value != 0:\n",
    "            blank_mat[:] = empty_value\n",
    "        for z in range(2, img_coords.shape[1]):\n",
    "            img_matrix = blank_mat.copy()\n",
    "            img_matrix[img_coords[0].astype(int),\n",
    "                       img_coords[1].astype(int)] = img_coords[z]\n",
    "            img_matrices.append(img_matrix)\n",
    "\n",
    "        return img_matrices\n",
    "\n",
    "    def transform_3d(self, X, empty_value=0):\n",
    "        \"\"\"Transform the input matrix into image matrices\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "                where n_features matches the training set.\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        img_coords = pd.DataFrame(np.vstack(\n",
    "            (self._coords, X.clip(0, 1))).T).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False)\n",
    "        avg_img_coords = img_coords.mean()\n",
    "        min_img_coords = img_coords.min()\n",
    "        max_img_coords = img_coords.max()\n",
    "\n",
    "        img_matrices = []\n",
    "        blank_mat = np.zeros((3, self._pixels[0], self._pixels[1]))\n",
    "        if empty_value != 0:\n",
    "            blank_mat[:, :, :] = empty_value\n",
    "        for z in range(2, avg_img_coords.shape[1]):\n",
    "            img_matrix = blank_mat.copy()\n",
    "            img_matrix[0, avg_img_coords[0].astype(int),\n",
    "                       avg_img_coords[1].astype(int)] = avg_img_coords[z]\n",
    "            img_matrix[1, min_img_coords[0].astype(int),\n",
    "                       min_img_coords[1].astype(int)] = min_img_coords[z]\n",
    "            img_matrix[2, max_img_coords[0].astype(int),\n",
    "                       max_img_coords[1].astype(int)] = max_img_coords[z]\n",
    "            img_matrices.append(img_matrix)\n",
    "\n",
    "        return img_matrices\n",
    "\n",
    "    def fit_transform(self, X, empty_value=0):\n",
    "        \"\"\"Train the image transformer from the training set (X) and return\n",
    "        the transformed data.\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X, empty_value=empty_value)\n",
    "\n",
    "    def fit_transform_3d(self, X, empty_value=0):\n",
    "        \"\"\"Train the image transformer from the training set (X) and return\n",
    "        the transformed data.\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform_3d(X, empty_value=empty_value)\n",
    "\n",
    "    def feature_density_matrix(self):\n",
    "        \"\"\"Generate image matrix with feature counts per pixel\n",
    "\n",
    "        Returns:\n",
    "            img_matrix (ndarray): matrix with feature counts per pixel\n",
    "        \"\"\"\n",
    "        fdmat = np.zeros(self._pixels)\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        coord_cnt = (\n",
    "            pd.DataFrame(self._coords.T).assign(count=1).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).count())\n",
    "        fdmat[coord_cnt[0].astype(int),\n",
    "              coord_cnt[1].astype(int)] = coord_cnt['count']\n",
    "        return fdmat\n",
    "\n",
    "    @staticmethod\n",
    "    def _minimum_bounding_rectangle(hull_points):\n",
    "        \"\"\"Find the smallest bounding rectangle for a set of points.\n",
    "\n",
    "        Modified from JesseBuesking at https://stackoverflow.com/a/33619018\n",
    "        Returns a set of points representing the corners of the bounding box.\n",
    "\n",
    "        Args:\n",
    "            hull_points : an nx2 matrix of hull coordinates\n",
    "\n",
    "        Returns:\n",
    "            (tuple): tuple containing\n",
    "                coords (ndarray): coordinates of the corners of the rectangle\n",
    "                rotmat (ndarray): rotation matrix to align edges of rectangle\n",
    "                    to x and y\n",
    "        \"\"\"\n",
    "\n",
    "        pi2 = np.pi / 2.\n",
    "\n",
    "        # Calculate edge angles\n",
    "        edges = hull_points[1:] - hull_points[:-1]\n",
    "        angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
    "        angles = np.abs(np.mod(angles, pi2))\n",
    "        angles = np.unique(angles)\n",
    "\n",
    "        # Find rotation matrices\n",
    "        rotations = np.vstack([\n",
    "            np.cos(angles),\n",
    "            np.cos(angles - pi2),\n",
    "            np.cos(angles + pi2),\n",
    "            np.cos(angles)\n",
    "        ]).T\n",
    "        rotations = rotations.reshape((-1, 2, 2))\n",
    "\n",
    "        # Apply rotations to the hull\n",
    "        rot_points = np.dot(rotations, hull_points.T)\n",
    "\n",
    "        # Find the bounding points\n",
    "        min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
    "        max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
    "        min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
    "        max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
    "\n",
    "        # Find the box with the best area\n",
    "        areas = (max_x - min_x) * (max_y - min_y)\n",
    "        best_idx = np.argmin(areas)\n",
    "\n",
    "        # Return the best box\n",
    "        x1 = max_x[best_idx]\n",
    "        x2 = min_x[best_idx]\n",
    "        y1 = max_y[best_idx]\n",
    "        y2 = min_y[best_idx]\n",
    "        rotmat = rotations[best_idx]\n",
    "\n",
    "        # Generate coordinates\n",
    "        coords = np.zeros((4, 2))\n",
    "        coords[0] = np.dot([x1, y2], rotmat)\n",
    "        coords[1] = np.dot([x2, y2], rotmat)\n",
    "        coords[2] = np.dot([x2, y1], rotmat)\n",
    "        coords[3] = np.dot([x1, y1], rotmat)\n",
    "\n",
    "        return coords, rotmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LogScaler:\n",
    "    \"\"\"Log normalize and scale data\n",
    "\n",
    "    Log normalization and scaling procedure as described as norm-2 in the\n",
    "    DeepInsight paper supplementary information.\n",
    "    \n",
    "    Note: The dimensions of input matrix is (N samples, d features)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._min0 = None\n",
    "        self._max = None\n",
    "\n",
    "    \"\"\"\n",
    "    Use this as a preprocessing step in inference mode.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "\n",
    "    \"\"\"\n",
    "    For training set only.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)\n",
    "\n",
    "    \"\"\"\n",
    "    For validation and test set only.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Adjust min. of each feature of X by _min0\n",
    "        for i in range(X.shape[1]):\n",
    "            X[:, i] = X[:, i].clip(min=self._min0[i], max=None)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MoAImageMultiTransformDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, tsne_transformer,\n",
    "                 kernel_pca_transformer, pca_transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.tsne_transformer = tsne_transformer\n",
    "        self.kernel_pca_transformer = kernel_pca_transformer\n",
    "        self.pca_transformer = pca_transformer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=1 to follow the setup in the paper\n",
    "        image = np.zeros((normalized.shape[1], normalized.shape[1], 3))\n",
    "        image[:, :, 0] = self.tsne_transformer.transform(normalized,\n",
    "                                                         empty_value=1)[0]\n",
    "        image[:, :,\n",
    "              1] = self.kernel_pca_transformer.transform(normalized,\n",
    "                                                         empty_value=1)[0]\n",
    "        image[:, :, 2] = self.pca_transformer.transform(normalized,\n",
    "                                                        empty_value=1)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image, (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        # print(image.shape)\n",
    "\n",
    "        return {\"x\": image, \"y\": self.labels[index, :]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "\n",
    "class MultiTransformTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, tsne_transformer,\n",
    "                 kernel_pca_transformer, pca_transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.tsne_transformer = tsne_transformer\n",
    "        self.kernel_pca_transformer = kernel_pca_transformer\n",
    "        self.pca_transformer = pca_transformer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=1 to follow the setup in the paper\n",
    "        image = np.zeros((normalized.shape[1], normalized.shape[1], 3))\n",
    "        image[:, :, 0] = self.tsne_transformer.transform(normalized,\n",
    "                                                         empty_value=1)[0]\n",
    "        image[:, :,\n",
    "              1] = self.kernel_pca_transformer.transform(normalized,\n",
    "                                                         empty_value=1)[0]\n",
    "        image[:, :, 2] = self.pca_transformer.transform(normalized,\n",
    "                                                        empty_value=1)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image, (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "\n",
    "        return {\"x\": image, \"y\": -1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MoAImageSwapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 features,\n",
    "                 labels,\n",
    "                 transformer,\n",
    "                 swap_prob=0.15,\n",
    "                 swap_portion=0.1):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "        self.swap_prob = swap_prob\n",
    "        self.swap_portion = swap_portion\n",
    "\n",
    "        self.crop = CropToFixedSize(width=image_size, height=image_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "\n",
    "        # Swap row featurs randomly\n",
    "        normalized = self.add_swap_noise(index, normalized)\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=0\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Crop to target size\n",
    "        #         image = (image * 255).astype(np.uint8)\n",
    "        #         image_aug = self.crop(image=image)\n",
    "        #         image = (image_aug / 255).astype(np.float32).clip(0, 1)\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image.transpose(1, 2, 0), (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "\n",
    "        return {\"x\": image, \"y\": self.labels[index, :]}\n",
    "\n",
    "    def add_swap_noise(self, index, X):\n",
    "        if np.random.rand() < self.swap_prob:\n",
    "            swap_index = np.random.randint(self.features.shape[0], size=1)[0]\n",
    "            # Select only gene expression and cell viability features\n",
    "            swap_features = np.random.choice(\n",
    "                np.array(range(3, self.features.shape[1])),\n",
    "                size=int(self.features.shape[1] * self.swap_portion),\n",
    "                replace=False)\n",
    "            X[swap_features] = self.features[swap_index, swap_features]\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MoAImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "\n",
    "\n",
    "#         self.transforms = transforms.Compose([\n",
    "#             # transforms.ToPILImage(),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=0\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image.transpose(1, 2, 0), (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "\n",
    "        return {\"x\": image, \"y\": self.labels[index, :]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=0\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image.transpose(1, 2, 0), (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "\n",
    "        return {\"x\": image, \"y\": -1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/vbmokin/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual#4.7-Smoothing\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets: torch.Tensor, n_labels: int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "                                           self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Trick to fix NaN loss in PyTorch:\n",
    "# Reference: https://www.kaggle.com/c/lish-moa/discussion/188651\n",
    "def recalibrate_layer(layer):\n",
    "\n",
    "    if (torch.isnan(layer.weight_v).sum() > 0):\n",
    "        print('recalibrate layer.weight_v')\n",
    "        layer.weight_v = torch.nn.Parameter(\n",
    "            torch.where(torch.isnan(layer.weight_v),\n",
    "                        torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "        layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "    if (torch.isnan(layer.weight).sum() > 0):\n",
    "        print('recalibrate layer.weight')\n",
    "        layer.weight = torch.where(torch.isnan(layer.weight),\n",
    "                                   torch.zeros_like(layer.weight),\n",
    "                                   layer.weight)\n",
    "        layer.weight += 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://github.com/rwightman/gen-efficientnet-pytorch/blob/master/geffnet/efficientnet_builder.py#L672\n",
    "def initialize_weight_goog(m, n='', fix_group_fanout=True):\n",
    "    # weight init as per Tensorflow Official impl\n",
    "    # https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_model.py\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        if fix_group_fanout:\n",
    "            fan_out //= m.groups\n",
    "        m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1.0)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        fan_out = m.weight.size(0)  # fan-out\n",
    "        fan_in = 0\n",
    "        if 'routing_fn' in n:\n",
    "            fan_in = m.weight.size(1)\n",
    "        init_range = 1.0 / math.sqrt(fan_in + fan_out)\n",
    "        m.weight.data.uniform_(-init_range, init_range)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def initialize_weight_default(m, n=''):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1.0)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight,\n",
    "                                 mode='fan_in',\n",
    "                                 nonlinearity='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MoAEfficientNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_model_name,\n",
    "            training_set=(None, None),  # tuple\n",
    "            valid_set=(None, None),  # tuple\n",
    "            test_set=None,\n",
    "            transformer=None,\n",
    "            num_classes=206,\n",
    "            in_chans=3,\n",
    "            drop_rate=0.,\n",
    "            drop_connect_rate=0.,\n",
    "            fc_size=512,\n",
    "            learning_rate=1e-3,\n",
    "            weight_init='goog'):\n",
    "        super(MoAEfficientNet, self).__init__()\n",
    "\n",
    "        self.train_data, self.train_labels = training_set\n",
    "        self.valid_data, self.valid_labels = valid_set\n",
    "        self.test_data = test_set\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.backbone = getattr(geffnet, pretrained_model)(\n",
    "            pretrained=True,\n",
    "            in_chans=in_chans,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_connect_rate=drop_connect_rate,\n",
    "            weight_init=weight_init)\n",
    "\n",
    "        #         self.backbone.classifier = nn.Sequential(\n",
    "        #             # recalibrate_layer(),\n",
    "        #             nn.Linear(self.backbone.classifier.in_features, fc_size,\n",
    "        #                       bias=True),\n",
    "        #             nn.ReLU(),\n",
    "        #             nn.Dropout(p=drop_rate),\n",
    "        #             # recalibrate_layer(),\n",
    "        #             nn.Linear(fc_size, fc_size, bias=True),\n",
    "        #             nn.ReLU(),\n",
    "        #             nn.Dropout(p=drop_rate),\n",
    "        #             nn.Linear(fc_size, num_classes, bias=True))\n",
    "\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Linear(self.backbone.classifier.in_features, fc_size,\n",
    "                      bias=True), nn.ELU(),\n",
    "            nn.Linear(fc_size, num_classes, bias=True))\n",
    "\n",
    "        if self.training:\n",
    "            for m in self.backbone.classifier.modules():\n",
    "                initialize_weight_goog(m)\n",
    "\n",
    "        # Save passed hyperparameters\n",
    "        self.save_hyperparameters(\"pretrained_model_name\", \"num_classes\",\n",
    "                                  \"in_chans\", \"drop_rate\", \"drop_connect_rate\",\n",
    "                                  \"weight_init\", \"fc_size\", \"learning_rate\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "\n",
    "        # loss = F.binary_cross_entropy_with_logits(logits, y, reduction=\"mean\")\n",
    "\n",
    "        # Label smoothing\n",
    "        loss = SmoothBCEwLogits(smoothing=label_smoothing)(logits, y)\n",
    "\n",
    "        self.log('train_loss',\n",
    "                 loss,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "\n",
    "        val_loss = F.binary_cross_entropy_with_logits(logits,\n",
    "                                                      y,\n",
    "                                                      reduction=\"mean\")\n",
    "\n",
    "        self.log('val_loss',\n",
    "                 val_loss,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=True)\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "        return {\"pred_logits\": logits}\n",
    "\n",
    "    def test_epoch_end(self, output_results):\n",
    "        all_outputs = torch.cat([out[\"pred_logits\"] for out in output_results],\n",
    "                                dim=0)\n",
    "        print(\"Logits:\", all_outputs)\n",
    "        pred_probs = F.sigmoid(all_outputs).detach().cpu().numpy()\n",
    "        print(\"Predictions: \", pred_probs)\n",
    "        return {\"pred_probs\": pred_probs}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        #         self.train_dataset = MoAImageDataset(self.train_data,\n",
    "        #                                              self.train_labels,\n",
    "        #                                              self.transformer)\n",
    "        self.train_dataset = MoAImageSwapDataset(self.train_data,\n",
    "                                                 self.train_labels,\n",
    "                                                 self.transformer,\n",
    "                                                 swap_prob=swap_prob,\n",
    "                                                 swap_portion=swap_portion)\n",
    "\n",
    "        self.val_dataset = MoAImageDataset(self.valid_data, self.valid_labels,\n",
    "                                           self.transformer)\n",
    "\n",
    "        self.test_dataset = TestDataset(self.test_data, None, self.transformer)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = DataLoader(self.train_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=num_workers,\n",
    "                                      pin_memory=True,\n",
    "                                      drop_last=False)\n",
    "        print(f\"Train iterations: {len(train_dataloader)}\")\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = DataLoader(self.val_dataset,\n",
    "                                    batch_size=infer_batch_size,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=num_workers,\n",
    "                                    pin_memory=True,\n",
    "                                    drop_last=False)\n",
    "        print(f\"Validate iterations: {len(val_dataloader)}\")\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataloader = DataLoader(self.test_dataset,\n",
    "                                     batch_size=infer_batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=num_workers,\n",
    "                                     pin_memory=True,\n",
    "                                     drop_last=False)\n",
    "        print(f\"Test iterations: {len(test_dataloader)}\")\n",
    "        return test_dataloader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print(f\"Initial Learning Rate: {self.hparams.learning_rate:.6f}\")\n",
    "        #         optimizer = optim.Adam(self.parameters(),\n",
    "        #                                lr=self.hparams.learning_rate,\n",
    "        #                                weight_decay=weight_decay)\n",
    "        #         optimizer = torch.optim.SGD(self.parameters(),\n",
    "        #                                     lr=self.hparams.learning_rate,\n",
    "        #                                     momentum=0.9,\n",
    "        #                                     dampening=0,\n",
    "        #                                     weight_decay=weight_decay,\n",
    "        #                                     nesterov=False)\n",
    "\n",
    "        optimizer = torch_optimizer.RAdam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                         T_max=T_max,\n",
    "                                                         eta_min=0,\n",
    "                                                         last_epoch=-1)\n",
    "\n",
    "        #         scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        #             optimizer,\n",
    "        #             T_0=T_0,\n",
    "        #             T_mult=1,\n",
    "        #             eta_min=0,\n",
    "        #             last_epoch=-1)\n",
    "\n",
    "        #         scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        #             optimizer=optimizer,\n",
    "        #             pct_start=0.1,\n",
    "        #             div_factor=1e3,\n",
    "        #             max_lr=1e-1,\n",
    "        #             # max_lr=1e-2,\n",
    "        #             epochs=epochs,\n",
    "        #             steps_per_epoch=len(self.train_images) // batch_size)\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = MoAEfficientNet(\n",
    "#     pretrained_model,\n",
    "#     training_set=(None, None),  # tuple\n",
    "#     valid_set=(None, None),  # tuple\n",
    "#     test_set=None,\n",
    "#     transformer=None,\n",
    "#     num_classes=len(train_classes),\n",
    "#     in_chans=3,\n",
    "#     drop_rate=drop_rate,\n",
    "#     drop_connect_rate=drop_connect_rate,\n",
    "#     fc_size=fc_size,\n",
    "#     learning_rate=learning_rate,\n",
    "#     weight_init='goog')\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## New CV Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kfolds = 10\n",
    "# skf = MultilabelStratifiedKFold(n_splits=kfolds,\n",
    "#                                 shuffle=True,\n",
    "#                                 random_state=rand_seed)\n",
    "\n",
    "# label_counts = np.sum(train_labels.drop(\"sig_id\", axis=1), axis=0)\n",
    "# y_labels = label_counts.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scored = (train_features[['sig_id', 'drug_id']]).merge(train_labels,\n",
    "                                                       on='sig_id')\n",
    "targets = scored.columns[2:]\n",
    "\n",
    "vc = train_features.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc <= 18].index\n",
    "vc2 = vc.loc[vc > 18].index\n",
    "folds = train_features.copy()\n",
    "\n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}\n",
    "dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits=kfolds, shuffle=True, random_state=34)\n",
    "tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n",
    "for fold, (idxT, idxV) in enumerate(skf.split(tmp, tmp[targets])):\n",
    "    dd = {k: fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits=kfolds, shuffle=True, random_state=34)\n",
    "tmp = scored.loc[train_features.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "for fold, (idxT, idxV) in enumerate(skf.split(tmp, tmp[targets])):\n",
    "    dd = {k: fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "folds['kfold'] = folds.drug_id.map(dct1)\n",
    "folds.loc[folds.kfold.isna(),'kfold'] =\\\n",
    "    folds.loc[folds.kfold.isna(),'sig_id'].map(dct2)\n",
    "folds.kfold = folds.kfold.astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(training_set, valid_set, test_set, transformer, model_path=None):\n",
    "    if training_mode:\n",
    "        model = MoAEfficientNet(\n",
    "            pretrained_model_name=pretrained_model,\n",
    "            training_set=training_set,  # tuple\n",
    "            valid_set=valid_set,  # tuple\n",
    "            test_set=test_set,\n",
    "            transformer=transformer,\n",
    "            num_classes=len(train_classes),\n",
    "            in_chans=3,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_connect_rate=drop_connect_rate,\n",
    "            fc_size=fc_size,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_init='goog')\n",
    "    else:\n",
    "        model = MoAEfficientNet.load_from_checkpoint(\n",
    "            model_path,\n",
    "            pretrained_model_name=pretrained_model,\n",
    "            training_set=training_set,  # tuple\n",
    "            valid_set=valid_set,  # tuple\n",
    "            test_set=test_set,\n",
    "            transformer=transformer,\n",
    "            num_classes=len(train_classes),\n",
    "            fc_size=fc_size)\n",
    "        model.freeze()\n",
    "        model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_pickle(obj, model_output_folder, fold_i, name):\n",
    "    dump(obj, open(f\"{model_output_folder}/fold{fold_i}_{name}.pkl\", 'wb'),\n",
    "         pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_pickle(model_output_folder, fold_i, name):\n",
    "    return load(open(f\"{model_output_folder}/fold{fold_i}_{name}.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm2_normalization(train, valid, test):\n",
    "    scaler = LogScaler()\n",
    "    train = scaler.fit_transform(train)\n",
    "    valid = scaler.transform(valid)\n",
    "    test = scaler.transform(test)\n",
    "    return train, valid, test, scaler\n",
    "\n",
    "\n",
    "def extract_feature_map(train,\n",
    "                        feature_extractor='tsne_exact',\n",
    "                        resolution=100,\n",
    "                        perplexity=30):\n",
    "    transformer = DeepInsightTransformer(feature_extractor=feature_extractor,\n",
    "                                         pixels=resolution,\n",
    "                                         perplexity=perplexity,\n",
    "                                         random_state=rand_seed,\n",
    "                                         n_jobs=-1)\n",
    "    transformer.fit(train)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_logloss(y_pred, y_true):\n",
    "    logloss = (1 - y_true) * np.log(1 - y_pred +\n",
    "                                    1e-15) + y_true * np.log(y_pred + 1e-15)\n",
    "    return np.mean(-logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Fold 0 ......\n",
      "(19764,) (2184,)\n",
      "Running norm-2 normalization ......\n",
      "Extracting feature map ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | backbone | GenEfficientNet | 4 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Learning Rate: 0.000382\n",
      "Validate iterations: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iterations: 309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbae5b0074eb406e96b9ab90269a29c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: val_loss_epoch reached 0.02223 (best 0.02223), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold0/epoch=0-train_loss_epoch=0.000000-val_loss_epoch=0.022230-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss_epoch reached 0.02102 (best 0.02102), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold0/epoch=1-train_loss_epoch=0.137626-val_loss_epoch=0.021019-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss_epoch reached 0.02062 (best 0.02062), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold0/epoch=2-train_loss_epoch=0.024215-val_loss_epoch=0.020617-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: val_loss_epoch reached 0.01895 (best 0.01895), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold0/epoch=7-train_loss_epoch=0.022386-val_loss_epoch=0.018953-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: val_loss_epoch reached 0.01854 (best 0.01854), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold0/epoch=11-train_loss_epoch=0.021164-val_loss_epoch=0.018543-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: val_loss_epoch reached 0.01817 (best 0.01817), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold0/epoch=12-train_loss_epoch=0.020720-val_loss_epoch=0.018169-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: val_loss_epoch reached 0.01798 (best 0.01798), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold0/epoch=13-train_loss_epoch=0.020272-val_loss_epoch=0.017976-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting on validation set ......\n",
      "Validate iterations: 18\n",
      "Initial Learning Rate: 0.000382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de17a7a4d214e65a93df9a039887c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.8516, -7.0977, -6.6641,  ..., -7.1836, -7.4688, -6.8398],\n",
      "        [-6.1172, -6.1289, -7.1211,  ..., -6.2383, -6.8828, -6.3164],\n",
      "        [-7.1992, -7.4414, -5.5000,  ..., -5.8281, -5.4453, -5.5234],\n",
      "        ...,\n",
      "        [-6.9688, -6.6562, -6.9570,  ..., -6.0781, -6.4180, -6.2930],\n",
      "        [-6.0859, -5.9805, -6.4453,  ..., -7.0547, -6.5195, -7.0703],\n",
      "        [-8.7891, -8.2109, -6.2305,  ..., -6.3359, -5.6211, -6.4492]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[0.001057  0.0008264 0.001274  ... 0.000758  0.0005703 0.001069 ]\n",
      " [0.0022    0.002174  0.0008073 ... 0.001949  0.001024  0.001803 ]\n",
      " [0.0007467 0.000586  0.00407   ... 0.002935  0.0043    0.003975 ]\n",
      " ...\n",
      " [0.00094   0.001285  0.000951  ... 0.002287  0.001629  0.001846 ]\n",
      " [0.00227   0.002522  0.001585  ... 0.0008626 0.001472  0.0008492]\n",
      " [0.0001523 0.0002716 0.001965  ... 0.001768  0.003607  0.001579 ]]\n",
      "\n",
      "[[0.001057  0.0008264 0.001274  ... 0.000758  0.0005703 0.001069 ]\n",
      " [0.0022    0.002174  0.0008073 ... 0.001949  0.001024  0.001803 ]\n",
      " [0.0007467 0.000586  0.00407   ... 0.002935  0.0043    0.003975 ]\n",
      " [0.003664  0.003063  0.001151  ... 0.001661  0.002165  0.001889 ]\n",
      " [0.002779  0.00209   0.001169  ... 0.001734  0.0022    0.001611 ]]\n",
      "Fold 0 Validation Loss: nan\n",
      "Predicting on test set ......\n",
      "Test iterations: 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715bb01b636d4b7c965dc5fa9e33207c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.5234, -6.3516, -6.5234,  ..., -5.9688, -5.5625, -6.1914],\n",
      "        [-8.9922, -7.6289, -6.9688,  ..., -7.2383, -5.1172, -6.9766],\n",
      "        [-7.5664, -7.4492, -5.8203,  ..., -5.0781, -4.7930, -5.2031],\n",
      "        ...,\n",
      "        [-6.1523, -6.0195, -7.1992,  ..., -6.6016, -7.1094, -6.7383],\n",
      "        [-6.4883, -6.8789, -6.3281,  ..., -6.1836, -5.7188, -5.9453],\n",
      "        [-6.4531, -6.8828, -7.2695,  ..., -6.1914, -6.9766, -6.6211]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[0.001467  0.001741  0.001467  ... 0.00255   0.003824  0.002043 ]\n",
      " [0.0001243 0.000486  0.00094   ... 0.000718  0.00596   0.0009327]\n",
      " [0.0005174 0.0005817 0.002958  ... 0.00619   0.00822   0.00547  ]\n",
      " ...\n",
      " [0.002125  0.002424  0.0007467 ... 0.001356  0.000817  0.0011835]\n",
      " [0.001519  0.001028  0.001782  ... 0.002058  0.003273  0.002611 ]\n",
      " [0.001574  0.001024  0.000696  ... 0.002043  0.0009327 0.00133  ]]\n",
      "\n",
      "(3982, 876) (3982, 206)\n",
      "Training on Fold 1 ......\n",
      "(19800,) (2148,)\n",
      "Running norm-2 normalization ......\n",
      "Extracting feature map ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n",
      "Missing logger folder: /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold1/logs\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | backbone | GenEfficientNet | 4 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Learning Rate: 0.000382\n",
      "Validate iterations: 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iterations: 310\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcf038ac9694318b189417a2ad249c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: val_loss_epoch reached 0.01940 (best 0.01940), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold1/epoch=4-train_loss_epoch=0.022906-val_loss_epoch=0.019402-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: val_loss_epoch reached 0.01891 (best 0.01891), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold1/epoch=10-train_loss_epoch=0.021536-val_loss_epoch=0.018910-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 21: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss_epoch reached 0.02046 (best 0.02046), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold2/epoch=1-train_loss_epoch=0.134810-val_loss_epoch=0.020457-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: val_loss_epoch reached 0.01863 (best 0.01863), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold2/epoch=6-train_loss_epoch=0.022281-val_loss_epoch=0.018630-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: val_loss_epoch reached 0.01801 (best 0.01801), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold2/epoch=11-train_loss_epoch=0.021256-val_loss_epoch=0.018015-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: val_loss_epoch reached 0.01735 (best 0.01735), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold2/epoch=15-train_loss_epoch=0.019393-val_loss_epoch=0.017347-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss_epoch reached 0.02059 (best 0.02059), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold3/epoch=1-train_loss_epoch=0.135947-val_loss_epoch=0.020590-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss_epoch reached 0.02003 (best 0.02003), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold3/epoch=2-train_loss_epoch=0.024218-val_loss_epoch=0.020028-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: val_loss_epoch reached 0.01889 (best 0.01889), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold3/epoch=6-train_loss_epoch=0.022542-val_loss_epoch=0.018894-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: val_loss_epoch reached 0.01809 (best 0.01809), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold3/epoch=11-train_loss_epoch=0.021301-val_loss_epoch=0.018091-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: val_loss_epoch reached 0.01746 (best 0.01746), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold3/epoch=12-train_loss_epoch=0.020897-val_loss_epoch=0.017463-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: val_loss_epoch was not in top 1\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting on validation set ......\n",
      "Validate iterations: 18\n",
      "Initial Learning Rate: 0.000382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888496a572064963bb345e4c95c1cdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -6.5820,  -6.8047,  -6.1016,  ...,  -5.8516,  -6.1875,  -5.6328],\n",
      "        [ -6.5117,  -7.0742,  -5.4180,  ...,  -6.2109,  -4.8008,  -4.8867],\n",
      "        [-10.0547,  -8.2422,  -8.7812,  ..., -10.5234, -11.0078,  -9.6094],\n",
      "        ...,\n",
      "        [ -6.8320,  -6.8945,  -6.1055,  ...,  -6.2188,  -5.8359,  -5.1992],\n",
      "        [ -6.0781,  -6.6289,  -6.4336,  ...,  -6.4297,  -6.4336,  -6.2188],\n",
      "        [ -7.3320,  -6.4492,  -6.7422,  ...,  -8.2031,  -7.4570,  -7.4023]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[1.383e-03 1.107e-03 2.234e-03 ... 2.867e-03 2.050e-03 3.565e-03]\n",
      " [1.484e-03 8.459e-04 4.417e-03 ... 2.003e-03 8.156e-03 7.488e-03]\n",
      " [4.297e-05 2.632e-04 1.535e-04 ... 2.688e-05 1.657e-05 6.711e-05]\n",
      " ...\n",
      " [1.078e-03 1.012e-03 2.226e-03 ... 1.987e-03 2.913e-03 5.489e-03]\n",
      " [2.287e-03 1.320e-03 1.604e-03 ... 1.611e-03 1.604e-03 1.987e-03]\n",
      " [6.537e-04 1.579e-03 1.179e-03 ... 2.737e-04 5.770e-04 6.094e-04]]\n",
      "\n",
      "[[1.383e-03 1.107e-03 2.234e-03 ... 2.867e-03 2.050e-03 3.565e-03]\n",
      " [1.484e-03 8.459e-04 4.417e-03 ... 2.003e-03 8.156e-03 7.488e-03]\n",
      " [4.297e-05 2.632e-04 1.535e-04 ... 2.688e-05 1.657e-05 6.711e-05]\n",
      " [1.875e-03 1.714e-03 1.592e-03 ... 1.519e-03 1.086e-03 1.531e-03]\n",
      " [1.912e-03 1.912e-03 1.972e-03 ... 2.182e-03 2.003e-03 2.531e-03]]\n",
      "Fold 3 Validation Loss: 0.016998\n",
      "Predicting on test set ......\n",
      "Test iterations: 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54e6aa5dc0541429ddcedff3be7f2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.0625, -6.1367, -6.2227,  ..., -6.0898, -7.0078, -6.2812],\n",
      "        [-6.4883, -6.2188, -6.5703,  ..., -6.5156, -6.5898, -6.1094],\n",
      "        [-6.7852, -7.1328, -5.8164,  ..., -5.8477, -6.8984, -5.3477],\n",
      "        ...,\n",
      "        [-6.5430, -6.5039, -6.4766,  ..., -6.3984, -6.6172, -6.5039],\n",
      "        [-6.0859, -6.6523, -6.4414,  ..., -6.0898, -6.8281, -6.0312],\n",
      "        [-6.4961, -6.4258, -6.4336,  ..., -6.2891, -6.9453, -6.4258]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[0.002323  0.002157  0.00198   ... 0.00226   0.000904  0.001867 ]\n",
      " [0.001519  0.001987  0.001399  ... 0.001478  0.001372  0.002216 ]\n",
      " [0.001129  0.0007977 0.00297   ... 0.002878  0.001008  0.004738 ]\n",
      " ...\n",
      " [0.001438  0.001495  0.001536  ... 0.001661  0.001335  0.001495 ]\n",
      " [0.00227   0.001289  0.001592  ... 0.00226   0.001081  0.002398 ]\n",
      " [0.001507  0.0016165 0.001604  ... 0.001853  0.0009623 0.0016165]]\n",
      "\n",
      "(3982, 876) (3982, 206)\n",
      "Training on Fold 4 ......\n",
      "(19762,) (2186,)\n",
      "Running norm-2 normalization ......\n",
      "Extracting feature map ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n",
      "Missing logger folder: /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold4/logs\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | backbone | GenEfficientNet | 4 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Learning Rate: 0.000382\n",
      "Validate iterations: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iterations: 309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8093f060456746d8b927109fdac94da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: val_loss_epoch reached 0.02148 (best 0.02148), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold4/epoch=0-train_loss_epoch=0.000000-val_loss_epoch=0.021482-image_size=224-resolution=100-perplexity=5-fc=512.ckpt as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03973315cd10453881e01912e42f86ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss_epoch was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting on validation set ......\n",
      "Validate iterations: 18\n",
      "Initial Learning Rate: 0.000382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21945fc19c3d43aea8a3036152309892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f89d392e950>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 221, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.0898, -6.2930, -6.3086,  ..., -5.8320, -6.3125, -6.4219],\n",
      "        [-6.0234, -6.2383, -6.2422,  ..., -5.8164, -6.2578, -6.3789],\n",
      "        [-5.9727, -6.1875, -6.1484,  ..., -5.7617, -6.1875, -6.3125],\n",
      "        ...,\n",
      "        [-5.8320, -6.0391, -6.0430,  ..., -5.6367, -6.0586, -6.1836],\n",
      "        [-6.0234, -6.1914, -6.1758,  ..., -5.7852, -6.2188, -6.3477],\n",
      "        [-6.0977, -6.2969, -6.3008,  ..., -5.8477, -6.3047, -6.4492]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[0.00226  0.001846 0.001818 ... 0.002924 0.00181  0.001623]\n",
      " [0.002415 0.001949 0.001942 ... 0.00297  0.001912 0.001694]\n",
      " [0.00254  0.00205  0.002132 ... 0.003136 0.00205  0.00181 ]\n",
      " ...\n",
      " [0.002924 0.002378 0.002369 ... 0.003551 0.002333 0.002058]\n",
      " [0.002415 0.002043 0.002075 ... 0.003063 0.001987 0.001748]\n",
      " [0.002243 0.001839 0.001831 ... 0.002878 0.001824 0.001579]]\n",
      "\n",
      "[[0.00226  0.001846 0.001818 ... 0.002924 0.00181  0.001623]\n",
      " [0.002415 0.001949 0.001942 ... 0.00297  0.001912 0.001694]\n",
      " [0.00254  0.00205  0.002132 ... 0.003136 0.00205  0.00181 ]\n",
      " [0.002388 0.001965 0.001926 ... 0.003075 0.001853 0.001775]\n",
      " [0.002443 0.00198  0.00201  ... 0.0031   0.001912 0.001727]]\n",
      "Fold 4 Validation Loss: 0.021484\n",
      "Predicting on test set ......\n",
      "Test iterations: 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed4bded6f0f4d80844e19de83f354dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.0156, -6.2656, -6.2148,  ..., -5.8086, -6.2344, -6.3555],\n",
      "        [-6.0039, -6.2109, -6.1992,  ..., -5.7500, -6.2305, -6.3594],\n",
      "        [-5.9883, -6.2227, -6.2031,  ..., -5.7852, -6.2188, -6.3750],\n",
      "        ...,\n",
      "        [-6.1094, -6.3203, -6.3047,  ..., -5.8633, -6.3320, -6.4688],\n",
      "        [-6.0586, -6.2891, -6.2773,  ..., -5.8672, -6.3008, -6.4648],\n",
      "        [-6.0781, -6.2656, -6.2422,  ..., -5.8320, -6.2930, -6.4375]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Predictions:  [[0.002434  0.001897  0.001995  ... 0.002993  0.001957  0.001734 ]\n",
      " [0.002462  0.002003  0.002028  ... 0.003172  0.001965  0.001727 ]\n",
      " [0.002502  0.00198   0.00202   ... 0.003063  0.001987  0.0017   ]\n",
      " ...\n",
      " [0.002216  0.001796  0.001824  ... 0.002834  0.001775  0.001549 ]\n",
      " [0.002333  0.001853  0.001875  ... 0.002823  0.001831  0.0015545]\n",
      " [0.002287  0.001897  0.001942  ... 0.002924  0.001846  0.001597 ]]\n",
      "\n",
      "(3982, 876) (3982, 206)\n",
      "Training on Fold 5 ......\n",
      "(19743,) (2205,)\n",
      "Running norm-2 normalization ......\n",
      "Extracting feature map ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n",
      "Missing logger folder: /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold5/logs\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | backbone | GenEfficientNet | 4 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Learning Rate: 0.000382\n",
      "Validate iterations: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iterations: 309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1710a23fd1c141b987a1acd7e4f0d7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure Reproducibility\n",
    "seed_everything(rand_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "best_model = None\n",
    "oof_predictions = np.zeros((train_features.shape[0], len(train_classes)))\n",
    "kfold_submit_preds = np.zeros((test_features.shape[0], len(train_classes)))\n",
    "# for i, (train_index, val_index) in enumerate(\n",
    "#         skf.split(train_features, train_labels[y_labels])):\n",
    "for i in range(kfolds):\n",
    "    train_index = folds[folds['kfold'] != i].index\n",
    "    val_index = folds[folds['kfold'] == i].index\n",
    "\n",
    "    if training_mode:\n",
    "        print(f\"Training on Fold {i} ......\")\n",
    "        print(train_index.shape, val_index.shape)\n",
    "\n",
    "        logger = TensorBoardLogger(model_output_folder,\n",
    "                                   name=f\"fold{i}/logs\",\n",
    "                                   default_hp_metric=False)\n",
    "\n",
    "        train = train_features.loc[train_index, all_features].copy().values\n",
    "        fold_train_labels = train_labels.loc[train_index,\n",
    "                                             train_classes].copy().values\n",
    "        valid = train_features.loc[val_index, all_features].copy().values\n",
    "        fold_valid_labels = train_labels.loc[val_index,\n",
    "                                             train_classes].copy().values\n",
    "        test = test_features[all_features].copy().values\n",
    "\n",
    "        # LogScaler (Norm-2 Normalization)\n",
    "        print(\"Running norm-2 normalization ......\")\n",
    "        train, valid, test, scaler = norm2_normalization(train, valid, test)\n",
    "        save_pickle(scaler, model_output_folder, i, \"log-scaler\")\n",
    "\n",
    "        # Extract DeepInsight Feature Map\n",
    "        print(\"Extracting feature map ......\")\n",
    "        transformer = extract_feature_map(train,\n",
    "                                          feature_extractor='tsne_exact',\n",
    "                                          resolution=resolution,\n",
    "                                          perplexity=perplexity)\n",
    "        save_pickle(transformer, model_output_folder, i,\n",
    "                    \"deepinsight-transform\")\n",
    "\n",
    "        model = get_model(training_set=(train, fold_train_labels),\n",
    "                          valid_set=(valid, fold_valid_labels),\n",
    "                          test_set=test,\n",
    "                          transformer=transformer)\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss_epoch',\n",
    "                          min_delta=1e-6,\n",
    "                          patience=patience,\n",
    "                          verbose=True,\n",
    "                          mode='min',\n",
    "                          strict=True),\n",
    "            LearningRateMonitor(logging_interval='step')\n",
    "        ]\n",
    "        # https://pytorch-lightning.readthedocs.io/en/latest/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            filepath=f\"{model_output_folder}/fold{i}\" +\n",
    "            \"/{epoch}-{train_loss_epoch:.6f}-{val_loss_epoch:.6f}\" +\n",
    "            f\"-image_size={image_size}-resolution={resolution}-perplexity={perplexity}-fc={fc_size}\",\n",
    "            save_top_k=1,\n",
    "            save_weights_only=False,\n",
    "            save_last=False,\n",
    "            verbose=True,\n",
    "            monitor='val_loss_epoch',\n",
    "            mode='min',\n",
    "            prefix='')\n",
    "\n",
    "        if debug_mode:\n",
    "            # Find best LR\n",
    "            # https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html\n",
    "            trainer = Trainer(\n",
    "                gpus=[gpus[0]],\n",
    "                distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "                auto_lr_find=True,\n",
    "                benchmark=False,\n",
    "                deterministic=True,\n",
    "                logger=logger,\n",
    "                accumulate_grad_batches=accumulate_grad_batches,\n",
    "                gradient_clip_val=gradient_clip_val,\n",
    "                precision=16,\n",
    "                max_epochs=1)\n",
    "\n",
    "            # Run learning rate finder\n",
    "            lr_finder = trainer.tuner.lr_find(\n",
    "                model,\n",
    "                min_lr=1e-7,\n",
    "                max_lr=1e2,\n",
    "                num_training=500,\n",
    "                mode='exponential',\n",
    "                early_stop_threshold=10.0,\n",
    "            )\n",
    "            fig = lr_finder.plot(suggest=True)\n",
    "            fig.show()\n",
    "\n",
    "            # Pick point based on plot, or get suggestion\n",
    "            suggested_lr = lr_finder.suggestion()\n",
    "\n",
    "            # Update hparams of the model\n",
    "            model.hparams.learning_rate = suggested_lr\n",
    "            print(\n",
    "                f\"Suggested Learning Rate: {model.hparams.learning_rate:.6f}\")\n",
    "\n",
    "            # trainer.fit(model)\n",
    "        else:\n",
    "            trainer = Trainer(\n",
    "                gpus=gpus,\n",
    "                distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "                max_epochs=epochs,\n",
    "                benchmark=False,\n",
    "                deterministic=True,\n",
    "                # fast_dev_run=True,\n",
    "                checkpoint_callback=checkpoint_callback,\n",
    "                callbacks=callbacks,\n",
    "                accumulate_grad_batches=accumulate_grad_batches,\n",
    "                gradient_clip_val=gradient_clip_val,\n",
    "                precision=16,\n",
    "                logger=logger)\n",
    "            trainer.fit(model)\n",
    "\n",
    "            # Load best model\n",
    "            seed_everything(rand_seed)\n",
    "            best_model = MoAEfficientNet.load_from_checkpoint(\n",
    "                checkpoint_callback.best_model_path,\n",
    "                pretrained_model_name=pretrained_model,\n",
    "                training_set=(train, fold_train_labels),  # tuple\n",
    "                valid_Set=(valid, fold_valid_labels),  # tuple\n",
    "                test_set=test,\n",
    "                transformer=transformer,\n",
    "                fc_size=fc_size)\n",
    "            best_model.freeze()\n",
    "\n",
    "            print(\"Predicting on validation set ......\")\n",
    "            output = trainer.test(ckpt_path=\"best\",\n",
    "                                  test_dataloaders=model.val_dataloader(),\n",
    "                                  verbose=False)[0]\n",
    "            fold_preds = output[\"pred_probs\"]\n",
    "            oof_predictions[val_index, :] = fold_preds\n",
    "\n",
    "            print(fold_preds[:5, :])\n",
    "            fold_valid_loss = mean_logloss(fold_preds, fold_valid_labels)\n",
    "            print(f\"Fold {i} Validation Loss: {fold_valid_loss:.6f}\")\n",
    "\n",
    "            # Generate submission predictions\n",
    "            print(\"Predicting on test set ......\")\n",
    "            best_model.setup()\n",
    "            output = trainer.test(best_model, verbose=False)[0]\n",
    "            submit_preds = output[\"pred_probs\"]\n",
    "            print(test_features.shape, submit_preds.shape)\n",
    "\n",
    "            kfold_submit_preds += submit_preds / kfolds\n",
    "\n",
    "        del model, trainer, train, valid, test, scaler, transformer\n",
    "    else:\n",
    "        print(f\"Inferencing on Fold {i} ......\")\n",
    "        print(train_index.shape, val_index.shape)\n",
    "\n",
    "        model_path = glob.glob(f'{model_output_folder}/fold{i}/epoch*.ckpt')[0]\n",
    "\n",
    "        test = test_features[all_features].copy().values\n",
    "\n",
    "        # Load LogScaler (Norm-2 Normalization)\n",
    "        scaler = load_pickle(f'{model_output_folder}', i, \"log-scaler\")\n",
    "        test = scaler.transform(test)\n",
    "\n",
    "        # Load DeepInsight Feature Map\n",
    "        transformer = load_pickle(f'{model_output_folder}', i,\n",
    "                                  \"deepinsight-transform\")\n",
    "\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        model = get_model(training_set=(None, None),\n",
    "                          valid_set=(None, None),\n",
    "                          test_set=test,\n",
    "                          transformer=transformer,\n",
    "                          model_path=model_path)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            logger=False,\n",
    "            gpus=gpus,\n",
    "            distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "            precision=16,\n",
    "            benchmark=False,\n",
    "            deterministic=True)\n",
    "        output = trainer.test(model, verbose=False)[0]\n",
    "        submit_preds = output[\"pred_probs\"]\n",
    "        kfold_submit_preds += submit_preds / kfolds\n",
    "\n",
    "        del model, trainer, scaler, transformer, test\n",
    "\n",
    "    if debug_mode:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [B0]\n",
    "# resolution=100, image_size=200\n",
    "# Epoch 15: val_loss_epoch reached 0.01766 (best 0.01766)\n",
    "# Epoch 14: val_loss_epoch reached 0.01777 (best 0.01777), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold0/epoch=14-train_loss_e\n",
    "# poch=0.019808-val_loss_epoch=0.017769-image_size=200-resolution=100-perplexity=5-fc=512.ckpt as top 1\n",
    "\n",
    "# resolution=100, image_size=150\n",
    "# Epoch 16: val_loss_epoch reached 0.01817 (best 0.01817), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold0/epoch=16-train_loss_epoch=0.019331-val_loss_epoch=0.018169-image_size=150-resolution=100-perplexity=5-fc=512.ckpt as top 1\n",
    "\n",
    "# resolution=200, image_size=224\n",
    "# Epoch 15: val_loss_epoch reached 0.01765 (best 0.01765), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b0_drug_id/fold0/epoch=15-train_loss_epoch=0.019586-val_loss_epoch=0.017654-image_size=224-resolution=200-perplexity=5-fc=512.ckpt as top 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolution=300, image_size=300\n",
    "# CV: 0.017893\n",
    "\n",
    "# resolution=100, image_size=100\n",
    "# Epoch 26: val_loss_epoch reached 0.01801 (best 0.01801), saving model to /workspace/Kaggle/MoA/deepinsight_efficientnet_v9_b3_drug_id/fold0/epoch=26-t\n",
    "\n",
    "# resolution=100, image_size=200\n",
    "# Epoch 14: val_loss_epoch reached 0.01714 (best 0.01714)\n",
    "\n",
    "# resolution=100, image_size=300\n",
    "# Epoch 15: val_loss_epoch reached 0.01724 (best 0.01724)\n",
    "\n",
    "# resolution=50, image_size=200\n",
    "# Epoch 14: val_loss_epoch reached 0.01833 (best 0.01833)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_mode:\n",
    "    print(oof_predictions.shape)\n",
    "else:\n",
    "    oof_predictions = glob.glob(f'{model_output_folder}/../oof_*.npy')[0]\n",
    "    oof_predictions = np.load(oof_predictions)\n",
    "\n",
    "oof_loss = mean_logloss(oof_predictions,\n",
    "                        train_labels[train_classes].values)\n",
    "print(f\"OOF Validation Loss: {oof_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [V9] B0\n",
    "# OOF Validation Loss: 0.017649\n",
    "# \"drop_connect_rate\":     0.2\n",
    "# \"drop_rate\":             0.3\n",
    "# \"fc_size\":               512\n",
    "# \"in_chans\":              3\n",
    "# \"learning_rate\":         0.000366\n",
    "# \"num_classes\":           206\n",
    "# \"pretrained_model_name\": tf_efficientnet_b0_ns\n",
    "# \"weight_init\":           goog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [V7 New CV without control groups]\n",
    "# OOF Validation Loss: 0.017633\n",
    "# \"drop_connect_rate\":     0.2\n",
    "# \"drop_rate\":             0.3\n",
    "# \"fc_size\":               512\n",
    "# \"in_chans\":              3\n",
    "# \"learning_rate\":         0.000366\n",
    "# \"num_classes\":           206\n",
    "# \"pretrained_model_name\": tf_efficientnet_b3_ns\n",
    "# \"weight_init\":           goog\n",
    "# (21948, 206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Avg/Min/Max Channels]\n",
    "# OOF Validation Loss: 0.014802\n",
    "# \"drop_connect_rate\":     0.2\n",
    "# \"drop_rate\":             0.3\n",
    "# \"fc_size\":               512\n",
    "# \"in_chans\":              3\n",
    "# \"learning_rate\":         0.000352\n",
    "# \"num_classes\":           206\n",
    "# \"pretrained_model_name\": tf_efficientnet_b3_ns\n",
    "# \"weight_init\":           goog\n",
    "# (23814, 206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ELU]\n",
    "# OOF Validation Loss: 0.014932\n",
    "# \"drop_connect_rate\":     0.2\n",
    "# \"drop_rate\":             0.3\n",
    "# \"fc_size\":               512\n",
    "# \"in_chans\":              3\n",
    "# \"learning_rate\":         0.000282\n",
    "# \"num_classes\":           206\n",
    "# \"pretrained_model_name\": tf_efficientnet_b3_ns\n",
    "# \"weight_init\":           goog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [SELU]\n",
    "# OOF Validation Loss: 0.014948\n",
    "# \"drop_connect_rate\":     0.2\n",
    "# \"drop_rate\":             0.3\n",
    "# \"fc_size\":               512\n",
    "# \"in_chans\":              3\n",
    "# \"learning_rate\":         0.000282\n",
    "# \"num_classes\":           206\n",
    "# \"pretrained_model_name\": tf_efficientnet_b3_ns\n",
    "# \"weight_init\":           goog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_mode and best_model is not None:\n",
    "    print(best_model.hparams)\n",
    "    extra_params = {\n",
    "        \"gpus\": len(gpus),\n",
    "        # \"pos_weight\": True\n",
    "    }\n",
    "    exp_logger.experiment.add_hparams(hparam_dict={\n",
    "        **dict(best_model.hparams),\n",
    "        **extra_params\n",
    "    },\n",
    "                                      metric_dict={\"oof_loss\": oof_loss})\n",
    "\n",
    "    oof_filename = \"_\".join(\n",
    "        [f\"{k}={v}\" for k, v in dict(best_model.hparams).items()])\n",
    "    with open(f'oof_{experiment_name}_{oof_loss}.npy', 'wb') as f:\n",
    "        np.save(f, oof_predictions)\n",
    "\n",
    "    with open(f'oof_{experiment_name}_{oof_loss}.npy', 'rb') as f:\n",
    "        tmp = np.load(f)\n",
    "        print(tmp.shape)\n",
    "\n",
    "    # Rename model filename to remove `=` for Kaggle Dataset rule\n",
    "    model_files = glob.glob(f'{model_output_folder}/fold*/epoch*.ckpt')\n",
    "    for f in model_files:\n",
    "        new_filename = f.replace(\"=\", \"\")\n",
    "        os.rename(f, new_filename)\n",
    "        print(new_filename)\n",
    "\n",
    "    del best_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kfold_submit_preds.shape)\n",
    "\n",
    "submission = pd.DataFrame(data=test_features[\"sig_id\"].values,\n",
    "                          columns=[\"sig_id\"])\n",
    "submission = submission.reindex(columns=[\"sig_id\"] + train_classes)\n",
    "submission[train_classes] = kfold_submit_preds\n",
    "# Set control type to 0 as control perturbations have no MoAs\n",
    "submission.loc[test_features['cp_type'] == 0, submission.columns[1:]] = 0\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "submission.to_csv(f'submission_effnet_v{version}_{model_type}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
