{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.6.0+cu101\n",
      "PyTorch Lightning Version: 1.0.4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[V1]\n",
    "* resnest50_fast_2s2x40d\n",
    "* Add Max./Min. Channels\n",
    "\n",
    "[V2]\n",
    "* resnest50_fast_2s2x40d\n",
    "* final_drop = 0.2\n",
    "* dropblock_prob = 0.0\n",
    "\n",
    "[TODO]\n",
    "* Separate gene expression, cell vaibility and other features\n",
    "* PCGrad (Project Conflicting Gradients)\n",
    "* Tuning resolution and image size\n",
    "\n",
    "ResNeSt:\n",
    "https://github.com/zhanghang1989/ResNeSt\n",
    "\"\"\"\n",
    "\n",
    "kernel_mode = False\n",
    "training_mode = True\n",
    "\n",
    "import sys\n",
    "if kernel_mode:\n",
    "    sys.path.insert(0, \"../input/iterative-stratification\")\n",
    "    sys.path.insert(0, \"../input/pytorch-lightning\")\n",
    "    sys.path.insert(0, \"../input/resnest\")\n",
    "    sys.path.insert(0, \"../input/pytorch-optimizer\")\n",
    "    sys.path.insert(0, \"../input/pytorch-ranger\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, \\\n",
    "    RobustScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch_optimizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional import classification\n",
    "\n",
    "import resnest\n",
    "from resnest.torch import resnest50, resnest101, resnest200, resnest269, \\\n",
    "    resnest50_fast_2s2x40d, resnest50_fast_1s2x40d, resnest50_fast_1s1x64d\n",
    "\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug.augmenters.size import CropToFixedSize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "rand_seed = 1120\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning Version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if kernel_mode:\n",
    "#     !mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "#     !cp ../input/deepinsight-resnest-v2-resnest50-output/*.pth /root/.cache/torch/hub/checkpoints/\n",
    "#     !ls -la /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"resnest50\"\n",
    "pretrained_model = f\"resnest50_fast_2s2x40d\"\n",
    "experiment_name = f\"deepinsight_ResNeSt_v2_{model_type}\"\n",
    "\n",
    "if kernel_mode:\n",
    "    dataset_folder = \"../input/lish-moa\"\n",
    "    model_output_folder = f\"./{experiment_name}\" if training_mode \\\n",
    "        else f\"../input/deepinsight-resnest-v2-resnest50-output/{experiment_name}\"\n",
    "else:\n",
    "    dataset_folder = \"/workspace/Kaggle/MoA\"\n",
    "    model_output_folder = f\"{dataset_folder}/{experiment_name}\" if training_mode \\\n",
    "        else f\"/workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v2_resnest50/{experiment_name}\"\n",
    "\n",
    "if training_mode:\n",
    "    os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "    # Dedicated logger for experiment\n",
    "    exp_logger = TensorBoardLogger(model_output_folder,\n",
    "                                   name=f\"overall_logs\",\n",
    "                                   default_hp_metric=False)\n",
    "\n",
    "# debug_mode = True\n",
    "debug_mode = False\n",
    "\n",
    "num_workers = 2 if kernel_mode else 6\n",
    "# gpus = [0, 1]\n",
    "gpus = [0]\n",
    "# gpus = [1]\n",
    "\n",
    "epochs = 200\n",
    "patience = 16\n",
    "\n",
    "# learning_rate = 1e-3\n",
    "learning_rate = 0.000352  # Suggested Learning Rate from LR finder (V7)\n",
    "learning_rate *= len(gpus)\n",
    "weight_decay = 1e-6\n",
    "# weight_decay = 0\n",
    "\n",
    "# T_max = 10  # epochs\n",
    "T_max = 5  # epochs\n",
    "T_0 = 5  # epochs\n",
    "\n",
    "accumulate_grad_batches = 1\n",
    "gradient_clip_val = 10.0\n",
    "\n",
    "if \"resnest50\" in model_type:\n",
    "    batch_size = 128\n",
    "    infer_batch_size = 256 if not kernel_mode else 256\n",
    "    image_size = 224\n",
    "    resolution = 224\n",
    "elif model_type == \"resnest101\":\n",
    "    batch_size = 48\n",
    "    infer_batch_size = 96\n",
    "    image_size = 256\n",
    "    resolution = 256\n",
    "elif model_type == \"resnest200\":\n",
    "    batch_size = 12\n",
    "    infer_batch_size = 24\n",
    "    image_size = 320\n",
    "    resolution = 320\n",
    "elif model_type == \"resnest269\":\n",
    "    batch_size = 4\n",
    "    infer_batch_size = 8\n",
    "    image_size = 416\n",
    "    resolution = 416\n",
    "\n",
    "# Prediction Clipping Thresholds\n",
    "prob_min = 0.001\n",
    "prob_max = 0.999\n",
    "\n",
    "# Swap Noise\n",
    "swap_prob = 0.1\n",
    "swap_portion = 0.15\n",
    "\n",
    "label_smoothing = 0.001\n",
    "\n",
    "# DeepInsight Transform\n",
    "perplexity = 5\n",
    "\n",
    "fc_size = 512\n",
    "\n",
    "final_drop = 0.2\n",
    "dropblock_prob = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_features.csv\", engine='c')\n",
    "train_labels = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_targets_scored.csv\", engine='c')\n",
    "\n",
    "train_extra_labels = pd.read_csv(\n",
    "    f\"{dataset_folder}/train_targets_nonscored.csv\", engine='c')\n",
    "\n",
    "test_features = pd.read_csv(\n",
    "    f\"{dataset_folder}/test_features.csv\", engine='c')\n",
    "\n",
    "sample_submission = pd.read_csv(\n",
    "    f\"{dataset_folder}/sample_submission.csv\", engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by sig_id to ensure that all row orders match\n",
    "train_features = train_features.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\n",
    "train_labels = train_labels.sort_values(by=[\"sig_id\"], axis=0,\n",
    "                                        inplace=False).reset_index(drop=True)\n",
    "train_extra_labels = train_extra_labels.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\n",
    "\n",
    "sample_submission = sample_submission.sort_values(\n",
    "    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23814, 876), (23814, 207), (23814, 403))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, train_labels.shape, train_extra_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3982, 876)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(873, 772, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_features = [\"cp_type\", \"cp_dose\"]\n",
    "numeric_features = [c for c in train_features.columns if c != \"sig_id\" and c not in category_features]\n",
    "all_features = category_features + numeric_features\n",
    "gene_experssion_features = [c for c in numeric_features if c.startswith(\"g-\")]\n",
    "cell_viability_features = [c for c in numeric_features if c.startswith(\"c-\")]\n",
    "len(numeric_features), len(gene_experssion_features), len(cell_viability_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206, 402)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes = [c for c in train_labels.columns if c != \"sig_id\"]\n",
    "train_extra_classes = [c for c in train_extra_labels.columns if c != \"sig_id\"]\n",
    "len(train_classes), len(train_extra_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for df in [train_features, test_features]:\n",
    "    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n",
    "    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    21948\n",
       "0     1866\n",
       "Name: cp_type, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12147\n",
       "1    11667\n",
       "Name: cp_dose, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_dose\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5    8250\n",
       "1.0    7792\n",
       "0.0    7772\n",
       "Name: cp_time, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"cp_time\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DeepInsight Transform (t-SNE)\n",
    "Based on https://github.com/alok-ai-lab/DeepInsight, but with some minor corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Modified from DeepInsight Transform\n",
    "# https://github.com/alok-ai-lab/DeepInsight/blob/master/pyDeepInsight/image_transformer.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib import pyplot as plt\n",
    "import inspect\n",
    "\n",
    "\n",
    "class DeepInsightTransformer:\n",
    "    \"\"\"Transform features to an image matrix using dimensionality reduction\n",
    "\n",
    "    This class takes in data normalized between 0 and 1 and converts it to a\n",
    "    CNN compatible 'image' matrix\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_extractor='tsne',\n",
    "                 perplexity=30,\n",
    "                 pixels=100,\n",
    "                 random_state=None,\n",
    "                 n_jobs=None):\n",
    "        \"\"\"Generate an ImageTransformer instance\n",
    "\n",
    "        Args:\n",
    "            feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n",
    "                class instance with method `fit_transform` that returns a\n",
    "                2-dimensional array of extracted features.\n",
    "            pixels: int (square matrix) or tuple of ints (height, width) that\n",
    "                defines the size of the image matrix.\n",
    "            random_state: int or RandomState. Determines the random number\n",
    "                generator, if present, of a string defined feature_extractor.\n",
    "            n_jobs: The number of parallel jobs to run for a string defined\n",
    "                feature_extractor.\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "        if isinstance(feature_extractor, str):\n",
    "            fe = feature_extractor.casefold()\n",
    "            if fe == 'tsne_exact'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='exact',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'tsne'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='barnes_hut',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'pca'.casefold():\n",
    "                fe = PCA(n_components=2, random_state=self.random_state)\n",
    "            elif fe == 'kpca'.casefold():\n",
    "                fe = KernelPCA(n_components=2,\n",
    "                               kernel='rbf',\n",
    "                               random_state=self.random_state,\n",
    "                               n_jobs=self.n_jobs)\n",
    "            else:\n",
    "                raise ValueError((\"Feature extraction method '{}' not accepted\"\n",
    "                                  ).format(feature_extractor))\n",
    "            self._fe = fe\n",
    "        elif hasattr(feature_extractor, 'fit_transform') and \\\n",
    "                inspect.ismethod(feature_extractor.fit_transform):\n",
    "            self._fe = feature_extractor\n",
    "        else:\n",
    "            raise TypeError('Parameter feature_extractor is not a '\n",
    "                            'string nor has method \"fit_transform\"')\n",
    "\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "\n",
    "        # The resolution of transformed image\n",
    "        self._pixels = pixels\n",
    "        self._xrot = None\n",
    "\n",
    "    def fit(self, X, y=None, plot=False):\n",
    "        \"\"\"Train the image transformer from the training set (X)\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            y: Ignored. Present for continuity with scikit-learn\n",
    "            plot: boolean of whether to produce a scatter plot showing the\n",
    "                feature reduction, hull points, and minimum bounding rectangle\n",
    "\n",
    "        Returns:\n",
    "            self: object\n",
    "        \"\"\"\n",
    "        # Transpose to get (n_features, n_samples)\n",
    "        X = X.T\n",
    "\n",
    "        # Perform dimensionality reduction\n",
    "        x_new = self._fe.fit_transform(X)\n",
    "\n",
    "        # Get the convex hull for the points\n",
    "        chvertices = ConvexHull(x_new).vertices\n",
    "        hull_points = x_new[chvertices]\n",
    "\n",
    "        # Determine the minimum bounding rectangle\n",
    "        mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n",
    "\n",
    "        # Rotate the matrix\n",
    "        # Save the rotated matrix in case user wants to change the pixel size\n",
    "        self._xrot = np.dot(mbr_rot, x_new.T).T\n",
    "\n",
    "        # Determine feature coordinates based on pixel dimension\n",
    "        self._calculate_coords()\n",
    "\n",
    "        # plot rotation diagram if requested\n",
    "        if plot is True:\n",
    "            # Create subplots\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n",
    "            ax[0, 0].scatter(x_new[:, 0],\n",
    "                             x_new[:, 1],\n",
    "                             cmap=plt.cm.get_cmap(\"jet\", 10),\n",
    "                             marker=\"x\",\n",
    "                             alpha=1.0)\n",
    "            ax[0, 0].fill(x_new[chvertices, 0],\n",
    "                          x_new[chvertices, 1],\n",
    "                          edgecolor='r',\n",
    "                          fill=False)\n",
    "            ax[0, 0].fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "            plt.show()\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def pixels(self):\n",
    "        \"\"\"The image matrix dimensions\n",
    "\n",
    "        Returns:\n",
    "            tuple: the image matrix dimensions (height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        return self._pixels\n",
    "\n",
    "    @pixels.setter\n",
    "    def pixels(self, pixels):\n",
    "        \"\"\"Set the image matrix dimension\n",
    "\n",
    "        Args:\n",
    "            pixels: int or tuple with the dimensions (height, width)\n",
    "            of the image matrix\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "        self._pixels = pixels\n",
    "        # recalculate coordinates if already fit\n",
    "        if hasattr(self, '_coords'):\n",
    "            self._calculate_coords()\n",
    "\n",
    "    def _calculate_coords(self):\n",
    "        \"\"\"Calculate the matrix coordinates of each feature based on the\n",
    "        pixel dimensions.\n",
    "        \"\"\"\n",
    "        ax0_coord = np.digitize(self._xrot[:, 0],\n",
    "                                bins=np.linspace(min(self._xrot[:, 0]),\n",
    "                                                 max(self._xrot[:, 0]),\n",
    "                                                 self._pixels[0])) - 1\n",
    "        ax1_coord = np.digitize(self._xrot[:, 1],\n",
    "                                bins=np.linspace(min(self._xrot[:, 1]),\n",
    "                                                 max(self._xrot[:, 1]),\n",
    "                                                 self._pixels[1])) - 1\n",
    "        self._coords = np.stack((ax0_coord, ax1_coord))\n",
    "\n",
    "    def transform(self, X, empty_value=0):\n",
    "        \"\"\"Transform the input matrix into image matrices\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "                where n_features matches the training set.\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        img_coords = pd.DataFrame(np.vstack(\n",
    "            (self._coords, X.clip(0, 1))).T).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).mean()\n",
    "\n",
    "        img_matrices = []\n",
    "        blank_mat = np.zeros(self._pixels)\n",
    "        if empty_value != 0:\n",
    "            blank_mat[:] = empty_value\n",
    "        for z in range(2, img_coords.shape[1]):\n",
    "            img_matrix = blank_mat.copy()\n",
    "            img_matrix[img_coords[0].astype(int),\n",
    "                       img_coords[1].astype(int)] = img_coords[z]\n",
    "            img_matrices.append(img_matrix)\n",
    "\n",
    "        return img_matrices\n",
    "\n",
    "    def transform_3d(self, X, empty_value=0):\n",
    "        \"\"\"Transform the input matrix into image matrices\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "                where n_features matches the training set.\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        img_coords = pd.DataFrame(np.vstack(\n",
    "            (self._coords, X.clip(0, 1))).T).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False)\n",
    "        avg_img_coords = img_coords.mean()\n",
    "        min_img_coords = img_coords.min()\n",
    "        max_img_coords = img_coords.max()\n",
    "\n",
    "        img_matrices = []\n",
    "        blank_mat = np.zeros((3, self._pixels[0], self._pixels[1]))\n",
    "        if empty_value != 0:\n",
    "            blank_mat[:, :, :] = empty_value\n",
    "        for z in range(2, avg_img_coords.shape[1]):\n",
    "            img_matrix = blank_mat.copy()\n",
    "            img_matrix[0, avg_img_coords[0].astype(int),\n",
    "                       avg_img_coords[1].astype(int)] = avg_img_coords[z]\n",
    "            img_matrix[1, min_img_coords[0].astype(int),\n",
    "                       min_img_coords[1].astype(int)] = min_img_coords[z]\n",
    "            img_matrix[2, max_img_coords[0].astype(int),\n",
    "                       max_img_coords[1].astype(int)] = max_img_coords[z]\n",
    "            img_matrices.append(img_matrix)\n",
    "\n",
    "        return img_matrices\n",
    "\n",
    "    def fit_transform(self, X, empty_value=0):\n",
    "        \"\"\"Train the image transformer from the training set (X) and return\n",
    "        the transformed data.\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X, empty_value=empty_value)\n",
    "\n",
    "    def fit_transform_3d(self, X, empty_value=0):\n",
    "        \"\"\"Train the image transformer from the training set (X) and return\n",
    "        the transformed data.\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform_3d(X, empty_value=empty_value)\n",
    "\n",
    "    def feature_density_matrix(self):\n",
    "        \"\"\"Generate image matrix with feature counts per pixel\n",
    "\n",
    "        Returns:\n",
    "            img_matrix (ndarray): matrix with feature counts per pixel\n",
    "        \"\"\"\n",
    "        fdmat = np.zeros(self._pixels)\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        coord_cnt = (\n",
    "            pd.DataFrame(self._coords.T).assign(count=1).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).count())\n",
    "        fdmat[coord_cnt[0].astype(int),\n",
    "              coord_cnt[1].astype(int)] = coord_cnt['count']\n",
    "        return fdmat\n",
    "\n",
    "    @staticmethod\n",
    "    def _minimum_bounding_rectangle(hull_points):\n",
    "        \"\"\"Find the smallest bounding rectangle for a set of points.\n",
    "\n",
    "        Modified from JesseBuesking at https://stackoverflow.com/a/33619018\n",
    "        Returns a set of points representing the corners of the bounding box.\n",
    "\n",
    "        Args:\n",
    "            hull_points : an nx2 matrix of hull coordinates\n",
    "\n",
    "        Returns:\n",
    "            (tuple): tuple containing\n",
    "                coords (ndarray): coordinates of the corners of the rectangle\n",
    "                rotmat (ndarray): rotation matrix to align edges of rectangle\n",
    "                    to x and y\n",
    "        \"\"\"\n",
    "\n",
    "        pi2 = np.pi / 2.\n",
    "\n",
    "        # Calculate edge angles\n",
    "        edges = hull_points[1:] - hull_points[:-1]\n",
    "        angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
    "        angles = np.abs(np.mod(angles, pi2))\n",
    "        angles = np.unique(angles)\n",
    "\n",
    "        # Find rotation matrices\n",
    "        rotations = np.vstack([\n",
    "            np.cos(angles),\n",
    "            np.cos(angles - pi2),\n",
    "            np.cos(angles + pi2),\n",
    "            np.cos(angles)\n",
    "        ]).T\n",
    "        rotations = rotations.reshape((-1, 2, 2))\n",
    "\n",
    "        # Apply rotations to the hull\n",
    "        rot_points = np.dot(rotations, hull_points.T)\n",
    "\n",
    "        # Find the bounding points\n",
    "        min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
    "        max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
    "        min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
    "        max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
    "\n",
    "        # Find the box with the best area\n",
    "        areas = (max_x - min_x) * (max_y - min_y)\n",
    "        best_idx = np.argmin(areas)\n",
    "\n",
    "        # Return the best box\n",
    "        x1 = max_x[best_idx]\n",
    "        x2 = min_x[best_idx]\n",
    "        y1 = max_y[best_idx]\n",
    "        y2 = min_y[best_idx]\n",
    "        rotmat = rotations[best_idx]\n",
    "\n",
    "        # Generate coordinates\n",
    "        coords = np.zeros((4, 2))\n",
    "        coords[0] = np.dot([x1, y2], rotmat)\n",
    "        coords[1] = np.dot([x2, y2], rotmat)\n",
    "        coords[2] = np.dot([x2, y1], rotmat)\n",
    "        coords[3] = np.dot([x1, y1], rotmat)\n",
    "\n",
    "        return coords, rotmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LogScaler:\n",
    "    \"\"\"Log normalize and scale data\n",
    "\n",
    "    Log normalization and scaling procedure as described as norm-2 in the\n",
    "    DeepInsight paper supplementary information.\n",
    "    \n",
    "    Note: The dimensions of input matrix is (N samples, d features)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._min0 = None\n",
    "        self._max = None\n",
    "\n",
    "    \"\"\"\n",
    "    Use this as a preprocessing step in inference mode.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "\n",
    "    \"\"\"\n",
    "    For training set only.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)\n",
    "\n",
    "    \"\"\"\n",
    "    For validation and test set only.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Adjust min. of each feature of X by _min0\n",
    "        for i in range(X.shape[1]):\n",
    "            X[:, i] = X[:, i].clip(min=self._min0[i], max=None)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(min=0, max=None)\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MoAImageSwapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 features,\n",
    "                 labels,\n",
    "                 transformer,\n",
    "                 swap_prob=0.15,\n",
    "                 swap_portion=0.1):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "        self.swap_prob = swap_prob\n",
    "        self.swap_portion = swap_portion\n",
    "\n",
    "        self.crop = CropToFixedSize(width=image_size, height=image_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "\n",
    "        # Swap row featurs randomly\n",
    "        normalized = self.add_swap_noise(index, normalized)\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=0\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image.transpose((1, 2, 0)),\n",
    "                           (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "\n",
    "        return {\"x\": image, \"y\": self.labels[index, :]}\n",
    "\n",
    "    def add_swap_noise(self, index, X):\n",
    "        if np.random.rand() < self.swap_prob:\n",
    "            swap_index = np.random.randint(self.features.shape[0], size=1)[0]\n",
    "            # Select only gene expression and cell viability features\n",
    "            swap_features = np.random.choice(\n",
    "                np.array(range(3, self.features.shape[1])),\n",
    "                size=int(self.features.shape[1] * self.swap_portion),\n",
    "                replace=False)\n",
    "            X[swap_features] = self.features[swap_index, swap_features]\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MoAImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=0\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image.transpose((1, 2, 0)),\n",
    "                           (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "\n",
    "        return {\"x\": image, \"y\": self.labels[index, :]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, transformer):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        normalized = self.features[index, :]\n",
    "        normalized = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "        # Note: we are setting empty_value=0\n",
    "        image = self.transformer.transform_3d(normalized, empty_value=0)[0]\n",
    "\n",
    "        # Resize to target size\n",
    "        image = cv2.resize(image.transpose((1, 2, 0)),\n",
    "                           (image_size, image_size),\n",
    "                           interpolation=cv2.INTER_CUBIC)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "\n",
    "        return {\"x\": image, \"y\": -1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/vbmokin/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual#4.7-Smoothing\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets: torch.Tensor, n_labels: int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "                                           self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(layer):\n",
    "    for m in layer.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1.0)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            fan_out = m.weight.size(0)  # fan-out\n",
    "            fan_in = 0\n",
    "            init_range = 1.0 / math.sqrt(fan_in + fan_out)\n",
    "            m.weight.data.uniform_(-init_range, init_range)\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MoAResNeSt(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_model_name,\n",
    "            training_set=(None, None),  # tuple\n",
    "            valid_set=(None, None),  # tuple\n",
    "            test_set=None,\n",
    "            transformer=None,\n",
    "            num_classes=206,\n",
    "            final_drop=0.0,\n",
    "            dropblock_prob=0,\n",
    "            fc_size=512,\n",
    "            learning_rate=1e-3):\n",
    "        super(MoAResNeSt, self).__init__()\n",
    "\n",
    "        self.train_data, self.train_labels = training_set\n",
    "        self.valid_data, self.valid_labels = valid_set\n",
    "        self.test_data = test_set\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.backbone = getattr(resnest.torch, pretrained_model)(\n",
    "            pretrained=True,\n",
    "            final_drop=final_drop)\n",
    "\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(self.backbone.fc.in_features, fc_size, bias=True),\n",
    "            nn.ELU(), nn.Linear(fc_size, num_classes, bias=True))\n",
    "\n",
    "        if self.training:\n",
    "            initialize_weights(self.backbone.fc)\n",
    "\n",
    "        # Save passed hyperparameters\n",
    "        self.save_hyperparameters(\"pretrained_model_name\", \"num_classes\",\n",
    "                                  \"final_drop\", \"dropblock_prob\", \"fc_size\",\n",
    "                                  \"learning_rate\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "\n",
    "        # loss = F.binary_cross_entropy_with_logits(logits, y, reduction=\"mean\")\n",
    "\n",
    "        # Label smoothing\n",
    "        loss = SmoothBCEwLogits(smoothing=label_smoothing)(logits, y)\n",
    "\n",
    "        self.log('train_loss',\n",
    "                 loss,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "\n",
    "        val_loss = F.binary_cross_entropy_with_logits(logits,\n",
    "                                                      y,\n",
    "                                                      reduction=\"mean\")\n",
    "\n",
    "        self.log('val_loss',\n",
    "                 val_loss,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=True)\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        x = x.float()\n",
    "        y = y.type_as(x)\n",
    "        logits = self(x)\n",
    "        return {\"pred_logits\": logits}\n",
    "\n",
    "    def test_epoch_end(self, output_results):\n",
    "        all_outputs = torch.cat([out[\"pred_logits\"] for out in output_results],\n",
    "                                dim=0)\n",
    "        print(\"Logits:\", all_outputs)\n",
    "        pred_probs = F.sigmoid(all_outputs).detach().cpu().numpy()\n",
    "        print(\"Predictions: \", pred_probs)\n",
    "        return {\"pred_probs\": pred_probs}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        #         self.train_dataset = MoAImageDataset(self.train_data,\n",
    "        #                                              self.train_labels,\n",
    "        #                                              self.transformer)\n",
    "        self.train_dataset = MoAImageSwapDataset(self.train_data,\n",
    "                                                 self.train_labels,\n",
    "                                                 self.transformer,\n",
    "                                                 swap_prob=swap_prob,\n",
    "                                                 swap_portion=swap_portion)\n",
    "\n",
    "        self.val_dataset = MoAImageDataset(self.valid_data, self.valid_labels,\n",
    "                                           self.transformer)\n",
    "\n",
    "        self.test_dataset = TestDataset(self.test_data, None, self.transformer)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = DataLoader(self.train_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=num_workers,\n",
    "                                      pin_memory=True,\n",
    "                                      drop_last=False)\n",
    "        print(f\"Train iterations: {len(train_dataloader)}\")\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = DataLoader(self.val_dataset,\n",
    "                                    batch_size=infer_batch_size,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=num_workers,\n",
    "                                    pin_memory=True,\n",
    "                                    drop_last=False)\n",
    "        print(f\"Validate iterations: {len(val_dataloader)}\")\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataloader = DataLoader(self.test_dataset,\n",
    "                                     batch_size=infer_batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=num_workers,\n",
    "                                     pin_memory=True,\n",
    "                                     drop_last=False)\n",
    "        print(f\"Test iterations: {len(test_dataloader)}\")\n",
    "        return test_dataloader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print(f\"Initial Learning Rate: {self.hparams.learning_rate:.6f}\")\n",
    "        #         optimizer = optim.Adam(self.parameters(),\n",
    "        #                                lr=self.hparams.learning_rate,\n",
    "        #                                weight_decay=weight_decay)\n",
    "        #         optimizer = torch.optim.SGD(self.parameters(),\n",
    "        #                                     lr=self.hparams.learning_rate,\n",
    "        #                                     momentum=0.9,\n",
    "        #                                     dampening=0,\n",
    "        #                                     weight_decay=weight_decay,\n",
    "        #                                     nesterov=False)\n",
    "\n",
    "        optimizer = torch_optimizer.RAdam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                         T_max=T_max,\n",
    "                                                         eta_min=0,\n",
    "                                                         last_epoch=-1)\n",
    "\n",
    "        #         scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        #             optimizer,\n",
    "        #             T_0=T_0,\n",
    "        #             T_mult=1,\n",
    "        #             eta_min=0,\n",
    "        #             last_epoch=-1)\n",
    "\n",
    "        #         scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        #             optimizer=optimizer,\n",
    "        #             pct_start=0.1,\n",
    "        #             div_factor=1e3,\n",
    "        #             max_lr=1e-1,\n",
    "        #             # max_lr=1e-2,\n",
    "        #             epochs=epochs,\n",
    "        #             steps_per_epoch=len(self.train_images) // batch_size)\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = MoAResNeSt(\n",
    "#     pretrained_model,\n",
    "#     training_set=(None, None),  # tuple\n",
    "#     valid_set=(None, None),  # tuple\n",
    "#     test_set=None,\n",
    "#     transformer=None,\n",
    "#     num_classes=206,\n",
    "#     final_drop=0.0,\n",
    "#     dropblock_prob=0,\n",
    "#     fc_size=fc_size,\n",
    "#     learning_rate=learning_rate)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = 10\n",
    "skf = MultilabelStratifiedKFold(n_splits=kfolds,\n",
    "                                shuffle=True,\n",
    "                                random_state=rand_seed)\n",
    "\n",
    "label_counts = np.sum(train_labels.drop(\"sig_id\", axis=1), axis=0)\n",
    "y_labels = label_counts.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(training_set, valid_set, test_set, transformer, model_path=None):\n",
    "    if training_mode:\n",
    "        model = MoAResNeSt(\n",
    "            pretrained_model_name=pretrained_model,\n",
    "            training_set=training_set,  # tuple\n",
    "            valid_set=valid_set,  # tuple\n",
    "            test_set=test_set,\n",
    "            transformer=transformer,\n",
    "            num_classes=len(train_classes),\n",
    "            final_drop=final_drop,\n",
    "            dropblock_prob=dropblock_prob,\n",
    "            fc_size=fc_size,\n",
    "            learning_rate=learning_rate)\n",
    "    else:\n",
    "        model = MoAResNeSt.load_from_checkpoint(\n",
    "            model_path,\n",
    "            pretrained_model_name=pretrained_model,\n",
    "            training_set=training_set,  # tuple\n",
    "            valid_set=valid_set,  # tuple\n",
    "            test_set=test_set,\n",
    "            transformer=transformer,\n",
    "            num_classes=len(train_classes),\n",
    "            fc_size=fc_size)\n",
    "        model.freeze()\n",
    "        model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_pickle(obj, model_output_folder, fold_i, name):\n",
    "    dump(obj, open(f\"{model_output_folder}/fold{fold_i}_{name}.pkl\", 'wb'),\n",
    "         pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_pickle(model_output_folder, fold_i, name):\n",
    "    return load(open(f\"{model_output_folder}/fold{fold_i}_{name}.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm2_normalization(train, valid, test):\n",
    "    scaler = LogScaler()\n",
    "    train = scaler.fit_transform(train)\n",
    "    valid = scaler.transform(valid)\n",
    "    test = scaler.transform(test)\n",
    "    return train, valid, test, scaler\n",
    "\n",
    "\n",
    "def quantile_transform(train, valid, test):\n",
    "    q_scaler = QuantileTransformer(n_quantiles=1000,\n",
    "                                   output_distribution='normal',\n",
    "                                   ignore_implicit_zeros=False,\n",
    "                                   subsample=100000,\n",
    "                                   random_state=rand_seed)\n",
    "    train = q_scaler.fit_transform(train)\n",
    "    valid = q_scaler.transform(valid)\n",
    "    test = q_scaler.transform(test)\n",
    "\n",
    "    # Transform to [0, 1]\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train = min_max_scaler.fit_transform(train)\n",
    "    valid = min_max_scaler.transform(valid)\n",
    "    test = min_max_scaler.transform(test)\n",
    "\n",
    "    return train, valid, test, q_scaler, min_max_scaler\n",
    "\n",
    "\n",
    "def extract_feature_map(train,\n",
    "                        feature_extractor='tsne_exact',\n",
    "                        resolution=100,\n",
    "                        perplexity=30):\n",
    "    transformer = DeepInsightTransformer(feature_extractor=feature_extractor,\n",
    "                                         pixels=resolution,\n",
    "                                         perplexity=perplexity,\n",
    "                                         random_state=rand_seed,\n",
    "                                         n_jobs=-1)\n",
    "    transformer.fit(train)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_logloss(y_pred, y_true):\n",
    "    logloss = (1 - y_true) * np.log(1 - y_pred +\n",
    "                                    1e-15) + y_true * np.log(y_pred + 1e-15)\n",
    "    return np.mean(-logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing on Fold 0 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v2_resnest50/deepinsight_ResNeSt_v2_resnest50/fold0/epoch=25-train_loss_epoch=0.016863-val_loss_epoch=0.014446-image_size=224-resolution=224-perplexity=5-fc=512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241ae9fd61e846c99d4b2382576831f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-7.1992, -8.0703, -6.8320,  ..., -7.0938, -6.2148, -5.9062],\n",
      "        [-9.7656, -9.1875, -7.5547,  ..., -7.7031, -9.1328, -5.5781],\n",
      "        [-7.8945, -7.7383, -7.6133,  ..., -7.5938, -7.8750, -7.5664],\n",
      "        ...,\n",
      "        [-5.6328, -6.8633, -7.2227,  ..., -6.8398, -7.9336, -6.5352],\n",
      "        [-7.1055, -7.6250, -7.3008,  ..., -6.4844, -7.0312, -5.6172],\n",
      "        [-6.2266, -6.8242, -6.8398,  ..., -6.6523, -7.4258, -6.6133]],\n",
      "       device='cuda:1', dtype=torch.float16)\n",
      "Predictions:  [[7.4673e-04 3.1257e-04 1.0777e-03 ... 8.2970e-04 1.9951e-03 2.7142e-03]\n",
      " [5.7399e-05 1.0228e-04 5.2357e-04 ... 4.5133e-04 1.0806e-04 3.7651e-03]\n",
      " [3.7265e-04 4.3559e-04 4.9353e-04 ... 5.0354e-04 3.8004e-04 5.1737e-04]\n",
      " ...\n",
      " [3.5648e-03 1.0443e-03 7.2956e-04 ... 1.0691e-03 3.5834e-04 1.4496e-03]\n",
      " [8.2016e-04 4.8780e-04 6.7472e-04 ... 1.5249e-03 8.8310e-04 3.6221e-03]\n",
      " [1.9722e-03 1.0862e-03 1.0691e-03 ... 1.2894e-03 5.9557e-04 1.3409e-03]]\n",
      "\n",
      "Inferencing on Fold 1 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v2_resnest50/deepinsight_ResNeSt_v2_resnest50/fold1/epoch=35-train_loss_epoch=0.015456-val_loss_epoch=0.014600-image_size=224-resolution=224-perplexity=5-fc=512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8878cb2fbdaa4e13a01c1affd8f67e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-7.6172, -7.1875, -6.1602,  ..., -6.3945, -7.7422, -6.9023],\n",
      "        [-7.2188, -7.5273, -8.4922,  ..., -7.6484, -8.6016, -5.4727],\n",
      "        [-7.7266, -7.9688, -7.5938,  ..., -7.7344, -7.8945, -7.4609],\n",
      "        ...,\n",
      "        [-5.1680, -5.4336, -6.4062,  ..., -6.8125, -7.7227, -6.8359],\n",
      "        [-6.8945, -7.6484, -7.9375,  ..., -6.4414, -6.6953, -5.7422],\n",
      "        [-6.5469, -8.0859, -6.7852,  ..., -7.6758, -7.4922, -7.3555]],\n",
      "       device='cuda:1', dtype=torch.float16)\n",
      "Predictions:  [[0.0004916 0.0007553 0.002108  ... 0.001668  0.000434  0.001004 ]\n",
      " [0.000732  0.000538  0.000205  ... 0.0004766 0.0001838 0.00418  ]\n",
      " [0.0004408 0.000346  0.0005035 ... 0.0004373 0.0003726 0.0005746]\n",
      " ...\n",
      " [0.005665  0.00435   0.001649  ... 0.001099  0.0004425 0.001073 ]\n",
      " [0.001012  0.0004766 0.000357  ... 0.001592  0.001235  0.003197 ]\n",
      " [0.001432  0.0003078 0.001129  ... 0.0004637 0.000557  0.0006385]]\n",
      "\n",
      "Inferencing on Fold 2 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v2_resnest50/deepinsight_ResNeSt_v2_resnest50/fold2/epoch=35-train_loss_epoch=0.015555-val_loss_epoch=0.014464-image_size=224-resolution=224-perplexity=5-fc=512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125f0544ed8f4efda0fdd640f5982f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-7.1875, -6.4922, -6.4883,  ..., -6.7227, -6.5898, -7.9453],\n",
      "        [-8.6406, -8.7344, -7.5000,  ..., -7.9453, -8.6641, -6.6719],\n",
      "        [-8.2422, -8.1562, -7.3008,  ..., -7.5469, -7.3711, -7.6836],\n",
      "        ...,\n",
      "        [-6.2344, -6.5820, -6.6953,  ..., -6.3906, -8.2031, -6.8438],\n",
      "        [-6.7734, -7.5820, -7.4922,  ..., -6.8164, -7.7461, -6.6445],\n",
      "        [-6.5000, -7.1289, -6.4141,  ..., -6.6328, -8.5547, -7.1016]],\n",
      "       device='cuda:1', dtype=torch.float16)\n",
      "Predictions:  [[0.0007553 0.001513  0.001519  ... 0.001202  0.001372  0.0003543]\n",
      " [0.0001768 0.0001609 0.0005527 ... 0.0003543 0.0001726 0.001265 ]\n",
      " [0.0002632 0.0002868 0.0006747 ... 0.0005274 0.000629  0.0004601]\n",
      " ...\n",
      " [0.001957  0.001383  0.001235  ... 0.001675  0.0002737 0.001065 ]\n",
      " [0.0011425 0.0005093 0.000557  ... 0.001095  0.0004323 0.0013   ]\n",
      " [0.001501  0.000801  0.001636  ... 0.001315  0.0001926 0.000823 ]]\n",
      "\n",
      "Inferencing on Fold 3 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v2_resnest50/deepinsight_ResNeSt_v2_resnest50/fold3/epoch=35-train_loss_epoch=0.015694-val_loss_epoch=0.014556-image_size=224-resolution=224-perplexity=5-fc=512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb0f0795d404c86a5f6393764be10e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.5898, -6.5391, -7.5352,  ..., -5.7148, -5.7578, -6.8125],\n",
      "        [-9.1719, -9.7500, -7.5273,  ..., -8.6484, -8.9219, -7.7617],\n",
      "        [-7.3242, -7.6289, -7.3008,  ..., -7.5820, -7.6797, -7.6875],\n",
      "        ...,\n",
      "        [-5.7148, -6.0000, -7.7891,  ..., -6.9492, -7.2617, -6.7422],\n",
      "        [-7.3672, -7.5859, -6.0234,  ..., -5.3711, -8.0078, -6.0742],\n",
      "        [-6.8281, -7.5312, -6.2305,  ..., -6.3008, -7.8203, -6.6484]],\n",
      "       device='cuda:1', dtype=torch.float16)\n",
      "Predictions:  [[1.372e-03 1.444e-03 5.336e-04 ... 3.286e-03 3.147e-03 1.099e-03]\n",
      " [1.039e-04 5.829e-05 5.379e-04 ... 1.754e-04 1.334e-04 4.256e-04]\n",
      " [6.590e-04 4.859e-04 6.747e-04 ... 5.093e-04 4.618e-04 4.582e-04]\n",
      " ...\n",
      " [3.286e-03 2.472e-03 4.141e-04 ... 9.584e-04 7.014e-04 1.179e-03]\n",
      " [6.313e-04 5.074e-04 2.415e-03 ... 4.627e-03 3.328e-04 2.296e-03]\n",
      " [1.081e-03 5.360e-04 1.965e-03 ... 1.831e-03 4.013e-04 1.294e-03]]\n",
      "\n",
      "Inferencing on Fold 4 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v2_resnest50/deepinsight_ResNeSt_v2_resnest50/fold4/epoch=25-train_loss_epoch=0.016684-val_loss_epoch=0.014621-image_size=224-resolution=224-perplexity=5-fc=512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f337b7b3a984498bdb82fdafe50542c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-6.3984, -6.3203, -6.1797,  ..., -6.1484, -7.2578, -6.6016],\n",
      "        [-8.5703, -7.7227, -7.4180,  ..., -7.4688, -8.1094, -5.9062],\n",
      "        [-7.3945, -7.5625, -7.1094,  ..., -7.4180, -7.7344, -7.0547],\n",
      "        ...,\n",
      "        [-6.3086, -5.9062, -6.3438,  ..., -6.3125, -7.6719, -6.0000],\n",
      "        [-6.4688, -6.5469, -6.5156,  ..., -6.1094, -7.7461, -6.3359],\n",
      "        [-6.5352, -7.2656, -6.5898,  ..., -6.7344, -7.2461, -6.2500]],\n",
      "       device='cuda:1', dtype=torch.float16)\n",
      "Predictions:  [[0.001661  0.001796  0.002068  ... 0.002132  0.0007043 0.001356 ]\n",
      " [0.0001897 0.0004425 0.0006    ... 0.0005703 0.0003006 0.002714 ]\n",
      " [0.000614  0.0005193 0.000817  ... 0.0006    0.0004373 0.0008626]\n",
      " ...\n",
      " [0.001818  0.002714  0.001755  ... 0.00181   0.0004656 0.002472 ]\n",
      " [0.001549  0.001432  0.001478  ... 0.002216  0.0004323 0.001768 ]\n",
      " [0.00145   0.0006986 0.001372  ... 0.001188  0.0007124 0.001926 ]]\n",
      "\n",
      "Inferencing on Fold 5 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v2_resnest50/deepinsight_ResNeSt_v2_resnest50/fold5/epoch=35-train_loss_epoch=0.015244-val_loss_epoch=0.014611-image_size=224-resolution=224-perplexity=5-fc=512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a1164084e24cf99cd6ed1be0e5eac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-7.8906, -6.8594, -6.5547,  ..., -6.1133, -6.4922, -7.8086],\n",
      "        [-7.5938, -9.4609, -8.2734,  ..., -9.6328, -8.7656, -5.8516],\n",
      "        [-7.7578, -7.2031, -7.3828,  ..., -7.3828, -8.3281, -7.6523],\n",
      "        ...,\n",
      "        [-4.7812, -5.9258, -7.1133,  ..., -7.6250, -8.4766, -7.5469],\n",
      "        [-7.6328, -7.8984, -6.6562,  ..., -6.3945, -7.6836, -6.1602],\n",
      "        [-7.1133, -6.7734, -5.8594,  ..., -6.0078, -7.9805, -7.0156]],\n",
      "       device='cuda:1', dtype=torch.float16)\n",
      "Predictions:  [[3.7408e-04 1.0481e-03 1.4210e-03 ... 2.2087e-03 1.5125e-03 4.0603e-04]\n",
      " [5.0354e-04 7.7844e-05 2.5511e-04 ... 6.5565e-05 1.5593e-04 2.8667e-03]\n",
      " [4.2725e-04 7.4387e-04 6.2132e-04 ... 6.2132e-04 2.4152e-04 4.7469e-04]\n",
      " ...\n",
      " [8.3160e-03 2.6627e-03 8.1348e-04 ... 4.8780e-04 2.0826e-04 5.2738e-04]\n",
      " [4.8399e-04 3.7122e-04 1.2846e-03 ... 1.6680e-03 4.6015e-04 2.1076e-03]\n",
      " [8.1348e-04 1.1425e-03 2.8458e-03 ... 2.4529e-03 3.4189e-04 8.9693e-04]]\n",
      "\n",
      "Inferencing on Fold 6 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v2_resnest50/deepinsight_ResNeSt_v2_resnest50/fold6/epoch=35-train_loss_epoch=0.015504-val_loss_epoch=0.014640-image_size=224-resolution=224-perplexity=5-fc=512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84906b4b69f94965a1f8a4a728177a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-7.9414, -7.6016, -6.3906,  ..., -6.6133, -4.0391, -7.3008],\n",
      "        [-9.5625, -9.4922, -6.1719,  ..., -8.0312, -7.8477, -6.4961],\n",
      "        [-7.5938, -7.4297, -7.1289,  ..., -7.4531, -7.5352, -7.5977],\n",
      "        ...,\n",
      "        [-5.4102, -6.4961, -6.6406,  ..., -6.8711, -7.5547, -7.2305],\n",
      "        [-6.8438, -7.1055, -7.7188,  ..., -6.8320, -7.0312, -6.2422],\n",
      "        [-7.3789, -6.8438, -6.2344,  ..., -6.1641, -7.9336, -7.1406]],\n",
      "       device='cuda:1', dtype=torch.float16)\n",
      "Predictions:  [[3.555e-04 4.992e-04 1.675e-03 ... 1.341e-03 1.730e-02 6.747e-04]\n",
      " [7.033e-05 7.546e-05 2.083e-03 ... 3.250e-04 3.905e-04 1.507e-03]\n",
      " [5.035e-04 5.932e-04 8.011e-04 ... 5.794e-04 5.336e-04 5.012e-04]\n",
      " ...\n",
      " [4.452e-03 1.507e-03 1.305e-03 ... 1.037e-03 5.236e-04 7.238e-04]\n",
      " [1.065e-03 8.202e-04 4.442e-04 ... 1.078e-03 8.831e-04 1.942e-03]\n",
      " [6.237e-04 1.065e-03 1.957e-03 ... 2.100e-03 3.583e-04 7.915e-04]]\n",
      "\n",
      "Inferencing on Fold 7 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v2_resnest50/deepinsight_ResNeSt_v2_resnest50/fold7/epoch=35-train_loss_epoch=0.015158-val_loss_epoch=0.014552-image_size=224-resolution=224-perplexity=5-fc=512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0645efd22474feba354a0998e70c5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-7.3516, -7.5703, -6.6016,  ..., -6.0195, -8.0469, -7.8750],\n",
      "        [-8.4219, -8.1250, -8.0000,  ..., -8.5547, -9.0547, -5.4375],\n",
      "        [-7.9961, -7.7422, -7.4102,  ..., -7.3828, -8.0703, -7.5781],\n",
      "        ...,\n",
      "        [-5.8789, -6.7695, -7.7773,  ..., -7.0039, -7.8438, -6.6953],\n",
      "        [-7.5898, -7.9492, -6.6680,  ..., -6.3164, -8.2344, -6.4492],\n",
      "        [-7.3477, -8.1719, -6.9102,  ..., -6.7109, -8.5312, -7.0234]],\n",
      "       device='cuda:1', dtype=torch.float16)\n",
      "Predictions:  [[0.0006413 0.0005155 0.001356  ... 0.002424  0.00032   0.00038  ]\n",
      " [0.00022   0.0002959 0.0003355 ... 0.0001926 0.0001168 0.00433  ]\n",
      " [0.0003366 0.000434  0.0006046 ... 0.0006213 0.0003126 0.000511 ]\n",
      " ...\n",
      " [0.00279   0.001147  0.000419  ... 0.0009074 0.000392  0.001235 ]\n",
      " [0.0005054 0.0003529 0.001269  ... 0.001803  0.0002654 0.001579 ]\n",
      " [0.0006437 0.0002825 0.000997  ... 0.001216  0.0001972 0.00089  ]]\n",
      "\n",
      "Inferencing on Fold 8 ......\n",
      "(21433,) (2381,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v2_resnest50/deepinsight_ResNeSt_v2_resnest50/fold8/epoch=35-train_loss_epoch=0.015602-val_loss_epoch=0.014548-image_size=224-resolution=224-perplexity=5-fc=512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61942b2f5e094747a170cbf8f1e953e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ -6.6641,  -6.1797,  -5.9805,  ...,  -6.3398,  -7.5195,  -6.3984],\n",
      "        [-10.2344,  -9.3828,  -6.0742,  ...,  -7.9531,  -9.8125,  -6.5547],\n",
      "        [ -7.7539,  -7.6133,  -7.2617,  ...,  -7.7812,  -8.0000,  -7.7617],\n",
      "        ...,\n",
      "        [ -6.9258,  -6.4062,  -6.7109,  ...,  -6.3906,  -8.4219,  -5.7266],\n",
      "        [ -6.9688,  -6.8945,  -6.2266,  ...,  -6.4219,  -6.2773,  -5.0391],\n",
      "        [ -7.2031,  -8.0234,  -7.2148,  ...,  -5.8984,  -9.2109,  -8.0000]],\n",
      "       device='cuda:1', dtype=torch.float16)\n",
      "Predictions:  [[1.274e-03 2.068e-03 2.522e-03 ... 1.761e-03 5.422e-04 1.661e-03]\n",
      " [3.594e-05 8.416e-05 2.296e-03 ... 3.514e-04 5.478e-05 1.421e-03]\n",
      " [4.289e-04 4.935e-04 7.014e-04 ... 4.172e-04 3.355e-04 4.256e-04]\n",
      " ...\n",
      " [9.813e-04 1.649e-03 1.216e-03 ... 1.675e-03 2.199e-04 3.248e-03]\n",
      " [9.398e-04 1.012e-03 1.972e-03 ... 1.623e-03 1.875e-03 6.439e-03]\n",
      " [7.439e-04 3.276e-04 7.353e-04 ... 2.737e-03 9.996e-05 3.355e-04]]\n",
      "\n",
      "Inferencing on Fold 9 ......\n",
      "(21432,) (2382,)\n",
      "Loading model from /workspace/Kaggle/MoA/completed/deepinsight_ResNeSt_v2_resnest50/deepinsight_ResNeSt_v2_resnest50/fold9/epoch=35-train_loss_epoch=0.015326-val_loss_epoch=0.014557-image_size=224-resolution=224-perplexity=5-fc=512.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iterations: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465c063725d84ad0ae4be4e6529ec11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-7.9219, -6.7930, -6.6914,  ..., -7.2734, -8.3047, -8.8906],\n",
      "        [-9.5234, -8.7969, -7.2188,  ..., -8.3828, -9.5547, -8.2812],\n",
      "        [-7.5586, -7.4297, -7.3281,  ..., -7.5156, -7.6484, -7.2852],\n",
      "        ...,\n",
      "        [-5.9375, -6.2383, -7.2227,  ..., -6.8984, -8.4453, -6.8633],\n",
      "        [-6.3867, -7.2695, -6.4180,  ..., -6.6797, -8.3516, -6.4297],\n",
      "        [-7.1875, -7.6133, -7.1875,  ..., -7.0625, -8.6641, -7.5703]],\n",
      "       device='cuda:1', dtype=torch.float16)\n",
      "Predictions:  [[3.626e-04 1.121e-03 1.240e-03 ... 6.933e-04 2.472e-04 1.377e-04]\n",
      " [7.313e-05 1.512e-04 7.319e-04 ... 2.288e-04 7.087e-05 2.532e-04]\n",
      " [5.212e-04 5.932e-04 6.566e-04 ... 5.441e-04 4.766e-04 6.852e-04]\n",
      " ...\n",
      " [2.632e-03 1.949e-03 7.296e-04 ... 1.008e-03 2.148e-04 1.044e-03]\n",
      " [1.681e-03 6.962e-04 1.629e-03 ... 1.255e-03 2.359e-04 1.611e-03]\n",
      " [7.553e-04 4.935e-04 7.553e-04 ... 8.559e-04 1.726e-04 5.155e-04]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure Reproducibility\n",
    "seed_everything(rand_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "best_model = None\n",
    "oof_predictions = np.zeros((train_features.shape[0], len(train_classes)))\n",
    "kfold_submit_preds = np.zeros((test_features.shape[0], len(train_classes)))\n",
    "for i, (train_index, val_index) in enumerate(\n",
    "        skf.split(train_features, train_labels[y_labels])):\n",
    "    if training_mode:\n",
    "        print(f\"Training on Fold {i} ......\")\n",
    "        print(train_index.shape, val_index.shape)\n",
    "\n",
    "        logger = TensorBoardLogger(model_output_folder,\n",
    "                                   name=f\"fold{i}/logs\",\n",
    "                                   default_hp_metric=False)\n",
    "\n",
    "        train = train_features.loc[train_index, all_features].copy().values\n",
    "        fold_train_labels = train_labels.loc[train_index,\n",
    "                                             train_classes].copy().values\n",
    "        valid = train_features.loc[val_index, all_features].copy().values\n",
    "        fold_valid_labels = train_labels.loc[val_index,\n",
    "                                             train_classes].copy().values\n",
    "        test = test_features[all_features].copy().values\n",
    "\n",
    "        # LogScaler (Norm-2 Normalization)\n",
    "        print(\"Running norm-2 normalization ......\")\n",
    "        train, valid, test, scaler = norm2_normalization(train, valid, test)\n",
    "        save_pickle(scaler, model_output_folder, i, \"log-scaler\")\n",
    "\n",
    "        # Extract DeepInsight Feature Map\n",
    "        print(\"Extracting feature map ......\")\n",
    "        transformer = extract_feature_map(train,\n",
    "                                          feature_extractor='tsne_exact',\n",
    "                                          resolution=resolution,\n",
    "                                          perplexity=perplexity)\n",
    "        save_pickle(transformer, model_output_folder, i,\n",
    "                    \"deepinsight-transform\")\n",
    "\n",
    "        model = get_model(training_set=(train, fold_train_labels),\n",
    "                          valid_set=(valid, fold_valid_labels),\n",
    "                          test_set=test,\n",
    "                          transformer=transformer)\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss_epoch',\n",
    "                          min_delta=1e-6,\n",
    "                          patience=patience,\n",
    "                          verbose=True,\n",
    "                          mode='min',\n",
    "                          strict=True),\n",
    "            LearningRateMonitor(logging_interval='step')\n",
    "        ]\n",
    "        # https://pytorch-lightning.readthedocs.io/en/latest/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            filepath=f\"{model_output_folder}/fold{i}\" +\n",
    "            \"/{epoch}-{train_loss_epoch:.6f}-{val_loss_epoch:.6f}\" +\n",
    "            f\"-image_size={image_size}-resolution={resolution}-perplexity={perplexity}-fc={fc_size}\",\n",
    "            save_top_k=1,\n",
    "            save_weights_only=False,\n",
    "            save_last=False,\n",
    "            verbose=True,\n",
    "            monitor='val_loss_epoch',\n",
    "            mode='min',\n",
    "            prefix='')\n",
    "\n",
    "        if debug_mode:\n",
    "            # Find best LR\n",
    "            # https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html\n",
    "            trainer = Trainer(\n",
    "                gpus=[gpus[0]],\n",
    "                distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "                auto_lr_find=True,\n",
    "                benchmark=False,\n",
    "                deterministic=True,\n",
    "                logger=logger,\n",
    "                accumulate_grad_batches=accumulate_grad_batches,\n",
    "                gradient_clip_val=gradient_clip_val,\n",
    "                precision=16,\n",
    "                max_epochs=1)\n",
    "\n",
    "            # Run learning rate finder\n",
    "            lr_finder = trainer.tuner.lr_find(\n",
    "                model,\n",
    "                min_lr=1e-7,\n",
    "                max_lr=1e2,\n",
    "                num_training=100,\n",
    "                mode='exponential',\n",
    "                early_stop_threshold=100.0,\n",
    "            )\n",
    "            fig = lr_finder.plot(suggest=True)\n",
    "            fig.show()\n",
    "\n",
    "            # Pick point based on plot, or get suggestion\n",
    "            suggested_lr = lr_finder.suggestion()\n",
    "\n",
    "            # Update hparams of the model\n",
    "            model.hparams.learning_rate = suggested_lr\n",
    "            print(\n",
    "                f\"Suggested Learning Rate: {model.hparams.learning_rate:.6f}\")\n",
    "\n",
    "        else:\n",
    "            trainer = Trainer(\n",
    "                gpus=gpus,\n",
    "                distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "                max_epochs=epochs,\n",
    "                benchmark=False,\n",
    "                deterministic=True,\n",
    "                # fast_dev_run=True,\n",
    "                checkpoint_callback=checkpoint_callback,\n",
    "                callbacks=callbacks,\n",
    "                accumulate_grad_batches=accumulate_grad_batches,\n",
    "                gradient_clip_val=gradient_clip_val,\n",
    "                precision=16,\n",
    "                logger=logger)\n",
    "            trainer.fit(model)\n",
    "\n",
    "            # Load best model\n",
    "            seed_everything(rand_seed)\n",
    "            best_model = MoAResNeSt.load_from_checkpoint(\n",
    "                checkpoint_callback.best_model_path,\n",
    "                pretrained_model_name=pretrained_model,\n",
    "                training_set=(train, fold_train_labels),  # tuple\n",
    "                valid_Set=(valid, fold_valid_labels),  # tuple\n",
    "                test_set=test,\n",
    "                transformer=transformer,\n",
    "                fc_size=fc_size)\n",
    "            best_model.freeze()\n",
    "\n",
    "            print(\"Predicting on validation set ......\")\n",
    "            output = trainer.test(ckpt_path=\"best\",\n",
    "                                  test_dataloaders=model.val_dataloader(),\n",
    "                                  verbose=False)[0]\n",
    "            fold_preds = output[\"pred_probs\"]\n",
    "            oof_predictions[val_index, :] = fold_preds\n",
    "\n",
    "            print(fold_preds[:5, :])\n",
    "            fold_valid_loss = mean_logloss(fold_preds, fold_valid_labels)\n",
    "            print(f\"Fold {i} Validation Loss: {fold_valid_loss:.6f}\")\n",
    "\n",
    "            # Generate submission predictions\n",
    "            print(\"Predicting on test set ......\")\n",
    "            best_model.setup()\n",
    "            output = trainer.test(best_model, verbose=False)[0]\n",
    "            submit_preds = output[\"pred_probs\"]\n",
    "            print(test_features.shape, submit_preds.shape)\n",
    "\n",
    "            kfold_submit_preds += submit_preds / kfolds\n",
    "\n",
    "        del model, trainer, train, valid, test, scaler, transformer\n",
    "    else:\n",
    "        print(f\"Inferencing on Fold {i} ......\")\n",
    "        print(train_index.shape, val_index.shape)\n",
    "\n",
    "        model_path = glob.glob(f'{model_output_folder}/fold{i}/epoch*.ckpt')[0]\n",
    "\n",
    "        test = test_features[all_features].copy().values\n",
    "\n",
    "        # Load LogScaler (Norm-2 Normalization)\n",
    "        scaler = load_pickle(f'{model_output_folder}', i, \"log-scaler\")\n",
    "        test = scaler.transform(test)\n",
    "\n",
    "        # Load DeepInsight Feature Map\n",
    "        transformer = load_pickle(f'{model_output_folder}', i,\n",
    "                                  \"deepinsight-transform\")\n",
    "\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        model = get_model(training_set=(None, None),\n",
    "                          valid_set=(None, None),\n",
    "                          test_set=test,\n",
    "                          transformer=transformer,\n",
    "                          model_path=model_path)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            logger=False,\n",
    "            gpus=gpus,\n",
    "            distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n",
    "            precision=16,\n",
    "            benchmark=False,\n",
    "            deterministic=True)\n",
    "        output = trainer.test(model, verbose=False)[0]\n",
    "        submit_preds = output[\"pred_probs\"]\n",
    "        kfold_submit_preds += submit_preds / kfolds\n",
    "\n",
    "        del model, trainer, scaler, transformer, test\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if debug_mode:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF Validation Loss: 0.014560\n"
     ]
    }
   ],
   "source": [
    "if training_mode:\n",
    "    print(oof_predictions.shape)\n",
    "else:\n",
    "    oof_predictions = glob.glob(f'{model_output_folder}/../oof_*.npy')[0]\n",
    "    oof_predictions = np.load(oof_predictions)\n",
    "\n",
    "oof_loss = mean_logloss(oof_predictions,\n",
    "                        train_labels[train_classes].values)\n",
    "print(f\"OOF Validation Loss: {oof_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof_filename = \"_\".join(\n",
    "#     [f\"{k}={v}\" for k, v in dict(model.hparams).items()])\n",
    "# with open(f'oof_{experiment_name}_{oof_loss}.npy', 'wb') as f:\n",
    "#     np.save(f, oof_predictions)\n",
    "\n",
    "# with open(f'oof_{experiment_name}_{oof_loss}.npy', 'rb') as f:\n",
    "#     tmp = np.load(f)\n",
    "#     print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ResNeSt]\n",
    "# OOF Validation Loss: 0.014620\n",
    "# \"dropblock_prob\":        0.0\n",
    "# \"fc_size\":               512\n",
    "# \"final_drop\":            0.0\n",
    "# \"learning_rate\":         0.000352\n",
    "# \"num_classes\":           206\n",
    "# \"pretrained_model_name\": resnest50_fast_2s2x40d\n",
    "\n",
    "# OOF Validation Loss: 0.014560\n",
    "# \"dropblock_prob\":        0.0\n",
    "# \"fc_size\":               512\n",
    "# \"final_drop\":            0.2\n",
    "# \"learning_rate\":         0.000352\n",
    "# \"num_classes\":           206\n",
    "# \"pretrained_model_name\": resnest50_fast_2s2x40d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_mode and best_model is not None:\n",
    "    print(best_model.hparams)\n",
    "    extra_params = {\n",
    "        \"gpus\": len(gpus),\n",
    "        # \"pos_weight\": True\n",
    "    }\n",
    "    exp_logger.experiment.add_hparams(hparam_dict={\n",
    "        **dict(best_model.hparams),\n",
    "        **extra_params\n",
    "    },\n",
    "                                      metric_dict={\"oof_loss\": oof_loss})\n",
    "\n",
    "    oof_filename = \"_\".join(\n",
    "        [f\"{k}={v}\" for k, v in dict(best_model.hparams).items()])\n",
    "    with open(f'oof_{experiment_name}_{oof_loss}.npy', 'wb') as f:\n",
    "        np.save(f, oof_predictions)\n",
    "\n",
    "    with open(f'oof_{experiment_name}_{oof_loss}.npy', 'rb') as f:\n",
    "        tmp = np.load(f)\n",
    "        print(tmp.shape)\n",
    "\n",
    "    # Rename model filename to remove `=` for Kaggle Dataset rule\n",
    "    model_files = glob.glob(f'{model_output_folder}/fold*/epoch*.ckpt')\n",
    "    for f in model_files:\n",
    "        new_filename = f.replace(\"=\", \"\")\n",
    "        os.rename(f, new_filename)\n",
    "        print(new_filename)\n",
    "\n",
    "    del best_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3982, 206)\n"
     ]
    }
   ],
   "source": [
    "print(kfold_submit_preds.shape)\n",
    "\n",
    "submission = pd.DataFrame(data=test_features[\"sig_id\"].values,\n",
    "                          columns=[\"sig_id\"])\n",
    "submission = submission.reindex(columns=[\"sig_id\"] + train_classes)\n",
    "submission[train_classes] = kfold_submit_preds\n",
    "# Set control type to 0 as control perturbations have no MoAs\n",
    "submission.loc[test_features['cp_type'] == 0, submission.columns[1:]] = 0\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "submission.to_csv('submission_resnest_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>adrenergic_receptor_antagonist</th>\n",
       "      <th>akt_inhibitor</th>\n",
       "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
       "      <th>alk_inhibitor</th>\n",
       "      <th>ampk_activator</th>\n",
       "      <th>analgesic</th>\n",
       "      <th>androgen_receptor_agonist</th>\n",
       "      <th>androgen_receptor_antagonist</th>\n",
       "      <th>anesthetic_-_local</th>\n",
       "      <th>angiogenesis_inhibitor</th>\n",
       "      <th>angiotensin_receptor_antagonist</th>\n",
       "      <th>anti-inflammatory</th>\n",
       "      <th>antiarrhythmic</th>\n",
       "      <th>antibiotic</th>\n",
       "      <th>anticonvulsant</th>\n",
       "      <th>antifungal</th>\n",
       "      <th>antihistamine</th>\n",
       "      <th>antimalarial</th>\n",
       "      <th>antioxidant</th>\n",
       "      <th>antiprotozoal</th>\n",
       "      <th>antiviral</th>\n",
       "      <th>apoptosis_stimulant</th>\n",
       "      <th>aromatase_inhibitor</th>\n",
       "      <th>atm_kinase_inhibitor</th>\n",
       "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
       "      <th>atp_synthase_inhibitor</th>\n",
       "      <th>atpase_inhibitor</th>\n",
       "      <th>atr_kinase_inhibitor</th>\n",
       "      <th>aurora_kinase_inhibitor</th>\n",
       "      <th>autotaxin_inhibitor</th>\n",
       "      <th>bacterial_30s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_50s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_antifolate</th>\n",
       "      <th>bacterial_cell_wall_synthesis_inhibitor</th>\n",
       "      <th>bacterial_dna_gyrase_inhibitor</th>\n",
       "      <th>bacterial_dna_inhibitor</th>\n",
       "      <th>bacterial_membrane_integrity_inhibitor</th>\n",
       "      <th>bcl_inhibitor</th>\n",
       "      <th>bcr-abl_inhibitor</th>\n",
       "      <th>benzodiazepine_receptor_agonist</th>\n",
       "      <th>beta_amyloid_inhibitor</th>\n",
       "      <th>bromodomain_inhibitor</th>\n",
       "      <th>btk_inhibitor</th>\n",
       "      <th>calcineurin_inhibitor</th>\n",
       "      <th>calcium_channel_blocker</th>\n",
       "      <th>cannabinoid_receptor_agonist</th>\n",
       "      <th>cannabinoid_receptor_antagonist</th>\n",
       "      <th>carbonic_anhydrase_inhibitor</th>\n",
       "      <th>casein_kinase_inhibitor</th>\n",
       "      <th>caspase_activator</th>\n",
       "      <th>catechol_o_methyltransferase_inhibitor</th>\n",
       "      <th>cc_chemokine_receptor_antagonist</th>\n",
       "      <th>cck_receptor_antagonist</th>\n",
       "      <th>cdk_inhibitor</th>\n",
       "      <th>chelating_agent</th>\n",
       "      <th>chk_inhibitor</th>\n",
       "      <th>chloride_channel_blocker</th>\n",
       "      <th>cholesterol_inhibitor</th>\n",
       "      <th>cholinergic_receptor_antagonist</th>\n",
       "      <th>coagulation_factor_inhibitor</th>\n",
       "      <th>corticosteroid_agonist</th>\n",
       "      <th>cyclooxygenase_inhibitor</th>\n",
       "      <th>cytochrome_p450_inhibitor</th>\n",
       "      <th>dihydrofolate_reductase_inhibitor</th>\n",
       "      <th>dipeptidyl_peptidase_inhibitor</th>\n",
       "      <th>diuretic</th>\n",
       "      <th>dna_alkylating_agent</th>\n",
       "      <th>dna_inhibitor</th>\n",
       "      <th>dopamine_receptor_agonist</th>\n",
       "      <th>dopamine_receptor_antagonist</th>\n",
       "      <th>egfr_inhibitor</th>\n",
       "      <th>elastase_inhibitor</th>\n",
       "      <th>erbb2_inhibitor</th>\n",
       "      <th>estrogen_receptor_agonist</th>\n",
       "      <th>estrogen_receptor_antagonist</th>\n",
       "      <th>faah_inhibitor</th>\n",
       "      <th>farnesyltransferase_inhibitor</th>\n",
       "      <th>fatty_acid_receptor_agonist</th>\n",
       "      <th>fgfr_inhibitor</th>\n",
       "      <th>flt3_inhibitor</th>\n",
       "      <th>focal_adhesion_kinase_inhibitor</th>\n",
       "      <th>free_radical_scavenger</th>\n",
       "      <th>fungal_squalene_epoxidase_inhibitor</th>\n",
       "      <th>gaba_receptor_agonist</th>\n",
       "      <th>gaba_receptor_antagonist</th>\n",
       "      <th>gamma_secretase_inhibitor</th>\n",
       "      <th>glucocorticoid_receptor_agonist</th>\n",
       "      <th>glutamate_inhibitor</th>\n",
       "      <th>glutamate_receptor_agonist</th>\n",
       "      <th>glutamate_receptor_antagonist</th>\n",
       "      <th>gonadotropin_receptor_agonist</th>\n",
       "      <th>gsk_inhibitor</th>\n",
       "      <th>hcv_inhibitor</th>\n",
       "      <th>hdac_inhibitor</th>\n",
       "      <th>histamine_receptor_agonist</th>\n",
       "      <th>histamine_receptor_antagonist</th>\n",
       "      <th>histone_lysine_demethylase_inhibitor</th>\n",
       "      <th>histone_lysine_methyltransferase_inhibitor</th>\n",
       "      <th>hiv_inhibitor</th>\n",
       "      <th>hmgcr_inhibitor</th>\n",
       "      <th>hsp_inhibitor</th>\n",
       "      <th>igf-1_inhibitor</th>\n",
       "      <th>ikk_inhibitor</th>\n",
       "      <th>imidazoline_receptor_agonist</th>\n",
       "      <th>immunosuppressant</th>\n",
       "      <th>insulin_secretagogue</th>\n",
       "      <th>insulin_sensitizer</th>\n",
       "      <th>integrin_inhibitor</th>\n",
       "      <th>jak_inhibitor</th>\n",
       "      <th>kit_inhibitor</th>\n",
       "      <th>laxative</th>\n",
       "      <th>leukotriene_inhibitor</th>\n",
       "      <th>leukotriene_receptor_antagonist</th>\n",
       "      <th>lipase_inhibitor</th>\n",
       "      <th>lipoxygenase_inhibitor</th>\n",
       "      <th>lxr_agonist</th>\n",
       "      <th>mdm_inhibitor</th>\n",
       "      <th>mek_inhibitor</th>\n",
       "      <th>membrane_integrity_inhibitor</th>\n",
       "      <th>mineralocorticoid_receptor_antagonist</th>\n",
       "      <th>monoacylglycerol_lipase_inhibitor</th>\n",
       "      <th>monoamine_oxidase_inhibitor</th>\n",
       "      <th>monopolar_spindle_1_kinase_inhibitor</th>\n",
       "      <th>mtor_inhibitor</th>\n",
       "      <th>mucolytic_agent</th>\n",
       "      <th>neuropeptide_receptor_antagonist</th>\n",
       "      <th>nfkb_inhibitor</th>\n",
       "      <th>nicotinic_receptor_agonist</th>\n",
       "      <th>nitric_oxide_donor</th>\n",
       "      <th>nitric_oxide_production_inhibitor</th>\n",
       "      <th>nitric_oxide_synthase_inhibitor</th>\n",
       "      <th>norepinephrine_reuptake_inhibitor</th>\n",
       "      <th>nrf2_activator</th>\n",
       "      <th>opioid_receptor_agonist</th>\n",
       "      <th>opioid_receptor_antagonist</th>\n",
       "      <th>orexin_receptor_antagonist</th>\n",
       "      <th>p38_mapk_inhibitor</th>\n",
       "      <th>p-glycoprotein_inhibitor</th>\n",
       "      <th>parp_inhibitor</th>\n",
       "      <th>pdgfr_inhibitor</th>\n",
       "      <th>pdk_inhibitor</th>\n",
       "      <th>phosphodiesterase_inhibitor</th>\n",
       "      <th>phospholipase_inhibitor</th>\n",
       "      <th>pi3k_inhibitor</th>\n",
       "      <th>pkc_inhibitor</th>\n",
       "      <th>potassium_channel_activator</th>\n",
       "      <th>potassium_channel_antagonist</th>\n",
       "      <th>ppar_receptor_agonist</th>\n",
       "      <th>ppar_receptor_antagonist</th>\n",
       "      <th>progesterone_receptor_agonist</th>\n",
       "      <th>progesterone_receptor_antagonist</th>\n",
       "      <th>prostaglandin_inhibitor</th>\n",
       "      <th>prostanoid_receptor_antagonist</th>\n",
       "      <th>proteasome_inhibitor</th>\n",
       "      <th>protein_kinase_inhibitor</th>\n",
       "      <th>protein_phosphatase_inhibitor</th>\n",
       "      <th>protein_synthesis_inhibitor</th>\n",
       "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
       "      <th>radiopaque_medium</th>\n",
       "      <th>raf_inhibitor</th>\n",
       "      <th>ras_gtpase_inhibitor</th>\n",
       "      <th>retinoid_receptor_agonist</th>\n",
       "      <th>retinoid_receptor_antagonist</th>\n",
       "      <th>rho_associated_kinase_inhibitor</th>\n",
       "      <th>ribonucleoside_reductase_inhibitor</th>\n",
       "      <th>rna_polymerase_inhibitor</th>\n",
       "      <th>serotonin_receptor_agonist</th>\n",
       "      <th>serotonin_receptor_antagonist</th>\n",
       "      <th>serotonin_reuptake_inhibitor</th>\n",
       "      <th>sigma_receptor_agonist</th>\n",
       "      <th>sigma_receptor_antagonist</th>\n",
       "      <th>smoothened_receptor_antagonist</th>\n",
       "      <th>sodium_channel_inhibitor</th>\n",
       "      <th>sphingosine_receptor_agonist</th>\n",
       "      <th>src_inhibitor</th>\n",
       "      <th>steroid</th>\n",
       "      <th>syk_inhibitor</th>\n",
       "      <th>tachykinin_antagonist</th>\n",
       "      <th>tgf-beta_receptor_inhibitor</th>\n",
       "      <th>thrombin_inhibitor</th>\n",
       "      <th>thymidylate_synthase_inhibitor</th>\n",
       "      <th>tlr_agonist</th>\n",
       "      <th>tlr_antagonist</th>\n",
       "      <th>tnf_inhibitor</th>\n",
       "      <th>topoisomerase_inhibitor</th>\n",
       "      <th>transient_receptor_potential_channel_antagonist</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>0.027563</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.010979</td>\n",
       "      <td>0.016184</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.005212</td>\n",
       "      <td>0.007125</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.006157</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.005490</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.003297</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.011861</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>0.011977</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.004380</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.039417</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.004117</td>\n",
       "      <td>0.016118</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>0.020471</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.016148</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.024740</td>\n",
       "      <td>0.010955</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>0.037561</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.003174</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.005191</td>\n",
       "      <td>0.017476</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.006742</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.004504</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.004671</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.016724</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.009314</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.007720</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.005113</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>0.017574</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.012823</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.000979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.094656</td>\n",
       "      <td>0.042344</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.014063</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>0.006638</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.009096</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.012573</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.004133</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.005183</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.056598</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.004380</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.167084</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.051338</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.003767</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.103723</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.002273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.019944</td>\n",
       "      <td>0.016188</td>\n",
       "      <td>0.003674</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>0.026644</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.002350</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.005939</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.022744</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.030823</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.014030</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.002644</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>0.023314</td>\n",
       "      <td>0.015707</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.004523</td>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.014185</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.036866</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>0.012254</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>0.005538</td>\n",
       "      <td>0.011145</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.005146</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>0.004520</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.005485</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.007801</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.020443</td>\n",
       "      <td>0.028215</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>0.043101</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.002619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.014584</td>\n",
       "      <td>0.023363</td>\n",
       "      <td>0.006292</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.012243</td>\n",
       "      <td>0.020616</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.009362</td>\n",
       "      <td>0.003649</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>0.005567</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.011342</td>\n",
       "      <td>0.020814</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.007170</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.003916</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.004036</td>\n",
       "      <td>0.036373</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.010122</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.012724</td>\n",
       "      <td>0.010719</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.006160</td>\n",
       "      <td>0.024165</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.018805</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.013951</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.005377</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>0.005280</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.011176</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.006067</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.009717</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>0.014813</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.003767</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.001909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>id_ff7004b87</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.002061</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.157218</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.058630</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.016042</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.039458</td>\n",
       "      <td>0.043610</td>\n",
       "      <td>0.037702</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.258514</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.055602</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.007659</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.016550</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.005733</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.010416</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.310272</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>id_ff925dd0d</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.003624</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.009997</td>\n",
       "      <td>0.017406</td>\n",
       "      <td>0.005986</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.031299</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.005622</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.013557</td>\n",
       "      <td>0.008016</td>\n",
       "      <td>0.013115</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.009278</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.016444</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.006953</td>\n",
       "      <td>0.013498</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.016369</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003761</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.026024</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.005663</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.003986</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.004750</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.003136</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.003698</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.006198</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.016646</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.025152</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>id_ffb710450</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.031978</td>\n",
       "      <td>0.010311</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.007468</td>\n",
       "      <td>0.017680</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.004023</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.010184</td>\n",
       "      <td>0.015766</td>\n",
       "      <td>0.006532</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.004456</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.003778</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.028746</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.017496</td>\n",
       "      <td>0.005091</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.010293</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>0.008883</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.006540</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.005203</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.003446</td>\n",
       "      <td>0.008796</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.009069</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.004771</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.004302</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.006967</td>\n",
       "      <td>0.025555</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.001402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>id_ffbb869f2</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>0.023339</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>0.009129</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.020582</td>\n",
       "      <td>0.038303</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.009034</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.013794</td>\n",
       "      <td>0.007205</td>\n",
       "      <td>0.009421</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.010757</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.004080</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.013470</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.017729</td>\n",
       "      <td>0.008616</td>\n",
       "      <td>0.019443</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.007446</td>\n",
       "      <td>0.007532</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.005870</td>\n",
       "      <td>0.020015</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.023331</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.017355</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.006062</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.006160</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.013278</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>0.003389</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.012715</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.002586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>id_ffd5800b6</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.012737</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.025755</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.005035</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.012033</td>\n",
       "      <td>0.004465</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.043402</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.005021</td>\n",
       "      <td>0.019767</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.008905</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.023062</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.017929</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.013839</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.004201</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.027802</td>\n",
       "      <td>0.015810</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.018617</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3982 rows  207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0     id_0004d9e33                     0.000804                0.001107   \n",
       "1     id_001897cda                     0.000216                0.000199   \n",
       "2     id_002429b5b                     0.000000                0.000000   \n",
       "3     id_00276f245                     0.000406                0.000342   \n",
       "4     id_0027f1083                     0.003126                0.002142   \n",
       "...            ...                          ...                     ...   \n",
       "3977  id_ff7004b87                     0.000316                0.000447   \n",
       "3978  id_ff925dd0d                     0.002801                0.003624   \n",
       "3979  id_ffb710450                     0.003546                0.002088   \n",
       "3980  id_ffbb869f2                     0.000983                0.000667   \n",
       "3981  id_ffd5800b6                     0.001102                0.000674   \n",
       "\n",
       "      acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0           0.001552                        0.012778   \n",
       "1           0.000812                        0.000222   \n",
       "2           0.000000                        0.000000   \n",
       "3           0.001809                        0.019944   \n",
       "4           0.001250                        0.014584   \n",
       "...              ...                             ...   \n",
       "3977        0.000539                        0.002161   \n",
       "3978        0.001073                        0.009997   \n",
       "3979        0.001026                        0.007327   \n",
       "3980        0.001208                        0.022553   \n",
       "3981        0.001446                        0.012737   \n",
       "\n",
       "      acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                              0.027563                        0.004221   \n",
       "1                              0.000884                        0.001523   \n",
       "2                              0.000000                        0.000000   \n",
       "3                              0.016188                        0.003674   \n",
       "4                              0.023363                        0.006292   \n",
       "...                                 ...                             ...   \n",
       "3977                           0.008044                        0.000694   \n",
       "3978                           0.017406                        0.005986   \n",
       "3979                           0.031978                        0.010311   \n",
       "3980                           0.023339                        0.003471   \n",
       "3981                           0.022400                        0.005842   \n",
       "\n",
       "      adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                       0.002629                       0.002348   \n",
       "1                       0.003238                       0.004895   \n",
       "2                       0.000000                       0.000000   \n",
       "3                       0.004747                       0.003792   \n",
       "4                       0.005174                       0.001502   \n",
       "...                          ...                            ...   \n",
       "3977                    0.000622                       0.000738   \n",
       "3978                    0.005878                       0.003193   \n",
       "3979                    0.002705                       0.001655   \n",
       "3980                    0.009129                       0.002413   \n",
       "3981                    0.005421                       0.003731   \n",
       "\n",
       "      adenylyl_cyclase_activator  adrenergic_receptor_agonist  \\\n",
       "0                       0.000242                     0.010979   \n",
       "1                       0.094656                     0.042344   \n",
       "2                       0.000000                     0.000000   \n",
       "3                       0.000416                     0.006956   \n",
       "4                       0.000418                     0.012243   \n",
       "...                          ...                          ...   \n",
       "3977                    0.000170                     0.001570   \n",
       "3978                    0.000239                     0.010948   \n",
       "3979                    0.000223                     0.007468   \n",
       "3980                    0.000569                     0.020582   \n",
       "3981                    0.000248                     0.006764   \n",
       "\n",
       "      adrenergic_receptor_antagonist  akt_inhibitor  \\\n",
       "0                           0.016184       0.000525   \n",
       "1                           0.004660       0.001159   \n",
       "2                           0.000000       0.000000   \n",
       "3                           0.026644       0.000803   \n",
       "4                           0.020616       0.000866   \n",
       "...                              ...            ...   \n",
       "3977                        0.002695       0.000798   \n",
       "3978                        0.031299       0.000739   \n",
       "3979                        0.017680       0.000916   \n",
       "3980                        0.038303       0.001100   \n",
       "3981                        0.025755       0.000813   \n",
       "\n",
       "      aldehyde_dehydrogenase_inhibitor  alk_inhibitor  ampk_activator  \\\n",
       "0                             0.000290       0.000565        0.002206   \n",
       "1                             0.000084       0.014063        0.000160   \n",
       "2                             0.000000       0.000000        0.000000   \n",
       "3                             0.000463       0.001530        0.001523   \n",
       "4                             0.000440       0.000802        0.001782   \n",
       "...                                ...            ...             ...   \n",
       "3977                          0.000650       0.001936        0.000420   \n",
       "3978                          0.000707       0.000340        0.000497   \n",
       "3979                          0.000519       0.000470        0.000776   \n",
       "3980                          0.000341       0.001273        0.001287   \n",
       "3981                          0.000418       0.000611        0.001037   \n",
       "\n",
       "      analgesic  androgen_receptor_agonist  androgen_receptor_antagonist  \\\n",
       "0      0.002196                   0.001404                      0.005212   \n",
       "1      0.000209                   0.000577                      0.001075   \n",
       "2      0.000000                   0.000000                      0.000000   \n",
       "3      0.002350                   0.001517                      0.002825   \n",
       "4      0.001807                   0.001988                      0.006296   \n",
       "...         ...                        ...                           ...   \n",
       "3977   0.000469                   0.000733                      0.001071   \n",
       "3978   0.000770                   0.005622                      0.005969   \n",
       "3979   0.000837                   0.003819                      0.005969   \n",
       "3980   0.002114                   0.002246                      0.006058   \n",
       "3981   0.001453                   0.002089                      0.004253   \n",
       "\n",
       "      anesthetic_-_local  angiogenesis_inhibitor  \\\n",
       "0               0.007125                0.001231   \n",
       "1               0.002302                0.002993   \n",
       "2               0.000000                0.000000   \n",
       "3               0.002842                0.002633   \n",
       "4               0.003873                0.001591   \n",
       "...                  ...                     ...   \n",
       "3977            0.000997                0.000833   \n",
       "3978            0.004484                0.002312   \n",
       "3979            0.003790                0.001268   \n",
       "3980            0.002607                0.002416   \n",
       "3981            0.005035                0.001609   \n",
       "\n",
       "      angiotensin_receptor_antagonist  anti-inflammatory  antiarrhythmic  \\\n",
       "0                            0.006157           0.002454        0.000383   \n",
       "1                            0.000283           0.000561        0.000176   \n",
       "2                            0.000000           0.000000        0.000000   \n",
       "3                            0.002262           0.002457        0.000453   \n",
       "4                            0.008162           0.006484        0.000795   \n",
       "...                               ...                ...             ...   \n",
       "3977                         0.000620           0.000234        0.002061   \n",
       "3978                         0.003288           0.004844        0.000946   \n",
       "3979                         0.003334           0.008837        0.000777   \n",
       "3980                         0.004723           0.004993        0.000549   \n",
       "3981                         0.002777           0.007210        0.000459   \n",
       "\n",
       "      antibiotic  anticonvulsant  antifungal  antihistamine  antimalarial  \\\n",
       "0       0.001867        0.000498    0.000347       0.001066      0.001036   \n",
       "1       0.000220        0.000265    0.000996       0.000503      0.000155   \n",
       "2       0.000000        0.000000    0.000000       0.000000      0.000000   \n",
       "3       0.003317        0.001261    0.000571       0.002749      0.001686   \n",
       "4       0.002455        0.001006    0.000365       0.000635      0.001060   \n",
       "...          ...             ...         ...            ...           ...   \n",
       "3977    0.000823        0.000190    0.003543       0.000839      0.000664   \n",
       "3978    0.002014        0.001184    0.000693       0.000510      0.000718   \n",
       "3979    0.001824        0.001616    0.000499       0.000448      0.000685   \n",
       "3980    0.002733        0.001065    0.000563       0.000942      0.000839   \n",
       "3981    0.001912        0.000832    0.000682       0.000957      0.000802   \n",
       "\n",
       "      antioxidant  antiprotozoal  antiviral  apoptosis_stimulant  \\\n",
       "0        0.005490       0.002314   0.001287             0.003297   \n",
       "1        0.001769       0.000863   0.000180             0.000226   \n",
       "2        0.000000       0.000000   0.000000             0.000000   \n",
       "3        0.005939       0.001304   0.001121             0.002876   \n",
       "4        0.009362       0.003649   0.002450             0.001761   \n",
       "...           ...            ...        ...                  ...   \n",
       "3977     0.002030       0.000362   0.000457             0.002626   \n",
       "3978     0.003423       0.004115   0.003509             0.001998   \n",
       "3979     0.004023       0.003108   0.002968             0.003252   \n",
       "3980     0.009034       0.003443   0.001568             0.001574   \n",
       "3981     0.011159       0.003313   0.001884             0.002239   \n",
       "\n",
       "      aromatase_inhibitor  atm_kinase_inhibitor  \\\n",
       "0                0.003332              0.000286   \n",
       "1                0.000346              0.000644   \n",
       "2                0.000000              0.000000   \n",
       "3                0.002217              0.001043   \n",
       "4                0.002654              0.000507   \n",
       "...                   ...                   ...   \n",
       "3977             0.001542              0.000621   \n",
       "3978             0.003181              0.000290   \n",
       "3979             0.003577              0.000349   \n",
       "3980             0.002824              0.000467   \n",
       "3981             0.002031              0.000321   \n",
       "\n",
       "      atp-sensitive_potassium_channel_antagonist  atp_synthase_inhibitor  \\\n",
       "0                                       0.000420                0.000452   \n",
       "1                                       0.000317                0.000485   \n",
       "2                                       0.000000                0.000000   \n",
       "3                                       0.001045                0.000506   \n",
       "4                                       0.000520                0.000483   \n",
       "...                                          ...                     ...   \n",
       "3977                                    0.000439                0.000106   \n",
       "3978                                    0.000542                0.000500   \n",
       "3979                                    0.000529                0.000385   \n",
       "3980                                    0.000542                0.000461   \n",
       "3981                                    0.000534                0.000424   \n",
       "\n",
       "      atpase_inhibitor  atr_kinase_inhibitor  aurora_kinase_inhibitor  \\\n",
       "0             0.002812              0.000208                 0.000269   \n",
       "1             0.001228              0.008904                 0.006638   \n",
       "2             0.000000              0.000000                 0.000000   \n",
       "3             0.004553              0.000378                 0.001292   \n",
       "4             0.002588              0.000302                 0.000180   \n",
       "...                ...                   ...                      ...   \n",
       "3977          0.006735              0.002218                 0.157218   \n",
       "3978          0.002792              0.000230                 0.000302   \n",
       "3979          0.002331              0.000208                 0.000140   \n",
       "3980          0.001774              0.000446                 0.000176   \n",
       "3981          0.003100              0.000327                 0.000349   \n",
       "\n",
       "      autotaxin_inhibitor  bacterial_30s_ribosomal_subunit_inhibitor  \\\n",
       "0                0.000753                                   0.005327   \n",
       "1                0.000366                                   0.000903   \n",
       "2                0.000000                                   0.000000   \n",
       "3                0.002494                                   0.001601   \n",
       "4                0.000811                                   0.005312   \n",
       "...                   ...                                        ...   \n",
       "3977             0.000663                                   0.000807   \n",
       "3978             0.000325                                   0.004455   \n",
       "3979             0.000402                                   0.004427   \n",
       "3980             0.001434                                   0.003442   \n",
       "3981             0.000559                                   0.003153   \n",
       "\n",
       "      bacterial_50s_ribosomal_subunit_inhibitor  bacterial_antifolate  \\\n",
       "0                                      0.011861              0.001741   \n",
       "1                                      0.000424              0.000283   \n",
       "2                                      0.000000              0.000000   \n",
       "3                                      0.002456              0.001669   \n",
       "4                                      0.005567              0.004221   \n",
       "...                                         ...                   ...   \n",
       "3977                                   0.000262              0.000644   \n",
       "3978                                   0.003804              0.002587   \n",
       "3979                                   0.004015              0.004002   \n",
       "3980                                   0.003079              0.003409   \n",
       "3981                                   0.004736              0.001468   \n",
       "\n",
       "      bacterial_cell_wall_synthesis_inhibitor  bacterial_dna_gyrase_inhibitor  \\\n",
       "0                                    0.011480                        0.008336   \n",
       "1                                    0.001100                        0.000706   \n",
       "2                                    0.000000                        0.000000   \n",
       "3                                    0.022744                        0.002584   \n",
       "4                                    0.011342                        0.020814   \n",
       "...                                       ...                             ...   \n",
       "3977                                 0.000926                        0.000820   \n",
       "3978                                 0.013557                        0.008016   \n",
       "3979                                 0.010184                        0.015766   \n",
       "3980                                 0.013794                        0.007205   \n",
       "3981                                 0.012033                        0.004465   \n",
       "\n",
       "      bacterial_dna_inhibitor  bacterial_membrane_integrity_inhibitor  \\\n",
       "0                    0.011977                                0.000353   \n",
       "1                    0.000428                                0.000152   \n",
       "2                    0.000000                                0.000000   \n",
       "3                    0.003642                                0.000658   \n",
       "4                    0.008728                                0.000499   \n",
       "...                       ...                                     ...   \n",
       "3977                 0.005397                                0.000783   \n",
       "3978                 0.013115                                0.000404   \n",
       "3979                 0.006532                                0.000594   \n",
       "3980                 0.009421                                0.000510   \n",
       "3981                 0.011641                                0.000386   \n",
       "\n",
       "      bcl_inhibitor  bcr-abl_inhibitor  benzodiazepine_receptor_agonist  \\\n",
       "0          0.003873           0.000540                         0.001218   \n",
       "1          0.000575           0.000332                         0.004717   \n",
       "2          0.000000           0.000000                         0.000000   \n",
       "3          0.000881           0.000689                         0.003060   \n",
       "4          0.004075           0.000437                         0.002150   \n",
       "...             ...                ...                              ...   \n",
       "3977       0.000470           0.058630                         0.002481   \n",
       "3978       0.001176           0.001920                         0.003709   \n",
       "3979       0.001072           0.000472                         0.002050   \n",
       "3980       0.001130           0.000544                         0.004619   \n",
       "3981       0.000965           0.000423                         0.002527   \n",
       "\n",
       "      beta_amyloid_inhibitor  bromodomain_inhibitor  btk_inhibitor  \\\n",
       "0                   0.001852               0.001621       0.000335   \n",
       "1                   0.000702               0.005311       0.011216   \n",
       "2                   0.000000               0.000000       0.000000   \n",
       "3                   0.002672               0.000831       0.001435   \n",
       "4                   0.002016               0.001391       0.000387   \n",
       "...                      ...                    ...            ...   \n",
       "3977                0.000842               0.000586       0.001715   \n",
       "3978                0.001062               0.000945       0.000589   \n",
       "3979                0.001345               0.001064       0.000336   \n",
       "3980                0.002684               0.001267       0.000803   \n",
       "3981                0.001611               0.001419       0.000544   \n",
       "\n",
       "      calcineurin_inhibitor  calcium_channel_blocker  \\\n",
       "0                  0.001050                 0.017963   \n",
       "1                  0.000098                 0.009096   \n",
       "2                  0.000000                 0.000000   \n",
       "3                  0.002065                 0.030823   \n",
       "4                  0.000861                 0.007170   \n",
       "...                     ...                      ...   \n",
       "3977               0.000350                 0.016042   \n",
       "3978               0.000703                 0.009278   \n",
       "3979               0.000745                 0.005701   \n",
       "3980               0.000851                 0.010757   \n",
       "3981               0.000861                 0.010018   \n",
       "\n",
       "      cannabinoid_receptor_agonist  cannabinoid_receptor_antagonist  \\\n",
       "0                         0.000730                         0.002243   \n",
       "1                         0.001966                         0.002292   \n",
       "2                         0.000000                         0.000000   \n",
       "3                         0.000626                         0.003148   \n",
       "4                         0.003475                         0.001236   \n",
       "...                            ...                              ...   \n",
       "3977                      0.000673                         0.000165   \n",
       "3978                      0.004361                         0.003054   \n",
       "3979                      0.004456                         0.001688   \n",
       "3980                      0.001375                         0.002989   \n",
       "3981                      0.001891                         0.002078   \n",
       "\n",
       "      carbonic_anhydrase_inhibitor  casein_kinase_inhibitor  \\\n",
       "0                         0.001256                 0.001328   \n",
       "1                         0.001225                 0.001355   \n",
       "2                         0.000000                 0.000000   \n",
       "3                         0.001044                 0.000781   \n",
       "4                         0.002368                 0.003463   \n",
       "...                            ...                      ...   \n",
       "3977                      0.000565                 0.000640   \n",
       "3978                      0.002456                 0.001899   \n",
       "3979                      0.003175                 0.001553   \n",
       "3980                      0.001599                 0.002476   \n",
       "3981                      0.001705                 0.001664   \n",
       "\n",
       "      caspase_activator  catechol_o_methyltransferase_inhibitor  \\\n",
       "0              0.001112                                0.002170   \n",
       "1              0.000512                                0.000197   \n",
       "2              0.000000                                0.000000   \n",
       "3              0.000788                                0.000962   \n",
       "4              0.001504                                0.001939   \n",
       "...                 ...                                     ...   \n",
       "3977           0.000179                                0.000825   \n",
       "3978           0.001291                                0.001537   \n",
       "3979           0.002116                                0.001480   \n",
       "3980           0.001328                                0.001038   \n",
       "3981           0.001220                                0.001685   \n",
       "\n",
       "      cc_chemokine_receptor_antagonist  cck_receptor_antagonist  \\\n",
       "0                             0.006627                 0.001558   \n",
       "1                             0.002049                 0.000871   \n",
       "2                             0.000000                 0.000000   \n",
       "3                             0.014030                 0.000994   \n",
       "4                             0.003916                 0.001867   \n",
       "...                                ...                      ...   \n",
       "3977                          0.001156                 0.000545   \n",
       "3978                          0.005391                 0.003754   \n",
       "3979                          0.002991                 0.003589   \n",
       "3980                          0.004080                 0.001561   \n",
       "3981                          0.006148                 0.001452   \n",
       "\n",
       "      cdk_inhibitor  chelating_agent  chk_inhibitor  chloride_channel_blocker  \\\n",
       "0          0.000832         0.003404       0.000290                  0.005302   \n",
       "1          0.001990         0.000587       0.000139                  0.000254   \n",
       "2          0.000000         0.000000       0.000000                  0.000000   \n",
       "3          0.001004         0.003469       0.000515                  0.003697   \n",
       "4          0.001096         0.004525       0.000322                  0.004169   \n",
       "...             ...              ...            ...                       ...   \n",
       "3977       0.000447         0.000954       0.002901                  0.000763   \n",
       "3978       0.001393         0.002932       0.000264                  0.003083   \n",
       "3979       0.001203         0.002901       0.000314                  0.003778   \n",
       "3980       0.001803         0.003318       0.000394                  0.002608   \n",
       "3981       0.000803         0.002790       0.000353                  0.004241   \n",
       "\n",
       "      cholesterol_inhibitor  cholinergic_receptor_antagonist  \\\n",
       "0                  0.002511                         0.004380   \n",
       "1                  0.008152                         0.000387   \n",
       "2                  0.000000                         0.000000   \n",
       "3                  0.002644                         0.002653   \n",
       "4                  0.001767                         0.003436   \n",
       "...                     ...                              ...   \n",
       "3977               0.000500                         0.000436   \n",
       "3978               0.001628                         0.002447   \n",
       "3979               0.001884                         0.002513   \n",
       "3980               0.001643                         0.003366   \n",
       "3981               0.003919                         0.001591   \n",
       "\n",
       "      coagulation_factor_inhibitor  corticosteroid_agonist  \\\n",
       "0                         0.000646                0.000676   \n",
       "1                         0.000480                0.000211   \n",
       "2                         0.000000                0.000000   \n",
       "3                         0.000453                0.000728   \n",
       "4                         0.000878                0.000952   \n",
       "...                            ...                     ...   \n",
       "3977                      0.001072                0.001689   \n",
       "3978                      0.000813                0.001471   \n",
       "3979                      0.000967                0.001014   \n",
       "3980                      0.000582                0.000950   \n",
       "3981                      0.000589                0.000699   \n",
       "\n",
       "      cyclooxygenase_inhibitor  cytochrome_p450_inhibitor  \\\n",
       "0                     0.039417                   0.005271   \n",
       "1                     0.000933                   0.001126   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.023565                   0.004060   \n",
       "4                     0.017922                   0.006860   \n",
       "...                        ...                        ...   \n",
       "3977                  0.002628                   0.000726   \n",
       "3978                  0.016444                   0.007398   \n",
       "3979                  0.028746                   0.006329   \n",
       "3980                  0.013470                   0.003378   \n",
       "3981                  0.043402                   0.006920   \n",
       "\n",
       "      dihydrofolate_reductase_inhibitor  dipeptidyl_peptidase_inhibitor  \\\n",
       "0                              0.001348                        0.001345   \n",
       "1                              0.000643                        0.000878   \n",
       "2                              0.000000                        0.000000   \n",
       "3                              0.001202                        0.000708   \n",
       "4                              0.002457                        0.001512   \n",
       "...                                 ...                             ...   \n",
       "3977                           0.000257                        0.001066   \n",
       "3978                           0.001516                        0.003585   \n",
       "3979                           0.001788                        0.002888   \n",
       "3980                           0.001802                        0.000955   \n",
       "3981                           0.000939                        0.001892   \n",
       "\n",
       "      diuretic  dna_alkylating_agent  dna_inhibitor  \\\n",
       "0     0.000546              0.004117       0.016118   \n",
       "1     0.000305              0.000564       0.000584   \n",
       "2     0.000000              0.000000       0.000000   \n",
       "3     0.000604              0.001110       0.008322   \n",
       "4     0.000931              0.004036       0.036373   \n",
       "...        ...                   ...            ...   \n",
       "3977  0.000529              0.039458       0.043610   \n",
       "3978  0.002110              0.001046       0.013999   \n",
       "3979  0.001470              0.001183       0.017496   \n",
       "3980  0.000632              0.002038       0.017729   \n",
       "3981  0.000631              0.002765       0.016428   \n",
       "\n",
       "      dopamine_receptor_agonist  dopamine_receptor_antagonist  egfr_inhibitor  \\\n",
       "0                      0.008389                      0.020471        0.000656   \n",
       "1                      0.000597                      0.001438        0.000202   \n",
       "2                      0.000000                      0.000000        0.000000   \n",
       "3                      0.006427                      0.023314        0.015707   \n",
       "4                      0.006430                      0.008718        0.000810   \n",
       "...                         ...                           ...             ...   \n",
       "3977                   0.037702                      0.004902        0.001245   \n",
       "3978                   0.006953                      0.013498        0.000510   \n",
       "3979                   0.005091                      0.014491        0.001100   \n",
       "3980                   0.008616                      0.019443        0.000682   \n",
       "3981                   0.005021                      0.019767        0.000637   \n",
       "\n",
       "      elastase_inhibitor  erbb2_inhibitor  estrogen_receptor_agonist  \\\n",
       "0               0.000860         0.000475                   0.016148   \n",
       "1               0.000448         0.000482                   0.000912   \n",
       "2               0.000000         0.000000                   0.000000   \n",
       "3               0.000481         0.001018                   0.002826   \n",
       "4               0.001096         0.000667                   0.010122   \n",
       "...                  ...              ...                        ...   \n",
       "3977            0.000385         0.000337                   0.002045   \n",
       "3978            0.000806         0.000481                   0.016369   \n",
       "3979            0.000939         0.000447                   0.010293   \n",
       "3980            0.000888         0.000546                   0.003367   \n",
       "3981            0.001292         0.000464                   0.008089   \n",
       "\n",
       "      estrogen_receptor_antagonist  faah_inhibitor  \\\n",
       "0                         0.000868        0.002394   \n",
       "1                         0.000355        0.001022   \n",
       "2                         0.000000        0.000000   \n",
       "3                         0.002366        0.003670   \n",
       "4                         0.000596        0.000990   \n",
       "...                            ...             ...   \n",
       "3977                      0.000180        0.000714   \n",
       "3978                      0.002465        0.000486   \n",
       "3979                      0.001257        0.000434   \n",
       "3980                      0.001197        0.001746   \n",
       "3981                      0.001333        0.001413   \n",
       "\n",
       "      farnesyltransferase_inhibitor  fatty_acid_receptor_agonist  \\\n",
       "0                          0.000279                     0.000885   \n",
       "1                          0.000112                     0.001637   \n",
       "2                          0.000000                     0.000000   \n",
       "3                          0.000687                     0.000772   \n",
       "4                          0.000384                     0.002626   \n",
       "...                             ...                          ...   \n",
       "3977                       0.002631                     0.000520   \n",
       "3978                       0.000250                     0.001429   \n",
       "3979                       0.000276                     0.001194   \n",
       "3980                       0.000328                     0.002523   \n",
       "3981                       0.000195                     0.000764   \n",
       "\n",
       "      fgfr_inhibitor  flt3_inhibitor  focal_adhesion_kinase_inhibitor  \\\n",
       "0           0.000340        0.000278                         0.000209   \n",
       "1           0.001401        0.000933                         0.000645   \n",
       "2           0.000000        0.000000                         0.000000   \n",
       "3           0.006028        0.000700                         0.000503   \n",
       "4           0.000462        0.000317                         0.000319   \n",
       "...              ...             ...                              ...   \n",
       "3977        0.000289        0.258514                         0.000411   \n",
       "3978        0.000596        0.000329                         0.000340   \n",
       "3979        0.000401        0.000193                         0.000365   \n",
       "3980        0.000663        0.000280                         0.000380   \n",
       "3981        0.000410        0.000223                         0.000206   \n",
       "\n",
       "      free_radical_scavenger  fungal_squalene_epoxidase_inhibitor  \\\n",
       "0                   0.001585                             0.000535   \n",
       "1                   0.000379                             0.000280   \n",
       "2                   0.000000                             0.000000   \n",
       "3                   0.000790                             0.000881   \n",
       "4                   0.001637                             0.000354   \n",
       "...                      ...                                  ...   \n",
       "3977                0.000451                             0.000674   \n",
       "3978                0.003761                             0.001218   \n",
       "3979                0.001900                             0.000368   \n",
       "3980                0.001331                             0.000429   \n",
       "3981                0.001609                             0.000359   \n",
       "\n",
       "      gaba_receptor_agonist  gaba_receptor_antagonist  \\\n",
       "0                  0.024740                  0.010955   \n",
       "1                  0.000780                  0.001472   \n",
       "2                  0.000000                  0.000000   \n",
       "3                  0.004523                  0.003371   \n",
       "4                  0.012724                  0.010719   \n",
       "...                     ...                       ...   \n",
       "3977               0.002740                  0.001623   \n",
       "3978               0.004587                  0.011004   \n",
       "3979               0.004455                  0.010769   \n",
       "3980               0.007446                  0.007532   \n",
       "3981               0.008905                  0.013421   \n",
       "\n",
       "      gamma_secretase_inhibitor  glucocorticoid_receptor_agonist  \\\n",
       "0                      0.000620                         0.001000   \n",
       "1                      0.002694                         0.000162   \n",
       "2                      0.000000                         0.000000   \n",
       "3                      0.000737                         0.001133   \n",
       "4                      0.000320                         0.001755   \n",
       "...                         ...                              ...   \n",
       "3977                   0.003646                         0.000517   \n",
       "3978                   0.000609                         0.001565   \n",
       "3979                   0.000326                         0.001743   \n",
       "3980                   0.000401                         0.001969   \n",
       "3981                   0.001303                         0.000764   \n",
       "\n",
       "      glutamate_inhibitor  glutamate_receptor_agonist  \\\n",
       "0                0.001426                    0.008225   \n",
       "1                0.000759                    0.001240   \n",
       "2                0.000000                    0.000000   \n",
       "3                0.000633                    0.002011   \n",
       "4                0.001218                    0.006160   \n",
       "...                   ...                         ...   \n",
       "3977             0.003816                    0.001208   \n",
       "3978             0.000993                    0.008681   \n",
       "3979             0.000533                    0.005282   \n",
       "3980             0.000797                    0.005870   \n",
       "3981             0.000850                    0.004952   \n",
       "\n",
       "      glutamate_receptor_antagonist  gonadotropin_receptor_agonist  \\\n",
       "0                          0.037561                       0.002485   \n",
       "1                          0.002954                       0.000373   \n",
       "2                          0.000000                       0.000000   \n",
       "3                          0.014185                       0.002502   \n",
       "4                          0.024165                       0.002873   \n",
       "...                             ...                            ...   \n",
       "3977                       0.004917                       0.000349   \n",
       "3978                       0.026024                       0.001859   \n",
       "3979                       0.020011                       0.001519   \n",
       "3980                       0.020015                       0.003247   \n",
       "3981                       0.023062                       0.002609   \n",
       "\n",
       "      gsk_inhibitor  hcv_inhibitor  hdac_inhibitor  \\\n",
       "0          0.000631       0.003174        0.000960   \n",
       "1          0.001444       0.012573        0.000221   \n",
       "2          0.000000       0.000000        0.000000   \n",
       "3          0.000872       0.003380        0.001260   \n",
       "4          0.000525       0.002477        0.001391   \n",
       "...             ...            ...             ...   \n",
       "3977       0.000104       0.008690        0.000301   \n",
       "3978       0.000842       0.002412        0.001293   \n",
       "3979       0.000557       0.001237        0.001026   \n",
       "3980       0.000584       0.003483        0.001463   \n",
       "3981       0.001683       0.002932        0.000831   \n",
       "\n",
       "      histamine_receptor_agonist  histamine_receptor_antagonist  \\\n",
       "0                       0.005191                       0.017476   \n",
       "1                       0.000151                       0.001725   \n",
       "2                       0.000000                       0.000000   \n",
       "3                       0.003488                       0.036866   \n",
       "4                       0.002697                       0.018805   \n",
       "...                          ...                            ...   \n",
       "3977                    0.000834                       0.001211   \n",
       "3978                    0.006116                       0.011154   \n",
       "3979                    0.005583                       0.008883   \n",
       "3980                    0.001972                       0.023331   \n",
       "3981                    0.005727                       0.017929   \n",
       "\n",
       "      histone_lysine_demethylase_inhibitor  \\\n",
       "0                                 0.000220   \n",
       "1                                 0.004133   \n",
       "2                                 0.000000   \n",
       "3                                 0.001704   \n",
       "4                                 0.000276   \n",
       "...                                    ...   \n",
       "3977                              0.000489   \n",
       "3978                              0.000505   \n",
       "3979                              0.000417   \n",
       "3980                              0.000638   \n",
       "3981                              0.000422   \n",
       "\n",
       "      histone_lysine_methyltransferase_inhibitor  hiv_inhibitor  \\\n",
       "0                                       0.000510       0.002525   \n",
       "1                                       0.004272       0.000772   \n",
       "2                                       0.000000       0.000000   \n",
       "3                                       0.003076       0.002141   \n",
       "4                                       0.000645       0.002431   \n",
       "...                                          ...            ...   \n",
       "3977                                    0.006738       0.000289   \n",
       "3978                                    0.000905       0.004999   \n",
       "3979                                    0.000579       0.003961   \n",
       "3980                                    0.000841       0.003988   \n",
       "3981                                    0.001080       0.001840   \n",
       "\n",
       "      hmgcr_inhibitor  hsp_inhibitor  igf-1_inhibitor  ikk_inhibitor  \\\n",
       "0            0.001765       0.000742         0.000646       0.001008   \n",
       "1            0.000173       0.000644         0.002015       0.000400   \n",
       "2            0.000000       0.000000         0.000000       0.000000   \n",
       "3            0.012254       0.000484         0.002562       0.000277   \n",
       "4            0.000949       0.001147         0.000234       0.000736   \n",
       "...               ...            ...              ...            ...   \n",
       "3977         0.000400       0.002690         0.004522       0.004142   \n",
       "3978         0.000238       0.001685         0.000266       0.000774   \n",
       "3979         0.000488       0.000844         0.000294       0.000615   \n",
       "3980         0.002035       0.000694         0.000578       0.000589   \n",
       "3981         0.000596       0.000496         0.000537       0.000899   \n",
       "\n",
       "      imidazoline_receptor_agonist  immunosuppressant  insulin_secretagogue  \\\n",
       "0                         0.002798           0.002246              0.001754   \n",
       "1                         0.000723           0.000436              0.000685   \n",
       "2                         0.000000           0.000000              0.000000   \n",
       "3                         0.002049           0.003738              0.001888   \n",
       "4                         0.002164           0.002229              0.003334   \n",
       "...                            ...                ...                   ...   \n",
       "3977                      0.001032           0.002016              0.000234   \n",
       "3978                      0.001701           0.000969              0.001453   \n",
       "3979                      0.001863           0.001001              0.001665   \n",
       "3980                      0.001760           0.002511              0.003635   \n",
       "3981                      0.004283           0.002088              0.001209   \n",
       "\n",
       "      insulin_sensitizer  integrin_inhibitor  jak_inhibitor  kit_inhibitor  \\\n",
       "0               0.001179            0.002642       0.001632       0.000492   \n",
       "1               0.002498            0.003980       0.001885       0.000383   \n",
       "2               0.000000            0.000000       0.000000       0.000000   \n",
       "3               0.000970            0.002302       0.001025       0.000329   \n",
       "4               0.001696            0.003417       0.000256       0.000794   \n",
       "...                  ...                 ...            ...            ...   \n",
       "3977            0.000467            0.000661       0.055602       0.000862   \n",
       "3978            0.000718            0.003755       0.000295       0.001314   \n",
       "3979            0.000920            0.002427       0.000154       0.000421   \n",
       "3980            0.004757            0.005413       0.000321       0.000529   \n",
       "3981            0.001092            0.002283       0.000890       0.000387   \n",
       "\n",
       "      laxative  leukotriene_inhibitor  leukotriene_receptor_antagonist  \\\n",
       "0     0.000557               0.000954                         0.002884   \n",
       "1     0.000276               0.000311                         0.003627   \n",
       "2     0.000000               0.000000                         0.000000   \n",
       "3     0.000537               0.001208                         0.001391   \n",
       "4     0.000745               0.000871                         0.005263   \n",
       "...        ...                    ...                              ...   \n",
       "3977  0.000365               0.000653                         0.002865   \n",
       "3978  0.001140               0.001083                         0.002305   \n",
       "3979  0.000813               0.001004                         0.003263   \n",
       "3980  0.000580               0.000681                         0.005528   \n",
       "3981  0.000477               0.000707                         0.005969   \n",
       "\n",
       "      lipase_inhibitor  lipoxygenase_inhibitor  lxr_agonist  mdm_inhibitor  \\\n",
       "0             0.001556                0.002021     0.000659       0.000272   \n",
       "1             0.000499                0.001112     0.000191       0.000223   \n",
       "2             0.000000                0.000000     0.000000       0.000000   \n",
       "3             0.001829                0.001103     0.004075       0.000593   \n",
       "4             0.000944                0.002672     0.000503       0.000302   \n",
       "...                ...                     ...          ...            ...   \n",
       "3977          0.000523                0.002541     0.000214       0.000251   \n",
       "3978          0.000668                0.002236     0.000369       0.000386   \n",
       "3979          0.000675                0.001390     0.000391       0.000263   \n",
       "3980          0.001248                0.001732     0.000514       0.000457   \n",
       "3981          0.001119                0.002286     0.000448       0.000286   \n",
       "\n",
       "      mek_inhibitor  membrane_integrity_inhibitor  \\\n",
       "0          0.000436                      0.006742   \n",
       "1          0.000129                      0.000798   \n",
       "2          0.000000                      0.000000   \n",
       "3          0.004360                      0.005538   \n",
       "4          0.000280                      0.013951   \n",
       "...             ...                           ...   \n",
       "3977       0.001084                      0.000567   \n",
       "3978       0.000528                      0.004170   \n",
       "3979       0.000339                      0.006540   \n",
       "3980       0.000366                      0.008281   \n",
       "3981       0.000422                      0.005406   \n",
       "\n",
       "      mineralocorticoid_receptor_antagonist  \\\n",
       "0                                  0.002239   \n",
       "1                                  0.000403   \n",
       "2                                  0.000000   \n",
       "3                                  0.011145   \n",
       "4                                  0.000845   \n",
       "...                                     ...   \n",
       "3977                               0.002358   \n",
       "3978                               0.000493   \n",
       "3979                               0.000533   \n",
       "3980                               0.001732   \n",
       "3981                               0.001435   \n",
       "\n",
       "      monoacylglycerol_lipase_inhibitor  monoamine_oxidase_inhibitor  \\\n",
       "0                              0.001190                     0.005102   \n",
       "1                              0.000198                     0.001236   \n",
       "2                              0.000000                     0.000000   \n",
       "3                              0.000561                     0.005146   \n",
       "4                              0.000931                     0.005377   \n",
       "...                                 ...                          ...   \n",
       "3977                           0.000525                     0.000715   \n",
       "3978                           0.001346                     0.005663   \n",
       "3979                           0.001241                     0.005200   \n",
       "3980                           0.000497                     0.004721   \n",
       "3981                           0.000870                     0.012696   \n",
       "\n",
       "      monopolar_spindle_1_kinase_inhibitor  mtor_inhibitor  mucolytic_agent  \\\n",
       "0                                 0.000290        0.001324         0.005559   \n",
       "1                                 0.000953        0.001851         0.000207   \n",
       "2                                 0.000000        0.000000         0.000000   \n",
       "3                                 0.000567        0.001076         0.003721   \n",
       "4                                 0.000338        0.001084         0.003646   \n",
       "...                                    ...             ...              ...   \n",
       "3977                              0.000726        0.000805         0.001622   \n",
       "3978                              0.000950        0.001635         0.003986   \n",
       "3979                              0.000422        0.001352         0.005203   \n",
       "3980                              0.000435        0.002168         0.004122   \n",
       "3981                              0.000319        0.001393         0.004567   \n",
       "\n",
       "      neuropeptide_receptor_antagonist  nfkb_inhibitor  \\\n",
       "0                             0.000931        0.004504   \n",
       "1                             0.000863        0.000676   \n",
       "2                             0.000000        0.000000   \n",
       "3                             0.001442        0.002147   \n",
       "4                             0.001527        0.003425   \n",
       "...                                ...             ...   \n",
       "3977                          0.000941        0.002699   \n",
       "3978                          0.001444        0.005406   \n",
       "3979                          0.002000        0.002784   \n",
       "3980                          0.002276        0.002363   \n",
       "3981                          0.001615        0.004619   \n",
       "\n",
       "      nicotinic_receptor_agonist  nitric_oxide_donor  \\\n",
       "0                       0.000972            0.000967   \n",
       "1                       0.000409            0.000344   \n",
       "2                       0.000000            0.000000   \n",
       "3                       0.000443            0.002267   \n",
       "4                       0.000925            0.003129   \n",
       "...                          ...                 ...   \n",
       "3977                    0.000652            0.000168   \n",
       "3978                    0.001158            0.002247   \n",
       "3979                    0.001090            0.003875   \n",
       "3980                    0.000629            0.004066   \n",
       "3981                    0.001055            0.002380   \n",
       "\n",
       "      nitric_oxide_production_inhibitor  nitric_oxide_synthase_inhibitor  \\\n",
       "0                              0.000648                         0.001571   \n",
       "1                              0.000149                         0.000153   \n",
       "2                              0.000000                         0.000000   \n",
       "3                              0.001844                         0.000797   \n",
       "4                              0.000737                         0.003368   \n",
       "...                                 ...                              ...   \n",
       "3977                           0.000481                         0.000673   \n",
       "3978                           0.000386                         0.002053   \n",
       "3979                           0.000518                         0.004858   \n",
       "3980                           0.001225                         0.001369   \n",
       "3981                           0.000824                         0.002275   \n",
       "\n",
       "      norepinephrine_reuptake_inhibitor  nrf2_activator  \\\n",
       "0                              0.000454        0.001113   \n",
       "1                              0.000402        0.000145   \n",
       "2                              0.000000        0.000000   \n",
       "3                              0.000442        0.000649   \n",
       "4                              0.000687        0.000951   \n",
       "...                                 ...             ...   \n",
       "3977                           0.000340        0.001013   \n",
       "3978                           0.001318        0.000913   \n",
       "3979                           0.001212        0.001315   \n",
       "3980                           0.000562        0.000626   \n",
       "3981                           0.000735        0.001017   \n",
       "\n",
       "      opioid_receptor_agonist  opioid_receptor_antagonist  \\\n",
       "0                    0.002337                    0.004671   \n",
       "1                    0.000312                    0.002038   \n",
       "2                    0.000000                    0.000000   \n",
       "3                    0.004586                    0.003254   \n",
       "4                    0.002628                    0.005610   \n",
       "...                       ...                         ...   \n",
       "3977                 0.004874                    0.000562   \n",
       "3978                 0.004750                    0.006411   \n",
       "3979                 0.003446                    0.008796   \n",
       "3980                 0.005690                    0.003802   \n",
       "3981                 0.002343                    0.005389   \n",
       "\n",
       "      orexin_receptor_antagonist  p38_mapk_inhibitor  \\\n",
       "0                       0.001482            0.000365   \n",
       "1                       0.000615            0.001490   \n",
       "2                       0.000000            0.000000   \n",
       "3                       0.003114            0.000705   \n",
       "4                       0.005280            0.000746   \n",
       "...                          ...                 ...   \n",
       "3977                    0.000671            0.000397   \n",
       "3978                    0.004601            0.000642   \n",
       "3979                    0.005620            0.001102   \n",
       "3980                    0.002267            0.000474   \n",
       "3981                    0.003585            0.000324   \n",
       "\n",
       "      p-glycoprotein_inhibitor  parp_inhibitor  pdgfr_inhibitor  \\\n",
       "0                     0.000677        0.000561         0.000346   \n",
       "1                     0.001550        0.005183         0.001085   \n",
       "2                     0.000000        0.000000         0.000000   \n",
       "3                     0.000606        0.000591         0.000472   \n",
       "4                     0.000762        0.001540         0.000342   \n",
       "...                        ...             ...              ...   \n",
       "3977                  0.000264        0.002293         0.001816   \n",
       "3978                  0.002118        0.003136         0.001291   \n",
       "3979                  0.001096        0.001347         0.000282   \n",
       "3980                  0.001298        0.000755         0.000338   \n",
       "3981                  0.000885        0.000577         0.000215   \n",
       "\n",
       "      pdk_inhibitor  phosphodiesterase_inhibitor  phospholipase_inhibitor  \\\n",
       "0          0.000740                     0.016724                 0.002532   \n",
       "1          0.001027                     0.056598                 0.000134   \n",
       "2          0.000000                     0.000000                 0.000000   \n",
       "3          0.000452                     0.007174                 0.005331   \n",
       "4          0.001289                     0.011176                 0.001534   \n",
       "...             ...                          ...                      ...   \n",
       "3977       0.000861                     0.002908                 0.000345   \n",
       "3978       0.002029                     0.014763                 0.001309   \n",
       "3979       0.001238                     0.009069                 0.001349   \n",
       "3980       0.000843                     0.017355                 0.001621   \n",
       "3981       0.000988                     0.013839                 0.003551   \n",
       "\n",
       "      pi3k_inhibitor  pkc_inhibitor  potassium_channel_activator  \\\n",
       "0           0.001410       0.000988                     0.003393   \n",
       "1           0.004380       0.000834                     0.001120   \n",
       "2           0.000000       0.000000                     0.000000   \n",
       "3           0.004520       0.002408                     0.005485   \n",
       "4           0.000778       0.002164                     0.004500   \n",
       "...              ...            ...                          ...   \n",
       "3977        0.007659       0.000258                     0.000351   \n",
       "3978        0.000960       0.001312                     0.003698   \n",
       "3979        0.000669       0.001524                     0.003551   \n",
       "3980        0.002505       0.002646                     0.009847   \n",
       "3981        0.001426       0.000948                     0.004201   \n",
       "\n",
       "      potassium_channel_antagonist  ppar_receptor_agonist  \\\n",
       "0                         0.006565               0.001740   \n",
       "1                         0.000923               0.001564   \n",
       "2                         0.000000               0.000000   \n",
       "3                         0.007542               0.000996   \n",
       "4                         0.003904               0.004489   \n",
       "...                            ...                    ...   \n",
       "3977                      0.001293               0.002197   \n",
       "3978                      0.003021               0.002550   \n",
       "3979                      0.003603               0.003563   \n",
       "3980                      0.006062               0.003715   \n",
       "3981                      0.009070               0.001924   \n",
       "\n",
       "      ppar_receptor_antagonist  progesterone_receptor_agonist  \\\n",
       "0                     0.002463                       0.009314   \n",
       "1                     0.001870                       0.000145   \n",
       "2                     0.000000                       0.000000   \n",
       "3                     0.002492                       0.001594   \n",
       "4                     0.001289                       0.005545   \n",
       "...                        ...                            ...   \n",
       "3977                  0.000149                       0.001221   \n",
       "3978                  0.000798                       0.006198   \n",
       "3979                  0.001130                       0.004771   \n",
       "3980                  0.001563                       0.003406   \n",
       "3981                  0.002106                       0.002833   \n",
       "\n",
       "      progesterone_receptor_antagonist  prostaglandin_inhibitor  \\\n",
       "0                             0.000864                 0.004303   \n",
       "1                             0.000894                 0.000313   \n",
       "2                             0.000000                 0.000000   \n",
       "3                             0.001108                 0.002889   \n",
       "4                             0.000622                 0.002669   \n",
       "...                                ...                      ...   \n",
       "3977                          0.000614                 0.000230   \n",
       "3978                          0.000443                 0.002370   \n",
       "3979                          0.000430                 0.002556   \n",
       "3980                          0.000812                 0.002177   \n",
       "3981                          0.000724                 0.002559   \n",
       "\n",
       "      prostanoid_receptor_antagonist  proteasome_inhibitor  \\\n",
       "0                           0.007720              0.000300   \n",
       "1                           0.000584              0.000175   \n",
       "2                           0.000000              0.000000   \n",
       "3                           0.002548              0.000437   \n",
       "4                           0.003282              0.000478   \n",
       "...                              ...                   ...   \n",
       "3977                        0.001011              0.000348   \n",
       "3978                        0.006111              0.000387   \n",
       "3979                        0.004302              0.000546   \n",
       "3980                        0.003061              0.000259   \n",
       "3981                        0.009246              0.000525   \n",
       "\n",
       "      protein_kinase_inhibitor  protein_phosphatase_inhibitor  \\\n",
       "0                     0.003334                       0.000378   \n",
       "1                     0.000854                       0.000321   \n",
       "2                     0.000000                       0.000000   \n",
       "3                     0.007801                       0.000304   \n",
       "4                     0.002196                       0.000703   \n",
       "...                        ...                            ...   \n",
       "3977                  0.000980                       0.000311   \n",
       "3978                  0.001064                       0.000665   \n",
       "3979                  0.001187                       0.000608   \n",
       "3980                  0.002632                       0.000530   \n",
       "3981                  0.002402                       0.000492   \n",
       "\n",
       "      protein_synthesis_inhibitor  protein_tyrosine_kinase_inhibitor  \\\n",
       "0                        0.005113                           0.000906   \n",
       "1                        0.000301                           0.001264   \n",
       "2                        0.000000                           0.000000   \n",
       "3                        0.006022                           0.001472   \n",
       "4                        0.006067                           0.001427   \n",
       "...                           ...                                ...   \n",
       "3977                     0.001196                           0.000915   \n",
       "3978                     0.002161                           0.002018   \n",
       "3979                     0.003693                           0.001720   \n",
       "3980                     0.003300                           0.001624   \n",
       "3981                     0.002583                           0.001041   \n",
       "\n",
       "      radiopaque_medium  raf_inhibitor  ras_gtpase_inhibitor  \\\n",
       "0              0.003468       0.000565              0.000521   \n",
       "1              0.000386       0.000442              0.000270   \n",
       "2              0.000000       0.000000              0.000000   \n",
       "3              0.001625       0.000270              0.001100   \n",
       "4              0.009717       0.000548              0.001243   \n",
       "...                 ...            ...                   ...   \n",
       "3977           0.000446       0.000171              0.000321   \n",
       "3978           0.004339       0.000490              0.001059   \n",
       "3979           0.006995       0.000298              0.001005   \n",
       "3980           0.006160       0.000390              0.000702   \n",
       "3981           0.003316       0.000334              0.000824   \n",
       "\n",
       "      retinoid_receptor_agonist  retinoid_receptor_antagonist  \\\n",
       "0                      0.000437                      0.000656   \n",
       "1                      0.001660                      0.000195   \n",
       "2                      0.000000                      0.000000   \n",
       "3                      0.000280                      0.001309   \n",
       "4                      0.001383                      0.000678   \n",
       "...                         ...                           ...   \n",
       "3977                   0.000083                      0.000246   \n",
       "3978                   0.000859                      0.000868   \n",
       "3979                   0.003721                      0.000564   \n",
       "3980                   0.000379                      0.000683   \n",
       "3981                   0.000399                      0.000838   \n",
       "\n",
       "      rho_associated_kinase_inhibitor  ribonucleoside_reductase_inhibitor  \\\n",
       "0                            0.000497                            0.001046   \n",
       "1                            0.167084                            0.000245   \n",
       "2                            0.000000                            0.000000   \n",
       "3                            0.000966                            0.002024   \n",
       "4                            0.001219                            0.002407   \n",
       "...                               ...                                 ...   \n",
       "3977                         0.000523                            0.000352   \n",
       "3978                         0.000810                            0.001614   \n",
       "3979                         0.001580                            0.002300   \n",
       "3980                         0.001049                            0.003205   \n",
       "3981                         0.000694                            0.001860   \n",
       "\n",
       "      rna_polymerase_inhibitor  serotonin_receptor_agonist  \\\n",
       "0                     0.004637                    0.017574   \n",
       "1                     0.000876                    0.051338   \n",
       "2                     0.000000                    0.000000   \n",
       "3                     0.001297                    0.020443   \n",
       "4                     0.002227                    0.008630   \n",
       "...                        ...                         ...   \n",
       "3977                  0.000781                    0.016550   \n",
       "3978                  0.001295                    0.010742   \n",
       "3979                  0.001064                    0.006967   \n",
       "3980                  0.001328                    0.013278   \n",
       "3981                  0.004545                    0.027802   \n",
       "\n",
       "      serotonin_receptor_antagonist  serotonin_reuptake_inhibitor  \\\n",
       "0                          0.011928                      0.002010   \n",
       "1                          0.001505                      0.000763   \n",
       "2                          0.000000                      0.000000   \n",
       "3                          0.028215                      0.004444   \n",
       "4                          0.014813                      0.002045   \n",
       "...                             ...                           ...   \n",
       "3977                       0.001820                      0.001042   \n",
       "3978                       0.016646                      0.002892   \n",
       "3979                       0.025555                      0.001889   \n",
       "3980                       0.018281                      0.003389   \n",
       "3981                       0.015810                      0.002983   \n",
       "\n",
       "      sigma_receptor_agonist  sigma_receptor_antagonist  \\\n",
       "0                   0.004528                   0.000960   \n",
       "1                   0.000711                   0.001013   \n",
       "2                   0.000000                   0.000000   \n",
       "3                   0.002027                   0.002681   \n",
       "4                   0.002820                   0.001129   \n",
       "...                      ...                        ...   \n",
       "3977                0.001182                   0.000879   \n",
       "3978                0.001920                   0.001186   \n",
       "3979                0.001778                   0.001383   \n",
       "3980                0.001829                   0.001521   \n",
       "3981                0.002148                   0.001360   \n",
       "\n",
       "      smoothened_receptor_antagonist  sodium_channel_inhibitor  \\\n",
       "0                           0.002169                  0.012823   \n",
       "1                           0.003767                  0.004358   \n",
       "2                           0.000000                  0.000000   \n",
       "3                           0.001761                  0.003448   \n",
       "4                           0.000999                  0.013145   \n",
       "...                              ...                       ...   \n",
       "3977                        0.000386                  0.005733   \n",
       "3978                        0.001012                  0.025152   \n",
       "3979                        0.000668                  0.022095   \n",
       "3980                        0.002172                  0.012715   \n",
       "3981                        0.001238                  0.018617   \n",
       "\n",
       "      sphingosine_receptor_agonist  src_inhibitor   steroid  syk_inhibitor  \\\n",
       "0                         0.004890       0.000860  0.000974       0.000516   \n",
       "1                         0.000075       0.103723  0.000096       0.001210   \n",
       "2                         0.000000       0.000000  0.000000       0.000000   \n",
       "3                         0.003011       0.000873  0.001462       0.000421   \n",
       "4                         0.002797       0.000519  0.001803       0.000469   \n",
       "...                            ...            ...       ...            ...   \n",
       "3977                      0.000307       0.010416  0.000129       0.000279   \n",
       "3978                      0.000391       0.001350  0.000473       0.001264   \n",
       "3979                      0.001082       0.000499  0.001047       0.000476   \n",
       "3980                      0.001954       0.000652  0.000941       0.000494   \n",
       "3981                      0.001373       0.001832  0.000585       0.000649   \n",
       "\n",
       "      tachykinin_antagonist  tgf-beta_receptor_inhibitor  thrombin_inhibitor  \\\n",
       "0                  0.001946                     0.000207            0.000418   \n",
       "1                  0.000811                     0.001363            0.000247   \n",
       "2                  0.000000                     0.000000            0.000000   \n",
       "3                  0.004121                     0.000410            0.001121   \n",
       "4                  0.003767                     0.000381            0.001439   \n",
       "...                     ...                          ...                 ...   \n",
       "3977               0.000852                     0.000149            0.000344   \n",
       "3978               0.002325                     0.000233            0.001401   \n",
       "3979               0.002612                     0.000349            0.002415   \n",
       "3980               0.006241                     0.000332            0.001985   \n",
       "3981               0.003233                     0.000377            0.000722   \n",
       "\n",
       "      thymidylate_synthase_inhibitor  tlr_agonist  tlr_antagonist  \\\n",
       "0                           0.001092     0.001815        0.000786   \n",
       "1                           0.000183     0.000221        0.000181   \n",
       "2                           0.000000     0.000000        0.000000   \n",
       "3                           0.001033     0.002138        0.000936   \n",
       "4                           0.004180     0.002265        0.001172   \n",
       "...                              ...          ...             ...   \n",
       "3977                        0.000207     0.001781        0.000124   \n",
       "3978                        0.001351     0.003280        0.000589   \n",
       "3979                        0.003111     0.002938        0.000759   \n",
       "3980                        0.002054     0.001958        0.001307   \n",
       "3981                        0.001607     0.002203        0.000888   \n",
       "\n",
       "      tnf_inhibitor  topoisomerase_inhibitor  \\\n",
       "0          0.001763                 0.000674   \n",
       "1          0.002601                 0.000129   \n",
       "2          0.000000                 0.000000   \n",
       "3          0.000773                 0.000833   \n",
       "4          0.001772                 0.001429   \n",
       "...             ...                      ...   \n",
       "3977       0.000985                 0.000313   \n",
       "3978       0.002187                 0.001571   \n",
       "3979       0.001405                 0.003265   \n",
       "3980       0.002296                 0.000978   \n",
       "3981       0.002765                 0.000725   \n",
       "\n",
       "      transient_receptor_potential_channel_antagonist  \\\n",
       "0                                            0.000956   \n",
       "1                                            0.000675   \n",
       "2                                            0.000000   \n",
       "3                                            0.001007   \n",
       "4                                            0.000751   \n",
       "...                                               ...   \n",
       "3977                                         0.000513   \n",
       "3978                                         0.001840   \n",
       "3979                                         0.001330   \n",
       "3980                                         0.000757   \n",
       "3981                                         0.001172   \n",
       "\n",
       "      tropomyosin_receptor_kinase_inhibitor  trpv_agonist  trpv_antagonist  \\\n",
       "0                                  0.001470      0.000418         0.002208   \n",
       "1                                  0.000307      0.000142         0.001385   \n",
       "2                                  0.000000      0.000000         0.000000   \n",
       "3                                  0.000877      0.001652         0.001582   \n",
       "4                                  0.001215      0.000780         0.002286   \n",
       "...                                     ...           ...              ...   \n",
       "3977                               0.000788      0.003557         0.002756   \n",
       "3978                               0.000547      0.000667         0.003309   \n",
       "3979                               0.000636      0.000853         0.002785   \n",
       "3980                               0.001164      0.000624         0.001806   \n",
       "3981                               0.000740      0.000718         0.001705   \n",
       "\n",
       "      tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "0              0.001864                   0.000958   \n",
       "1              0.001087                   0.001686   \n",
       "2              0.000000                   0.000000   \n",
       "3              0.018524                   0.043101   \n",
       "4              0.001355                   0.001175   \n",
       "...                 ...                        ...   \n",
       "3977           0.310272                   0.003709   \n",
       "3978           0.001734                   0.001168   \n",
       "3979           0.001060                   0.001870   \n",
       "3980           0.001202                   0.001308   \n",
       "3981           0.002339                   0.001133   \n",
       "\n",
       "      ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n",
       "0                                  0.000638         0.000670   0.001755   \n",
       "1                                  0.000111         0.005895   0.000319   \n",
       "2                                  0.000000         0.000000   0.000000   \n",
       "3                                  0.000489         0.003454   0.002697   \n",
       "4                                  0.000853         0.000805   0.001314   \n",
       "...                                     ...              ...        ...   \n",
       "3977                               0.001189         0.006384   0.000460   \n",
       "3978                               0.001049         0.002244   0.002268   \n",
       "3979                               0.000727         0.000656   0.001172   \n",
       "3980                               0.000482         0.000661   0.001848   \n",
       "3981                               0.000569         0.000827   0.001545   \n",
       "\n",
       "      vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                       0.002758       0.000979  \n",
       "1                       0.000169       0.002273  \n",
       "2                       0.000000       0.000000  \n",
       "3                       0.002451       0.002619  \n",
       "4                       0.000476       0.001909  \n",
       "...                          ...            ...  \n",
       "3977                    0.000433       0.000413  \n",
       "3978                    0.000337       0.000828  \n",
       "3979                    0.000380       0.001402  \n",
       "3980                    0.000704       0.002586  \n",
       "3981                    0.000363       0.000945  \n",
       "\n",
       "[3982 rows x 207 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
