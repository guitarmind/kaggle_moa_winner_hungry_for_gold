{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\n\n# for kaggle kernel\n# add datasets iterative-stratification and umaplearn\n\nsys.path.append('../input/iterative-stratification')\nsys.path.append('../input/umaplearn/umap')\n%mkdir model\n%mkdir interim\n\nfrom scipy.sparse.csgraph import connected_components\nfrom umap import UMAP\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold, RepeatedMultilabelStratifiedKFold\n\nimport numpy as np\nimport scipy as sp\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\nimport time\n# import joblib\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.manifold import TSNE\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nprint(f\"is cuda available: {torch.cuda.is_available()}\")\n\nimport warnings\n# warnings.filterwarnings('ignore')\n\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nDEFAULT_SEED = 512\nseed_everything(seed_value=DEFAULT_SEED)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# file name prefix\nNB = '101'\n\nIS_TRAIN = True\n\nMODEL_DIR = \"model\" # \"../model\"\nINT_DIR = \"interim\" # \"../interim\"\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n# label smoothing\nPMIN = 0.0\nPMAX = 1.0\n\n# submission smoothing\nSMIN = 0.0\nSMAX = 1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nfor col in (GENES + CELLS):\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = pd.concat([train_features, test_features])[col].values.reshape(vec_len+vec_len_test, 1)\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f'{MODEL_DIR}/{NB}_{col}_quantile_transformer.pkl')\n    else:\n        transformer = pd.read_pickle(f'{MODEL_DIR}/{NB}_{col}_quantile_transformer.pkl')        \n\n    train_features[col] = transformer.transform(train_features[col].values.reshape(vec_len, 1)).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\nn_comp = 50\nn_dim = 15\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n\nif IS_TRAIN:\n    pca = PCA(n_components=n_comp, random_state=DEFAULT_SEED).fit(train_features[GENES])\n    umap = UMAP(n_components=n_dim, random_state=DEFAULT_SEED).fit(train_features[GENES])\n    pd.to_pickle(pca, f\"{MODEL_DIR}/{NB}_pca_g.pkl\")\n    pd.to_pickle(umap, f\"{MODEL_DIR}/{NB}_umap_g.pkl\")\nelse:\n    pca = pd.read_pickle(f\"{MODEL_DIR}/{MB}_pca_g.pkl\")\n    umap = pd.read_pickle(f\"{MODEL_DIR}/{NB}_umap_g.pkl\")\n    \ndata2 = pca.transform(data[GENES])\ndata3 = umap.transform(data[GENES])\n\ntrain2 = data2[:train_features.shape[0]]\ntest2 = data2[-test_features.shape[0]:]\ntrain3 = data3[:train_features.shape[0]]\ntest3 = data3[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntrain3 = pd.DataFrame(train3, columns=[f'umap_G-{i}' for i in range(n_dim)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest3 = pd.DataFrame(test3, columns=[f'umap_G-{i}' for i in range(n_dim)])\n\ntrain_features = pd.concat((train_features, train2, train3), axis=1)\ntest_features = pd.concat((test_features, test2, test3), axis=1)\n\n#CELLS\nn_comp = 15\nn_dim = 5\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n\n\nif IS_TRAIN:\n    pca = PCA(n_components=n_comp, random_state=DEFAULT_SEED).fit(train_features[CELLS])\n    umap = UMAP(n_components=n_dim, random_state=DEFAULT_SEED).fit(train_features[CELLS])\n    pd.to_pickle(pca, f\"{MODEL_DIR}/{NB}_pca_c.pkl\")\n    pd.to_pickle(umap, f\"{MODEL_DIR}/{NB}_umap_c.pkl\")\nelse:\n    pca = pd.read_pickle(f\"{MODEL_DIR}/{NB}_pca_c.pkl\")\n    umap = pd.read_pickle(f\"{MODEL_DIR}/{NB}_umap_c.pkl\")   \n\ndata2 = pca.transform(data[CELLS])\ndata3 = umap.transform(data[CELLS])\n\ntrain2 = data2[:train_features.shape[0]]\ntest2 = data2[-test_features.shape[0]:]\ntrain3 = data3[:train_features.shape[0]]\ntest3 = data3[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntrain3 = pd.DataFrame(train3, columns=[f'umap_C-{i}' for i in range(n_dim)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest3 = pd.DataFrame(test3, columns=[f'umap_C-{i}' for i in range(n_dim)])\n\ntrain_features = pd.concat((train_features, train2, train3), axis=1)\ntest_features = pd.concat((test_features, test2, test3), axis=1)\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\nif IS_TRAIN:\n    var_thresh = VarianceThreshold(threshold=0.5).fit(train_features.iloc[:, 4:])\n    pd.to_pickle(var_thresh, f\"{MODEL_DIR}/{NB}_variance_thresh0_5.pkl\")\nelse:\n    var_thresh = pd.read_pickle(f\"{MODEL_DIR}/{NB}_variance_thresh0_5.pkl\")\n                                \ndata = train_features.append(test_features)\ndata_transformed = var_thresh.transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\nprint(train_features.shape)\nprint(test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features[train_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_pickle(f\"{INT_DIR}/{NB}_train_preprocessed.pkl\")\ntest.to_pickle(f\"{INT_DIR}/{NB}_test_preprocessed.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 203-101-nonscored-pred-2layers.ipynb"},{"metadata":{"trusted":true},"cell_type":"code","source":"# file name prefix\nNB = '203'\n\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n# label smoothing\nPMIN = 0.0\nPMAX = 1.0\n\n# submission smoothing\nSMIN = 0.0\nSMAX = 1.0\n\n# model hyper params\nHIDDEN_SIZE = 2048\n\n# training hyper params\nEPOCHS = 15\nBATCH_SIZE = 256\nNFOLDS = 10 # 10\nNREPEATS = 1\nNSEEDS = 5 # 5\n\n# Adam hyper params\nLEARNING_RATE = 5e-4\nWEIGHT_DECAY = 1e-5\n\n# scheduler hyper params\nPCT_START = 0.2\nDIV_FACS = 1e3\nMAX_LR = 1e-2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data):    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data\n\nclass MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n\ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n\ndef calc_valid_log_loss(train, target, target_cols):\n    y_pred = train[target_cols].values\n    y_true = target[target_cols].values\n    \n    y_true_t = torch.from_numpy(y_true.astype(np.float64)).clone()\n    y_pred_t = torch.from_numpy(y_pred.astype(np.float64)).clone()\n    \n    return torch.nn.BCELoss()(y_pred_t, y_true_t).to('cpu').detach().numpy().copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size=HIDDEN_SIZE):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n               \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n                \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(train, test, trn_idx, val_idx, feature_cols, target_cols, fold, seed):\n    \n    seed_everything(seed)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    train_df = train_.loc[trn_idx,:].reset_index(drop=True)\n    valid_df = train_.loc[val_idx,:].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=len(feature_cols),\n        num_targets=len(target_cols),\n    )\n    \n    model.to(DEVICE)\n       \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=PCT_START, div_factor=DIV_FACS, \n                                              max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    best_loss_epoch = -1\n    \n    if IS_TRAIN:\n        for epoch in range(EPOCHS):\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_fn, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n\n            if valid_loss < best_loss:            \n                best_loss = valid_loss\n                best_loss_epoch = epoch\n                oof[val_idx] = valid_preds\n                model.to('cpu')\n                torch.save(model.state_dict(), f\"{MODEL_DIR}/{NB}_nonscored_SEED{seed}_FOLD{fold}_.pth\")\n                model.to(DEVICE)\n\n            if epoch % 10 == 0 or epoch == EPOCHS-1:\n                print(f\"seed: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}, best_loss: {best_loss:.6f}, best_loss_epoch: {best_loss_epoch}\")                           \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=len(feature_cols),\n        num_targets=len(target_cols),\n    )\n    \n    model.load_state_dict(torch.load(f\"{MODEL_DIR}/{NB}_nonscored_SEED{seed}_FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    if not IS_TRAIN:\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        oof[val_idx] = valid_preds\n\n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(train, test, feature_cols, target_cols, NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    mskf = RepeatedMultilabelStratifiedKFold(n_splits=NFOLDS, n_repeats=NREPEATS, random_state=None)\n    \n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n        oof_, pred_ = run_training(train, test, t_idx, v_idx, feature_cols, target_cols, f, seed)\n        \n        predictions += pred_ / NFOLDS / NREPEATS\n        oof += oof_ / NREPEATS\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_seeds(train, test, feature_cols, target_cols, nfolds=NFOLDS, nseed=NSEEDS):\n    seed_list = range(nseed)\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    time_start = time.time()\n\n    for seed in seed_list:\n\n        oof_, predictions_ = run_k_fold(train, test, feature_cols, target_cols, nfolds, seed)\n        oof += oof_ / nseed\n        predictions += predictions_ / nseed\n        print(f\"seed {seed}, elapsed time: {time.time() - time_start}\")\n\n    train[target_cols] = oof\n    test[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_pickle(f\"{INT_DIR}/101_train_preprocessed.pkl\")\ntest = pd.read_pickle(f\"{INT_DIR}/101_test_preprocessed.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### non-scored labels prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove nonscored labels if all values == 0\ntrain_targets_nonscored = train_targets_nonscored.loc[:, train_targets_nonscored.sum() != 0]\nprint(train_targets_nonscored.shape)\n\ntrain = train.merge(train_targets_nonscored, on='sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train[train_targets_nonscored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\nfeature_cols = [c for c in process_data(train).columns if c not in target_cols and c not in ['kfold','sig_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_seeds(train, test, feature_cols, target_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"train shape: {train.shape}\")\nprint(f\"test  shape: {test.shape}\")\nprint(f\"features : {len(feature_cols)}\")\nprint(f\"targets  : {len(target_cols)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_loss_total = calc_valid_log_loss(train, target, target_cols)\nprint(f\"CV loss: {valid_loss_total}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_pickle(f\"{INT_DIR}/{NB}_train_nonscored_pred.pkl\")\ntest.to_pickle(f\"{INT_DIR}/{NB}_test_nonscored_pred.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = train_targets_nonscored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_nonscored[target_cols].values\ny_true = y_true > 0.5\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 503-203-tabnet-with-nonscored-features-10fold3seed"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# file name prefix\nNB = '503'\nNB_PREV = '203'\n\n# IS_TRAIN = False\n\n# MODEL_DIR = \"../input/moa503/503-tabnet\" # \"../model\"\n# INT_DIR = \"../input/moa503/203-nonscored-pred\" # \"../interim\"\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n# label smoothing\nPMIN = 0.0\nPMAX = 1.0\n\n# submission smoothing\nSMIN = 0.0\nSMAX = 1.0\n\n# model hyper params\n\n# training hyper params\n# EPOCHS = 25\n# BATCH_SIZE = 256\nNFOLDS = 10 # 10\nNREPEATS = 1\nNSEEDS = 3 # 5\n\n# Adam hyper params\nLEARNING_RATE = 5e-4\nWEIGHT_DECAY = 1e-5\n\n# scheduler hyper params\nPCT_START = 0.2\nDIV_FACS = 1e3\nMAX_LR = 1e-2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"(nsamples, nfeatures)\")\nprint(train_features.shape)\nprint(train_targets_scored.shape)\nprint(train_targets_nonscored.shape)\nprint(test_features.shape)\nprint(sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\nuse_test_for_preprocessing = False\n\nfor col in (GENES + CELLS):\n\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        if use_test_for_preprocessing:\n            raw_vec = pd.concat([train_features, test_features])[col].values.reshape(vec_len+vec_len_test, 1)\n            transformer.fit(raw_vec)\n        else:\n            raw_vec = train_features[col].values.reshape(vec_len, 1)\n            transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f'{MODEL_DIR}/{NB}_{col}_quantile_transformer.pkl')\n    else:\n        transformer = pd.read_pickle(f'{MODEL_DIR}/{NB}_{col}_quantile_transformer.pkl') \n\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n\n\n    train_features[col] = transformer.transform(train_features[col].values.reshape(vec_len, 1)).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\n\nn_comp = 90\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=42).fit(data[GENES])\n    pd.to_pickle(fa, f'{MODEL_DIR}/{NB}_factor_analysis_g.pkl')\nelse:\n    fa = pd.read_pickle(f'{MODEL_DIR}/{NB}_factor_analysis_g.pkl')\n    \ndata2 = (fa.transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n#CELLS\n\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=42).fit(data[CELLS])\n    pd.to_pickle(fa, f'{MODEL_DIR}/{NB}_factor_analysis_c.pkl')\nelse:\n    fa = pd.read_pickle(f'{MODEL_DIR}/{NB}_factor_analysis_c.pkl')\n\ndata2 = (fa.transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        \n        if IS_TRAIN:\n            kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n            pd.to_pickle(kmeans, f\"{MODEL_DIR}/{NB}_kmeans_{kind}.pkl\")\n        else:\n            kmeans = pd.read_pickle(f\"{MODEL_DIR}/{NB}_kmeans_{kind}.pkl\")\n            \n        train[f'clusters_{kind}'] = kmeans.predict(train_)\n        test[f'clusters_{kind}'] = kmeans.predict(test_)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features ,test_features=fe_cluster(train_features,test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_features.shape)\nprint(test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\ntrain_features,test_features=fe_stats(train_features,test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_features.shape)\nprint(test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_vehicle = True\n\nif remove_vehicle:\n    trt_idx = train_features['cp_type']=='trt_cp'\n    train_features = train_features.loc[trt_idx].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[trt_idx].reset_index(drop=True)\n    train_targets_nonscored = train_targets_nonscored.loc[trt_idx].reset_index(drop=True)\nelse:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\n# target = train[train_targets_scored.columns]\ntarget = train[train_targets_scored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(target.shape)\nprint(train_features.shape)\nprint(test_features.shape)\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nonscored_pred = pd.read_pickle(f'{INT_DIR}/{NB_PREV}_train_nonscored_pred.pkl')\ntest_nonscored_pred = pd.read_pickle(f'{INT_DIR}/{NB_PREV}_test_nonscored_pred.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_nonscored = train_targets_nonscored.loc[:, train_targets_nonscored.sum() != 0]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(train_nonscored_pred[train_targets_nonscored.columns], on='sig_id')\ntest = test.merge(test_nonscored_pred[train_targets_nonscored.columns], on='sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\nnonscored_target = [c for c in train_targets_nonscored.columns if c != \"sig_id\"]\n\nfor col in (nonscored_target):\n\n    vec_len = len(train[col].values)\n    vec_len_test = len(test[col].values)\n    raw_vec = train[col].values.reshape(vec_len, 1)\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f'{MODEL_DIR}/{NB}_{col}_quantile_transformer.pkl')\n    else:\n        transformer = pd.read_pickle(f'{MODEL_DIR}/{NB}_{col}_quantile_transformer.pkl')        \n\n    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]\nlen(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features=len(feature_cols)\nnum_targets=len(target_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom pytorch_tabnet.metrics import Metric\n\nclass LabelSmoothing(nn.Module):\n    \"\"\"\n    NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.0, n_cls=2):\n        \"\"\"\n        Constructor for the LabelSmoothing module.\n        :param smoothing: label smoothing factor\n        \"\"\"\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing + smoothing / n_cls\n        self.smoothing = smoothing / n_cls\n\n    def forward(self, x, target):\n        probs = torch.nn.functional.sigmoid(x,)\n        target1 = self.confidence * target + (1-target) * self.smoothing\n        loss = -(torch.log(probs+1e-15) * target1 + (1-target1) * torch.log(1-probs+1e-15))\n        return loss.mean()\n    \nclass SmoothedLogLossMetric(Metric):\n    \"\"\"\n    BCE with logit loss\n    \"\"\"\n    def __init__(self, smoothing=0.001):\n        self._name = f\"{smoothing:.3f}\" # write an understandable name here\n        self._maximize = False\n        self._lossfn = LabelSmoothing(smoothing)\n\n    def __call__(self, y_true, y_score):\n        \"\"\"\n        \"\"\"\n        y_true = torch.from_numpy(y_true.astype(np.float32)).clone()\n        y_score = torch.from_numpy(y_score.astype(np.float32)).clone()\n        return self._lossfn(y_score, y_true).to('cpu').detach().numpy().copy().take(0)\n    \nclass LogLossMetric(Metric):\n    \"\"\"\n    BCE with logit loss\n    \"\"\"\n    def __init__(self, smoothing=0.0):\n        self._name = f\"{smoothing:.3f}\" # write an understandable name here\n        self._maximize = False\n        self._lossfn = LabelSmoothing(smoothing)\n\n    def __call__(self, y_true, y_score):\n        \"\"\"\n        \"\"\"\n        y_true = torch.from_numpy(y_true.astype(np.float32)).clone()\n        y_score = torch.from_numpy(y_score.astype(np.float32)).clone()\n#         print(\"log loss metric: \", self._lossfn(y_score, y_true).to('cpu').detach().numpy().copy())\n        return self._lossfn(y_score, y_true).to('cpu').detach().numpy().copy().take(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data):\n#     data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2, 0: 0, 1: 1, 2: 2})\n    data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1, 0: 0, 1: 1})   \n    return data\n\ndef run_training_tabnet(train, test, trn_idx, val_idx, feature_cols, target_cols, fold, seed, filename=\"tabnet\"):\n    \n    seed_everything(seed)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    train_df = train_.loc[trn_idx,:].reset_index(drop=True)\n    valid_df = train_.loc[val_idx,:].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n        \n    model = TabNetRegressor(n_d=32, n_a=32, n_steps=1, lambda_sparse=0,\n                            cat_dims=[3, 2], cat_emb_dim=[1, 1], cat_idxs=[0, 1],\n                            optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                            mask_type='entmax',  # device_name=DEVICE,\n                            scheduler_params=dict(milestones=[100, 150], gamma=0.9),#)\n                            scheduler_fn=torch.optim.lr_scheduler.MultiStepLR,\n                            verbose=10,\n                            seed = seed)\n    \n    loss_fn = LabelSmoothing(0.001)\n       \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    \n    if IS_TRAIN:\n        model.fit(X_train=x_train, y_train=y_train,\n                  eval_set=[(x_valid, y_valid)], eval_metric=[LogLossMetric, SmoothedLogLossMetric],\n                  max_epochs=200, patience=50, batch_size=1024, virtual_batch_size=128,\n                    num_workers=0, drop_last=False, loss_fn=loss_fn\n                  )\n        model.save_model(f\"{MODEL_DIR}/{NB}_{filename}_SEED{seed}_FOLD{fold}\")\n            \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    \n    model = TabNetRegressor(n_d=32, n_a=32, n_steps=1, lambda_sparse=0,\n                            cat_dims=[3, 2], cat_emb_dim=[1, 1], cat_idxs=[0, 1],\n                            optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                            mask_type='entmax',  # device_name=DEVICE,\n                            scheduler_params=dict(milestones=[100, 150], gamma=0.9),#)\n                            scheduler_fn=torch.optim.lr_scheduler.MultiStepLR,\n                            verbose=10,\n                            seed = seed)\n    \n    model.load_model(f\"{MODEL_DIR}/{NB}_{filename}_SEED{seed}_FOLD{fold}.zip\")\n\n    valid_preds = model.predict(x_valid)\n\n    valid_preds = torch.sigmoid(torch.as_tensor(valid_preds)).detach().cpu().numpy()\n    oof[val_idx] = valid_preds\n        \n    predictions = model.predict(x_test)\n    predictions = torch.sigmoid(torch.as_tensor(predictions)).detach().cpu().numpy()\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(train, test, feature_cols, target_cols, NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state = seed)\n    \n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n        oof_, pred_ = run_training_tabnet(train, test, t_idx, v_idx, feature_cols, target_cols, f, seed)\n        \n        predictions += pred_ / NFOLDS / NREPEATS\n        oof += oof_ / NREPEATS\n        \n    return oof, predictions\n\ndef run_seeds(train, test, feature_cols, target_cols, nfolds=NFOLDS, nseed=NSEEDS):\n    seed_list = range(nseed)\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    time_start = time.time()\n\n    for seed in seed_list:\n\n        oof_, predictions_ = run_k_fold(train, test, feature_cols, target_cols, nfolds, seed)\n        oof += oof_ / nseed\n        predictions += predictions_ / nseed\n        print(f\"seed {seed}, elapsed time: {time.time() - time_start}\")\n\n    train[target_cols] = oof\n    test[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_pickle(f\"{INT_DIR}/{NB}_pre_train.pkl\")\ntest.to_pickle(f\"{INT_DIR}/{NB}_pre_test.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_seeds(train, test, feature_cols, target_cols, NFOLDS, NSEEDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_pickle(f\"{INT_DIR}/{NB}_train.pkl\")\ntest.to_pickle(f\"{INT_DIR}/{NB}_test.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train[target_cols] = np.maximum(PMIN, np.minimum(PMAX, train[target_cols]))\nvalid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_true = y_true > 0.5\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for root, dirs, files in os.walk(\"model/\"):\n    for f in files:\n        if f[-3:] == \"zip\":\n            print(f)\n            os.rename(\"model/\" + f, \"model/\" + f[:-3]+\"model\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
